"""
éºå‚³ç®—æ³•å„ªåŒ– gMLP æ¨¡å‹ - æ•´åˆå°ˆæ¥­éºå‚³ç®—æ³•æ¡†æ¶
åŸºæ–¼ neural-network-genetic-algorithm-master çš„å¯¦ç¾
æ”¯æ´è¶…åƒæ•¸å„ªåŒ–ã€æ¶æ§‹æœç´¢å’Œè¨“ç·´ç­–ç•¥å„ªåŒ–
"""

import random
import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Any, Optional
import matplotlib.pyplot as plt
import json
from datetime import datetime
from dataclasses import dataclass, field
from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor
import copy
import logging
import pickle
import os
from collections import defaultdict
import time

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class Individual:
    """å€‹é«”é¡åˆ¥ - ä»£è¡¨ä¸€å€‹ gMLP é…ç½®"""

    genes: Dict[str, Any] = field(default_factory=dict)
    fitness: float = 0.0
    accuracy: float = 0.0
    training_time: float = 0.0
    parameters: int = 0
    generation: int = 0
    parent_ids: List[int] = field(default_factory=list)
    mutation_history: List[str] = field(default_factory=list)

    def __post_init__(self):
        self.id = id(self)

    def __hash__(self):
        return hash(str(sorted(self.genes.items())))

    def to_dict(self) -> Dict:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return {
            "genes": self.genes,
            "fitness": float(self.fitness),
            "accuracy": float(self.accuracy),
            "training_time": float(self.training_time),
            "parameters": int(self.parameters),
            "generation": int(self.generation),
            "parent_ids": list(self.parent_ids),
            "mutation_history": list(self.mutation_history),
            "id": str(self.id),
        }


class NetworkConfig:
    """ç¶²çµ¡é…ç½®é¡ - åŸºæ–¼ network.py çš„è¨­è¨ˆ"""

    def __init__(self):
        self.gene_ranges = {
            # æ¨¡å‹æ¶æ§‹åŸºå›  - æ“´å¤§ç¯„åœä»¥å¢åŠ éš¨æ©Ÿæ€§å’Œå¤šæ¨£æ€§
            "depth": {
                "type": "int",
                "range": (4, 36),  # æ“´å¤§æ·±åº¦ç¯„åœï¼Œå…è¨±æ›´å¤šè®ŠåŒ–
                "mutation_strength": 1,
            },
            "dim": {
                "type": "int",
                "range": (4, 192),  # æ“´å¤§ç¶­åº¦ç¯„åœï¼Œå¢åŠ æ¨¡å‹å¤§å°çš„å¤šæ¨£æ€§
                "mutation_strength": 16,
            },
            "ff_mult": {
                "type": "int",
                "range": (2, 4),  # å…è¨±æ›´å¤šFFNå€æ•¸é¸æ“‡
                "mutation_strength": 1,
            },
            "prob_survival": {
                "type": "float",
                "range": (0.85, 1.0),  # æ“´å¤§å­˜æ´»æ¦‚ç‡ç¯„åœ
                "mutation_strength": 0.03,
            },
        }

        # å›ºå®šçš„è¨“ç·´è¶…åƒæ•¸å’Œç­–ç•¥ - ä½¿ç”¨ model_16.py çš„é è¨­å€¼
        self.fixed_params = {
            # è¨“ç·´è¶…åƒæ•¸ (ä¾†è‡ª model_16.py çš„é è¨­å€¼)
            "lr": 0.01,
            "weight_decay": 0.012,
            "batch_size": 64,  # ä¾†è‡ª load_cifar10_data_enhanced
            "label_smoothing": 0.08,  # ä¾†è‡ª train_enhanced
            "gradient_clip": 0.8,  # ä¾†è‡ª train_enhanced
            # è¨“ç·´ç­–ç•¥
            "use_mixup": True,  # é è¨­å•Ÿç”¨
            "alpha": 0.1,  # é è¨­å€¼
            # å„ªåŒ–å™¨å’Œèª¿åº¦å™¨è¨­å®š
            "optimizer_type": "AdamW",
            "scheduler_type": "CosineAnnealingLR",
            "betas": (0.9, 0.95),  # AdamW é è¨­å€¼
            "eta_min": 8e-6,  # CosineAnnealingLR é è¨­å€¼
        }

    def get_random_gene(self, gene_name: str) -> Any:
        """ç²å–éš¨æ©ŸåŸºå› å€¼"""
        config = self.gene_ranges[gene_name]

        if config["type"] == "int":
            return random.randint(config["range"][0], config["range"][1])
        elif config["type"] == "float":
            if config.get("log_scale", False):
                # å°æ•¸å°ºåº¦æ¡æ¨£
                log_min = np.log10(config["range"][0])
                log_max = np.log10(config["range"][1])
                return 10 ** random.uniform(log_min, log_max)
            else:
                return random.uniform(config["range"][0], config["range"][1])
        elif config["type"] == "choice":
            return random.choice(config["choices"])
        elif config["type"] == "bool":
            return random.choice([True, False])

    def mutate_gene(self, gene_name: str, current_value: Any) -> Any:
        """çªè®ŠåŸºå› """
        config = self.gene_ranges[gene_name]

        if config["type"] == "int":
            mutation = random.randint(
                -config["mutation_strength"], config["mutation_strength"]
            )
            new_value = current_value + mutation
            return max(config["range"][0], min(config["range"][1], new_value))

        elif config["type"] == "float":
            if config.get("log_scale", False):
                # å°æ•¸å°ºåº¦çªè®Š
                log_current = np.log10(current_value)
                mutation = random.gauss(0, 0.1)  # æ¨™æº–å·®ç‚º 0.1
                new_log = log_current + mutation
                new_value = 10**new_log
                return max(config["range"][0], min(config["range"][1], new_value))
            else:
                mutation = random.gauss(0, config["mutation_strength"])
                new_value = current_value + mutation
                return max(config["range"][0], min(config["range"][1], new_value))

        elif config["type"] == "choice":
            return random.choice(config["choices"])
        elif config["type"] == "bool":
            return not current_value  # å¸ƒæ—å€¼ç¿»è½‰

        return current_value


class FitnessEvaluator:
    """é©æ‡‰åº¦è©•ä¼°å™¨ - åŸºæ–¼ train.py çš„è¨­è¨ˆ"""

    def __init__(self, weights: Dict[str, float] = None):
        self.weights = weights or {
            "accuracy": 0.6,  # æº–ç¢ºç‡æ¬Šé‡
            "efficiency": 0.25,  # æ•ˆç‡æ¬Šé‡ (åƒæ•¸å°‘/è¨“ç·´å¿«)
            "stability": 0.15,  # ç©©å®šæ€§æ¬Šé‡
        }
        self.evaluation_cache = {}  # è©•ä¼°å¿«å–

    def evaluate(
        self,
        individual: Individual,
        trainloader,
        testloader,
        device,
        cache_key: str = None,
    ) -> Individual:
        """è©•ä¼°å€‹é«”é©æ‡‰åº¦"""

        # æª¢æŸ¥å¿«å–
        if cache_key and cache_key in self.evaluation_cache:
            cached_result = self.evaluation_cache[cache_key]
            individual.accuracy = cached_result["accuracy"]
            individual.training_time = cached_result["training_time"]
            individual.parameters = cached_result["parameters"]
            individual.fitness = cached_result["fitness"]
            logger.info(f"ä½¿ç”¨å¿«å–çµæœ: æº–ç¢ºç‡={individual.accuracy:.2f}%")
            return individual

        try:
            # 1. å‰µå»ºæ¨¡å‹
            model = self.create_model_from_genes(individual.genes)

            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸå‰µå»º
            if model is None:
                raise ValueError("æ¨¡å‹å‰µå»ºå¤±æ•—")

            model = model.to(device)

            # 2. è¨ˆç®—åƒæ•¸æ•¸é‡
            total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            individual.parameters = total_params

            # 3. å¿«é€Ÿè¨“ç·´è©•ä¼°
            accuracy, training_time, stability_score = self.quick_train_evaluate(
                model, individual.genes, trainloader, testloader, device
            )

            individual.accuracy = accuracy
            individual.training_time = training_time

            # 4. è¨ˆç®—ç¶œåˆé©æ‡‰åº¦
            individual.fitness = self.calculate_fitness(
                accuracy, total_params, training_time, stability_score
            )

            # 5. å„²å­˜åˆ°å¿«å–
            if cache_key:
                self.evaluation_cache[cache_key] = {
                    "accuracy": accuracy,
                    "training_time": training_time,
                    "parameters": total_params,
                    "fitness": individual.fitness,
                }

            logger.info(
                f"å€‹é«”è©•ä¼°: æº–ç¢ºç‡={accuracy:.2f}%, åƒæ•¸={total_params/1e6:.2f}M, é©æ‡‰åº¦={individual.fitness:.4f}"
            )

        except Exception as e:
            logger.error(f"è©•ä¼°å¤±æ•—: {str(e)}")
            individual.fitness = 0.001  # è¨­ç½®ä¸€å€‹å¾ˆå°çš„æ­£å€¼è€Œä¸æ˜¯0
            individual.accuracy = 10.0  # è¨­ç½®ä¸€å€‹åŸºç·šæº–ç¢ºç‡
            individual.training_time = 0.0
            individual.parameters = 100000  # è¨­ç½®ä¸€å€‹é è¨­åƒæ•¸æ•¸é‡

        return individual

    def create_model_from_genes(self, genes: Dict) -> nn.Module:
        """æ ¹æ“šåŸºå› å‰µå»º gMLP æ¨¡å‹"""
        from g_mlp_pytorch import gMLPVision

        # ç¢ºä¿ image_size èƒ½è¢« patch_size æ•´é™¤ï¼Œé¿å…å¼µé‡å°ºå¯¸ä¸åŒ¹é…
        image_size = 32  # CIFAR-10 åœ–åƒå°ºå¯¸
        patch_size = 4  # ç¢ºä¿ 32 % 4 == 0

        # ç›´æ¥ä½¿ç”¨åŸºå› ä¸­çš„éš¨æ©Ÿåƒæ•¸ï¼Œä½†é€²è¡Œåˆç†æ€§æª¢æŸ¥
        raw_dim = int(genes["dim"])
        raw_depth = int(genes["depth"])
        raw_ff_mult = int(genes["ff_mult"])
        raw_prob_survival = float(genes["prob_survival"])

        # ç¢ºä¿åƒæ•¸åœ¨åˆç†ç¯„åœå…§ï¼ˆä½†ä¿æŒéš¨æ©Ÿæ€§å’Œå¤šæ¨£æ€§ï¼‰
        dim = max(4, min(192, raw_dim))  # å…è¨±æ›´å¤§çš„ç¶­åº¦ç¯„åœï¼š4-192
        depth = max(4, min(36, raw_depth))  # å…è¨±æ›´æ·±çš„æ¨¡å‹ï¼š4-36
        ff_mult = max(2, min(4, raw_ff_mult))  # å…è¨±æ›´å¤šFFNé¸æ“‡
        prob_survival = max(0.85, min(1.0, raw_prob_survival))  # æ›´å¯¬çš„å­˜æ´»æ¦‚ç‡ç¯„åœ

        # å¤šæ¬¡å˜—è©¦å‰µå»ºæ¨¡å‹ï¼Œä½¿ç”¨éæ¸›çš„è¤‡é›œåº¦ç­–ç•¥
        model_attempts = [
            # ç¬¬ä¸€æ¬¡å˜—è©¦ï¼šä½¿ç”¨åŸºå› æŒ‡å®šçš„éš¨æ©Ÿåƒæ•¸
            {
                "dim": dim,
                "depth": depth,
                "ff_mult": ff_mult,
                "prob_survival": prob_survival,
            },
            # ç¬¬äºŒæ¬¡å˜—è©¦ï¼šé©åº¦é™ä½è¤‡é›œåº¦ä½†ä¿æŒéš¨æ©Ÿæ€§
            {
                "dim": max(4, min(128, dim - 16)),  # ç¨å¾®é™ä½ç¶­åº¦ï¼Œä½†ä¿æŒåœ¨4-128ç¯„åœ
                "depth": max(4, min(24, depth - 4)),  # ç¨å¾®é™ä½æ·±åº¦ï¼Œä½†ä¿æŒåœ¨4-24ç¯„åœ
                "ff_mult": max(2, ff_mult - 1),  # é™ä½ff_multä½†ä¿æŒéš¨æ©Ÿ
                "prob_survival": min(1.0, prob_survival + 0.05),  # ç¨å¾®æé«˜å­˜æ´»æ¦‚ç‡
            },
            # ç¬¬ä¸‰æ¬¡å˜—è©¦ï¼šä½¿ç”¨åŸºæ–¼åŸå§‹åŸºå› çš„éš¨æ©ŸåŒ–å®‰å…¨é…ç½®
            {
                "dim": max(
                    4, 4 + ((raw_dim - 4) % 64)
                ),  # åŸºæ–¼åŸå§‹åŸºå› çš„éš¨æ©Ÿç¶­åº¦ï¼š4-68ç¯„åœå…§
                "depth": max(
                    4, 4 + (raw_depth % 8)
                ),  # åŸºæ–¼åŸå§‹åŸºå› çš„éš¨æ©Ÿæ·±åº¦ï¼š4-11 ä¸­çš„ä¸€å€‹
                "ff_mult": 2 + (raw_ff_mult % 2),  # åŸºæ–¼åŸå§‹åŸºå› çš„éš¨æ©Ÿff_multï¼š2æˆ–3
                "prob_survival": 0.9 + (raw_prob_survival % 0.1),  # 0.9-1.0 ä¹‹é–“
            },
        ]

        # å˜—è©¦æ¯å€‹é…ç½®
        for i, config in enumerate(model_attempts):
            try:
                logger.debug(f"å˜—è©¦éš¨æ©Ÿé…ç½® {i+1}/3: {config}")

                model = gMLPVision(
                    image_size=image_size,
                    patch_size=patch_size,
                    num_classes=10,
                    dim=config["dim"],
                    depth=config["depth"],
                    ff_mult=config["ff_mult"],
                    channels=3,
                    prob_survival=config["prob_survival"],
                )

                # æ¸¬è©¦æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸å‰å‘å‚³æ’­
                test_input = torch.randn(2, 3, 32, 32)  # å°æ‰¹æ¬¡æ¸¬è©¦
                with torch.no_grad():
                    test_output = model(test_input)
                    if test_output.shape == (2, 10):  # æª¢æŸ¥è¼¸å‡ºå½¢ç‹€
                        logger.info(
                            f"æ¨¡å‹å‰µå»ºæˆåŠŸ (é…ç½®: dim={config['dim']}, depth={config['depth']}, ff_mult={config['ff_mult']}, prob_survival={config['prob_survival']:.3f})"
                        )
                        return model
                    else:
                        raise ValueError(f"æ¨¡å‹è¼¸å‡ºå½¢ç‹€éŒ¯èª¤: {test_output.shape}")

            except Exception as e:
                logger.warning(f"éš¨æ©Ÿé…ç½® {i+1} å¤±æ•—: {e}")
                continue

        # å¦‚æœæ‰€æœ‰éš¨æ©Ÿé…ç½®éƒ½å¤±æ•—ï¼Œä½¿ç”¨çµ•å°æœ€å®‰å…¨çš„é…ç½®
        logger.error("æ‰€æœ‰éš¨æ©Ÿé…ç½®éƒ½å¤±æ•—ï¼Œä½¿ç”¨æœ€åŸºæœ¬é…ç½®")
        try:
            model = gMLPVision(
                image_size=32,
                patch_size=8,  # ä½¿ç”¨æ›´å¤§çš„ patch_size (32/8=4 patches per side)
                num_classes=10,
                dim=64,
                depth=4,
                ff_mult=2,
                channels=3,
                prob_survival=1.0,
            )
            # æ¸¬è©¦é€™å€‹æœ€åŸºæœ¬çš„é…ç½®
            test_input = torch.randn(2, 3, 32, 32)
            with torch.no_grad():
                test_output = model(test_input)
                if test_output.shape == (2, 10):
                    logger.info("ä½¿ç”¨æœ€åŸºæœ¬é…ç½®æˆåŠŸ")
                    return model
        except Exception as e:
            logger.error(f"é€£æœ€åŸºæœ¬é…ç½®éƒ½å¤±æ•—: {e}")

        # æœ€å¾Œçš„å¾Œå‚™ï¼šç°¡å–®çš„å…¨é€£æ¥ç¶²çµ¡
        logger.error("ä½¿ç”¨ç°¡å–®çš„å…¨é€£æ¥ç¶²çµ¡ä½œç‚ºå¾Œå‚™")
        return nn.Sequential(
            nn.Flatten(),
            nn.Linear(32 * 32 * 3, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 10),
        )

    def quick_train_evaluate(
        self, model, genes: Dict, trainloader, testloader, device
    ) -> Tuple[float, float, float]:
        """å¿«é€Ÿè¨“ç·´è©•ä¼° - ä½¿ç”¨ model_16.py çš„é è¨­å€¼"""
        from torch.optim import AdamW
        from torch.optim.lr_scheduler import CosineAnnealingLR

        # å¿«é€Ÿè¨“ç·´é…ç½® - ä½¿ç”¨é è¨­å€¼ä½†æ¸›å°‘è¨“ç·´æ™‚é–“
        epochs = 8  # æ¯å€‹å€‹é«”è¨“ç·´8å€‹epochsï¼Œç²å¾—æ›´å¥½çš„è©•ä¼°çµæœ
        max_batches_per_epoch = 35  # æ¸›å°‘æ‰¹æ¬¡æ•¸

        # ä½¿ç”¨ model_16.py ä¸­çš„é è¨­é…ç½®
        try:
            criterion = nn.CrossEntropyLoss(
                label_smoothing=0.08
            )  # ä½¿ç”¨å›ºå®šå€¼é¿å…åŸºå› éŒ¯èª¤
            optimizer = AdamW(
                model.parameters(),
                lr=genes.get("lr", 0.01),  # ä½¿ç”¨é è¨­å€¼é¿å…KeyError
                weight_decay=genes.get("weight_decay", 0.012),
                betas=genes.get("betas", (0.9, 0.95)),
            )
            scheduler = CosineAnnealingLR(
                optimizer, T_max=epochs, eta_min=genes.get("eta_min", 8e-6)
            )
        except Exception as e:
            logger.error(f"å„ªåŒ–å™¨è¨­ç½®å¤±æ•—: {e}")
            return 10.0, 60.0, 0.5  # è¿”å›åŸºç·šçµæœ

        start_time = time.time()
        epoch_accuracies = []

        for epoch in range(epochs):
            try:
                model.train()
                epoch_loss = 0.0
                batch_count = 0

                for batch_idx, (inputs, targets) in enumerate(trainloader):
                    if batch_idx >= max_batches_per_epoch:
                        break

                    try:
                        inputs, targets = inputs.to(device), targets.to(device)
                        optimizer.zero_grad()

                        # ä½¿ç”¨é è¨­çš„ Mixup è¨­å®š
                        if genes.get("use_mixup", True):
                            alpha = genes.get("alpha", 0.1)
                            lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0
                            batch_size = inputs.size()[0]
                            index = torch.randperm(batch_size).to(device)
                            mixed_x = lam * inputs + (1 - lam) * inputs[index, :]
                            y_a, y_b = targets, targets[index]

                            outputs = model(mixed_x)
                            loss = lam * criterion(outputs, y_a) + (
                                1 - lam
                            ) * criterion(outputs, y_b)
                        else:
                            outputs = model(inputs)
                            loss = criterion(outputs, targets)

                        loss.backward()

                        # ä½¿ç”¨é è¨­çš„æ¢¯åº¦è£å‰ªå€¼
                        torch.nn.utils.clip_grad_norm_(
                            model.parameters(), max_norm=genes.get("gradient_clip", 0.8)
                        )

                        optimizer.step()
                        epoch_loss += loss.item()
                        batch_count += 1

                    except RuntimeError as e:
                        error_msg = str(e).lower()
                        if "out of memory" in error_msg:
                            logger.error("GPUè¨˜æ†¶é«”ä¸è¶³ï¼Œè·³éæ­¤æ‰¹æ¬¡")
                            torch.cuda.empty_cache()
                            continue
                        elif (
                            "size of tensor" in error_msg and "must match" in error_msg
                        ):
                            # å¼µé‡å°ºå¯¸ä¸åŒ¹é…éŒ¯èª¤ï¼Œé€™å€‹æ¨¡å‹é…ç½®æœ‰å•é¡Œ
                            logger.error(f"å¼µé‡å°ºå¯¸ä¸åŒ¹é…éŒ¯èª¤: {e}")
                            logger.error("æ­¤æ¨¡å‹é…ç½®å­˜åœ¨çµæ§‹å•é¡Œï¼Œçµ‚æ­¢è¨“ç·´")
                            return 5.0, time.time() - start_time, 0.1  # è¿”å›æ¥µä½åˆ†æ•¸
                        else:
                            logger.warning(f"è¨“ç·´æ‰¹æ¬¡å¤±æ•—: {e}")
                            continue

                scheduler.step()

                # æ¯å€‹epochå¾Œè©•ä¼°ä¸€æ¬¡
                try:
                    epoch_acc = self.evaluate_accuracy(
                        model, testloader, device, max_batches=5
                    )
                    epoch_accuracies.append(epoch_acc)

                    # é¡¯ç¤ºæ¯å€‹ epoch çš„é€²åº¦
                    logger.info(
                        f"   å¿«é€Ÿè¨“ç·´: epoch {epoch+1}/{epochs}, æº–ç¢ºç‡: {epoch_acc:.1f}%"
                    )
                except Exception as e:
                    logger.warning(f"æº–ç¢ºç‡è©•ä¼°å¤±æ•—: {e}")
                    epoch_accuracies.append(10.0)

            except Exception as e:
                error_msg = str(e).lower()
                if "size of tensor" in error_msg and "must match" in error_msg:
                    logger.error(f"è¨“ç·´epoch {epoch+1} é‡åˆ°å¼µé‡å°ºå¯¸éŒ¯èª¤: {e}")
                    logger.error("æ¨¡å‹é…ç½®å­˜åœ¨å•é¡Œï¼Œæå‰çµ‚æ­¢æ­¤å€‹é«”çš„è©•ä¼°")
                    # ç‚ºå‰©é¤˜çš„ epochs å¡«å……ä½åˆ†
                    remaining_epochs = epochs - epoch
                    epoch_accuracies.extend([5.0] * remaining_epochs)
                    break  # è·³å‡º epoch å¾ªç’°
                else:
                    logger.warning(f"è¨“ç·´epoch {epoch+1} å¤±æ•—: {e}")
                    epoch_accuracies.append(10.0)
                    continue

        training_time = time.time() - start_time

        # æœ€çµ‚æº–ç¢ºç‡è©•ä¼°
        try:
            final_accuracy = self.evaluate_accuracy(
                model, testloader, device, max_batches=8
            )
        except Exception as e:
            logger.warning(f"æœ€çµ‚æº–ç¢ºç‡è©•ä¼°å¤±æ•—: {e}")
            final_accuracy = max(epoch_accuracies) if epoch_accuracies else 10.0

        # è¨ˆç®—ç©©å®šæ€§åˆ†æ•¸
        if len(epoch_accuracies) > 1:
            stability_score = 1.0 - min(np.std(epoch_accuracies) / 100.0, 0.5)
        else:
            stability_score = 0.5

        return final_accuracy, training_time, max(0.0, min(1.0, stability_score))

    def evaluate_accuracy(
        self, model, testloader, device, max_batches: int = 20
    ) -> float:
        """è©•ä¼°æº–ç¢ºç‡"""
        model.eval()
        correct = 0
        total = 0

        try:
            with torch.no_grad():
                for batch_idx, (inputs, targets) in enumerate(testloader):
                    if batch_idx >= max_batches:
                        break
                    try:
                        inputs, targets = inputs.to(device), targets.to(device)
                        outputs = model(inputs)
                        _, predicted = outputs.max(1)
                        total += targets.size(0)
                        correct += predicted.eq(targets).sum().item()
                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            torch.cuda.empty_cache()
                            continue
                        else:
                            logger.warning(f"è©•ä¼°æ‰¹æ¬¡å¤±æ•—: {e}")
                            continue
        except Exception as e:
            logger.warning(f"æº–ç¢ºç‡è©•ä¼°éç¨‹å¤±æ•—: {e}")
            return 10.0  # è¿”å›åŸºç·šæº–ç¢ºç‡

        return 100.0 * correct / total if total > 0 else 10.0  # ç¢ºä¿ä¸è¿”å›0

    def calculate_fitness(
        self, accuracy: float, params: int, training_time: float, stability: float
    ) -> float:
        """è¨ˆç®—é©æ‡‰åº¦å‡½æ•¸ - æ”¹é€²ç‰ˆ"""
        # æ­£è¦åŒ–æŒ‡æ¨™
        acc_norm = min(1.0, accuracy / 100.0)  # æº–ç¢ºç‡æ­£è¦åŒ–

        # æ•ˆç‡æŒ‡æ¨™ (åƒæ•¸è¶Šå°‘è¶Šå¥½ï¼Œè¨“ç·´æ™‚é–“è¶ŠçŸ­è¶Šå¥½)
        param_efficiency = 1.0 / (1.0 + params / 1e6)  # 1Måƒæ•¸ç‚ºåŸºæº–
        time_efficiency = 1.0 / (1.0 + training_time / 30.0)  # 30ç§’ç‚ºåŸºæº–
        efficiency = (param_efficiency + time_efficiency) / 2.0

        # ç©©å®šæ€§å·²ç¶“æ­£è¦åŒ–
        stability_norm = max(0.0, min(1.0, stability))

        # ç¶œåˆé©æ‡‰åº¦
        fitness = (
            self.weights["accuracy"] * acc_norm
            + self.weights["efficiency"] * efficiency
            + self.weights["stability"] * stability_norm
        )

        # æ·»åŠ æº–ç¢ºç‡é–¾å€¼çå‹µ
        if accuracy > 80.0:
            fitness += 0.1  # æº–ç¢ºç‡è¶…é80%çµ¦é¡å¤–çå‹µ
        if accuracy > 90.0:
            fitness += 0.1  # æº–ç¢ºç‡è¶…é90%çµ¦æ›´å¤šçå‹µ

        return fitness


class GeneticOperators:
    """éºå‚³æ“ä½œå™¨ - åŸºæ–¼ optimizer.py çš„è¨­è¨ˆ"""

    def __init__(self, config: NetworkConfig):
        self.config = config

    def tournament_selection(
        self,
        population: List[Individual],
        tournament_size: int = 3,
        num_select: int = 2,
    ) -> List[Individual]:
        """éŒ¦æ¨™è³½é¸æ“‡"""
        selected = []
        for _ in range(num_select):
            tournament = random.sample(
                population, min(tournament_size, len(population))
            )
            winner = max(tournament, key=lambda x: x.fitness)
            selected.append(copy.deepcopy(winner))
        return selected

    def roulette_selection(
        self, population: List[Individual], num_select: int = 2
    ) -> List[Individual]:
        """è¼ªç›¤è³­é¸æ“‡"""
        # ç¢ºä¿æ‰€æœ‰é©æ‡‰åº¦éƒ½æ˜¯æ­£æ•¸
        min_fitness = min(ind.fitness for ind in population)
        if min_fitness < 0:
            adjusted_fitness = [ind.fitness - min_fitness + 0.01 for ind in population]
        else:
            adjusted_fitness = [ind.fitness + 0.01 for ind in population]

        total_fitness = sum(adjusted_fitness)
        probabilities = [f / total_fitness for f in adjusted_fitness]

        selected = []
        for _ in range(num_select):
            r = random.random()
            cumsum = 0
            for i, prob in enumerate(probabilities):
                cumsum += prob
                if r <= cumsum:
                    selected.append(copy.deepcopy(population[i]))
                    break

        return selected

    def uniform_crossover(
        self, parent1: Individual, parent2: Individual, crossover_rate: float = 0.7
    ) -> Tuple[Individual, Individual]:
        """å‡å‹»äº¤å‰ - åªå°å¯è®ŠåŸºå› é€²è¡Œäº¤å‰"""
        if random.random() > crossover_rate:
            return copy.deepcopy(parent1), copy.deepcopy(parent2)

        # è¤‡è£½æ‰€æœ‰åŸºå› 
        child1_genes = copy.deepcopy(parent1.genes)
        child2_genes = copy.deepcopy(parent2.genes)

        # åªå°å¯è®ŠåŸºå› é€²è¡Œäº¤å‰
        mutable_genes = [
            gene for gene in parent1.genes.keys() if gene in self.config.gene_ranges
        ]

        for gene_name in mutable_genes:
            if random.random() < 0.5:
                # äº¤æ›åŸºå› å€¼
                child1_genes[gene_name] = parent2.genes[gene_name]
                child2_genes[gene_name] = parent1.genes[gene_name]

        child1 = Individual(genes=child1_genes, parent_ids=[parent1.id, parent2.id])
        child2 = Individual(genes=child2_genes, parent_ids=[parent1.id, parent2.id])

        return child1, child2

    def adaptive_mutation(
        self,
        individual: Individual,
        mutation_rate: float = 0.3,
        generation: int = 0,
        max_generations: int = 100,
    ) -> Individual:
        """è‡ªé©æ‡‰çªè®Š - åªçªè®Šå¯è®ŠåŸºå› """
        # æ ¹æ“šä¸–ä»£èª¿æ•´çªè®Šç‡
        adaptive_rate = mutation_rate * (1.0 - generation / max_generations * 0.5)

        if random.random() > adaptive_rate:
            return individual

        # åªå¾å¯è®ŠåŸºå› ä¸­é¸æ“‡è¦çªè®Šçš„åŸºå› 
        mutable_genes = [
            gene for gene in individual.genes.keys() if gene in self.config.gene_ranges
        ]

        if not mutable_genes:
            return individual

        # é¸æ“‡çªè®Šçš„åŸºå› æ•¸é‡
        num_genes_to_mutate = max(1, int(len(mutable_genes) * adaptive_rate))
        genes_to_mutate = random.sample(
            mutable_genes, min(num_genes_to_mutate, len(mutable_genes))
        )

        for gene_name in genes_to_mutate:
            old_value = individual.genes[gene_name]
            individual.genes[gene_name] = self.config.mutate_gene(gene_name, old_value)
            individual.mutation_history.append(
                f"G{generation}: {gene_name} {old_value} -> {individual.genes[gene_name]}"
            )

        return individual


class AdvancedGeneticOptimizerGMLP:
    """é€²éšéºå‚³ç®—æ³•å„ªåŒ–å™¨ - æ•´åˆæ‰€æœ‰æ”¹é€²"""

    def __init__(
        self,
        population_size: int = 20,
        generations: int = 10,
        mutation_rate: float = 0.3,
        crossover_rate: float = 0.7,
        elite_ratio: float = 0.2,
        selection_method: str = "tournament",
        parallel_evaluation: bool = False,
    ):

        self.population_size = population_size
        self.generations = generations
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.elite_size = int(population_size * elite_ratio)
        self.selection_method = selection_method
        self.parallel_evaluation = parallel_evaluation

        # åˆå§‹åŒ–çµ„ä»¶
        self.config = NetworkConfig()
        self.evaluator = FitnessEvaluator()
        self.operators = GeneticOperators(self.config)

        # æ­·å²è¨˜éŒ„å’Œçµ±è¨ˆ
        self.history = {
            "best_fitness": [],
            "avg_fitness": [],
            "best_accuracy": [],
            "population_diversity": [],
            "best_individuals": [],
            "convergence_data": [],
        }

        self.statistics = {
            "total_evaluations": 0,
            "cache_hits": 0,
            "best_ever_individual": None,
        }

    def create_individual(self, generation: int = 0) -> Individual:
        """å‰µå»ºä¸€å€‹éš¨æ©Ÿå€‹é«” - åªåŒ…å«å¯è®Šçš„æ¶æ§‹åŸºå› """
        genes = {}

        # æ·»åŠ å¯è®Šçš„æ¨¡å‹æ¶æ§‹åŸºå› 
        for gene_name in self.config.gene_ranges.keys():
            genes[gene_name] = self.config.get_random_gene(gene_name)

        # æ·»åŠ å›ºå®šçš„è¨“ç·´åƒæ•¸
        genes.update(self.config.fixed_params)

        return Individual(genes=genes, generation=generation)

    def initialize_population(self, generation: int = 0) -> List[Individual]:
        """åˆå§‹åŒ–ç¨®ç¾¤ - æ”¯æŒé‡æ–°é–‹å§‹"""
        logger.info(f"åˆå§‹åŒ–ç¨®ç¾¤ (å¤§å°: {self.population_size})")

        population = []
        for i in range(self.population_size):
            individual = self.create_individual(generation)
            population.append(individual)

        logger.info(f"å‰µå»ºäº† {len(population)} å€‹å€‹é«”")
        return population

    def evaluate_population(
        self, population: List[Individual], trainloader, testloader, device
    ) -> List[Individual]:
        """è©•ä¼°ç¨®ç¾¤ - æ”¯æŒå¹³è¡Œè™•ç†"""
        if self.parallel_evaluation and len(population) > 4:
            return self._parallel_evaluate(population, trainloader, testloader, device)
        else:
            return self._sequential_evaluate(
                population, trainloader, testloader, device
            )

    def _sequential_evaluate(
        self, population: List[Individual], trainloader, testloader, device
    ) -> List[Individual]:
        """é †åºè©•ä¼°"""
        successful_evaluations = 0
        failed_evaluations = 0

        for i, individual in enumerate(population):
            try:
                logger.info(
                    f"è©•ä¼°å€‹é«” {i+1}/{len(population)} (æˆåŠŸ:{successful_evaluations}, å¤±æ•—:{failed_evaluations})"
                )
                cache_key = str(hash(str(sorted(individual.genes.items()))))
                population[i] = self.evaluator.evaluate(
                    individual, trainloader, testloader, device, cache_key
                )
                self.statistics["total_evaluations"] += 1
                successful_evaluations += 1

                # æ¯5å€‹å€‹é«”é¡¯ç¤ºä¸€æ¬¡é€²åº¦
                if (i + 1) % 5 == 0 or i == len(population) - 1:
                    logger.info(
                        f"ğŸ“Š é€²åº¦: {i+1}/{len(population)} ({((i+1)/len(population)*100):.1f}%)"
                    )

            except KeyboardInterrupt:
                logger.info(f"âš ï¸  è©•ä¼°åœ¨ç¬¬ {i+1} å€‹å€‹é«”æ™‚è¢«ç”¨æˆ¶ä¸­æ–·")
                # çµ¦å‰©é¤˜æœªè©•ä¼°çš„å€‹é«”è¨­ç½®é»˜èªé©æ‡‰åº¦
                for j in range(i, len(population)):
                    if population[j].fitness == 0.0:
                        population[j].fitness = 0.001  # è¨­ç½®ä¸€å€‹å¾ˆå°çš„é©æ‡‰åº¦å€¼
                break
            except Exception as e:
                logger.error(f"è©•ä¼°ç¬¬ {i+1} å€‹å€‹é«”æ™‚å‡ºéŒ¯: {e}")
                failed_evaluations += 1
                population[i].fitness = 0.001  # è¨­ç½®ä¸€å€‹å¾ˆå°çš„é©æ‡‰åº¦å€¼
                continue

        logger.info(
            f"è©•ä¼°å®Œæˆ: æˆåŠŸ {successful_evaluations}, å¤±æ•— {failed_evaluations}"
        )
        return population

    def _parallel_evaluate(
        self, population: List[Individual], trainloader, testloader, device
    ) -> List[Individual]:
        """å¹³è¡Œè©•ä¼° (å¯¦é©—æ€§åŠŸèƒ½)"""
        logger.info("ä½¿ç”¨å¹³è¡Œè©•ä¼°æ¨¡å¼")
        # æ³¨æ„: ç”±æ–¼ PyTorch æ¨¡å‹å’Œ CUDA çš„é™åˆ¶ï¼Œå¹³è¡Œè©•ä¼°å¯èƒ½æœ‰å•é¡Œ
        # é€™è£¡ä¿ç•™æ¥å£ï¼Œå¯¦éš›ä½¿ç”¨æ™‚å»ºè­°ç”¨é †åºè©•ä¼°
        return self._sequential_evaluate(population, trainloader, testloader, device)

    def calculate_diversity(self, population: List[Individual]) -> float:
        """è¨ˆç®—ç¨®ç¾¤å¤šæ¨£æ€§"""
        if len(population) < 2:
            return 0.0

        total_distance = 0.0
        comparisons = 0

        for i in range(len(population)):
            for j in range(i + 1, len(population)):
                distance = self._calculate_individual_distance(
                    population[i], population[j]
                )
                total_distance += distance
                comparisons += 1

        return total_distance / comparisons if comparisons > 0 else 0.0

    def _calculate_individual_distance(
        self, ind1: Individual, ind2: Individual
    ) -> float:
        """è¨ˆç®—å…©å€‹å€‹é«”ä¹‹é–“çš„è·é›¢ - åªè€ƒæ…®å¯è®ŠåŸºå› """
        distance = 0.0
        gene_count = 0

        # åªè¨ˆç®—å¯è®ŠåŸºå› çš„è·é›¢ï¼ˆæ¨¡å‹æ¶æ§‹åŸºå› ï¼‰
        for gene_name in self.config.gene_ranges.keys():
            if gene_name in ind1.genes and gene_name in ind2.genes:
                config = self.config.gene_ranges[gene_name]
                if config["type"] in ["int", "float"]:
                    # æ•¸å€¼åŸºå› ï¼šæ­£è¦åŒ–å·®ç•°
                    range_size = config["range"][1] - config["range"][0]
                    diff = (
                        abs(ind1.genes[gene_name] - ind2.genes[gene_name]) / range_size
                    )
                    distance += diff
                else:
                    # åˆ†é¡æˆ–å¸ƒæ—åŸºå› ï¼šä¸åŒç‚º1ï¼Œç›¸åŒç‚º0
                    distance += (
                        0 if ind1.genes[gene_name] == ind2.genes[gene_name] else 1
                    )
                gene_count += 1

        return distance / gene_count if gene_count > 0 else 0.0

    def optimize(self, trainloader, testloader, device) -> Individual:
        """ä¸»è¦å„ªåŒ–æµç¨‹ - é€²éšç‰ˆ"""
        logger.info("é–‹å§‹é€²éšéºå‚³ç®—æ³•å„ªåŒ– gMLP")
        logger.info(
            f"é…ç½®: ç¨®ç¾¤={self.population_size}, ä¸–ä»£={self.generations}, çªè®Šç‡={self.mutation_rate}"
        )

        # åˆå§‹åŒ–ç¨®ç¾¤
        population = self.initialize_population()
        best_individual_so_far = None

        # é€²åŒ–å¾ªç’°
        for generation in range(self.generations):
            logger.info(f"\nğŸ§¬ ç¬¬ {generation + 1}/{self.generations} ä¸–ä»£")

            try:
                # è©•ä¼°é©æ‡‰åº¦
                population = self.evaluate_population(
                    population, trainloader, testloader, device
                )

                # æ’åºç¨®ç¾¤
                population.sort(key=lambda x: x.fitness, reverse=True)

                # è¨˜éŒ„çµ±è¨ˆ
                best_individual = population[0]
                best_individual_so_far = best_individual  # ä¿å­˜ç•¶å‰æœ€ä½³å€‹é«”
                avg_fitness = sum(ind.fitness for ind in population) / len(population)
                diversity = self.calculate_diversity(population)

                # æ›´æ–°æœ€ä½³å€‹é«”è¨˜éŒ„
                if (
                    self.statistics["best_ever_individual"] is None
                    or best_individual.fitness
                    > self.statistics["best_ever_individual"].fitness
                ):
                    self.statistics["best_ever_individual"] = copy.deepcopy(
                        best_individual
                    )

                # è¨˜éŒ„æ­·å²
                self.history["best_fitness"].append(best_individual.fitness)
                self.history["avg_fitness"].append(avg_fitness)
                self.history["best_accuracy"].append(best_individual.accuracy)
                self.history["population_diversity"].append(diversity)
                self.history["best_individuals"].append(copy.deepcopy(best_individual))

                # æ”¶æ–‚æª¢æ¸¬
                convergence_measure = self._check_convergence()
                self.history["convergence_data"].append(convergence_measure)

                logger.info(f"ğŸ† æœ€ä½³é©æ‡‰åº¦: {best_individual.fitness:.4f}")
                logger.info(f"ğŸ“ˆ æœ€ä½³æº–ç¢ºç‡: {best_individual.accuracy:.2f}%")
                logger.info(f"ğŸ”€ ç¨®ç¾¤å¤šæ¨£æ€§: {diversity:.4f}")
                logger.info(f"ğŸ“Š å¹³å‡é©æ‡‰åº¦: {avg_fitness:.4f}")
                logger.info(f"ğŸ“‰ æ”¶æ–‚ç¨‹åº¦: {convergence_measure:.4f}")

                # æ—©åœæª¢æŸ¥
                if self._should_early_stop(generation):
                    logger.info("ğŸ›‘ è§¸ç™¼æ—©åœæ¢ä»¶ï¼Œæå‰çµæŸå„ªåŒ–")
                    break

                if generation < self.generations - 1:
                    # ç”Ÿæˆä¸‹ä¸€ä»£
                    population = self._generate_next_generation(population, generation)

            except KeyboardInterrupt:
                logger.info(f"âš ï¸  ç¬¬ {generation + 1} ä¸–ä»£è¢«ç”¨æˆ¶ä¸­æ–·")
                if best_individual_so_far:
                    self.statistics["best_ever_individual"] = copy.deepcopy(
                        best_individual_so_far
                    )
                break
            except Exception as e:
                logger.error(f"ç¬¬ {generation + 1} ä¸–ä»£è©•ä¼°å‡ºéŒ¯: {e}")
                if best_individual_so_far:
                    self.statistics["best_ever_individual"] = copy.deepcopy(
                        best_individual_so_far
                    )
                break

        # è¿”å›æœ€ä½³å€‹é«”
        best_individual = self.statistics["best_ever_individual"] or (
            best_individual_so_far
            if best_individual_so_far
            else population[0] if population else None
        )

        if best_individual:
            logger.info(f"\nğŸ‰ å„ªåŒ–å®Œæˆï¼")
            logger.info(f"ğŸ† æœ€ä½³é…ç½®: {best_individual.genes}")
            logger.info(f"ğŸ“ˆ æœ€ä½³æº–ç¢ºç‡: {best_individual.accuracy:.2f}%")
            logger.info(f"ğŸ¯ æœ€ä½³é©æ‡‰åº¦: {best_individual.fitness:.4f}")
            logger.info(f"ï¿½ æ¨¡å‹åƒæ•¸: {best_individual.parameters/1e6:.2f}M")
            logger.info(f"ï¿½ ç¸½è©•ä¼°æ¬¡æ•¸: {self.statistics['total_evaluations']}")
        else:
            logger.warning("âš ï¸  å„ªåŒ–éç¨‹ä¸­æ–·ï¼Œæ²’æœ‰å¯ç”¨çš„æœ€ä½³å€‹é«”")
            # å‰µå»ºä¸€å€‹é»˜èªå€‹é«”
            best_individual = self.create_individual()

        return best_individual

    def _generate_next_generation(
        self, population: List[Individual], generation: int
    ) -> List[Individual]:
        """ç”Ÿæˆä¸‹ä¸€ä»£"""
        logger.info("ğŸ§¬ ç”Ÿæˆä¸‹ä¸€ä»£å€‹é«”...")

        # ä¿ç•™ç²¾è‹±
        new_population = population[: self.elite_size]
        logger.info(f"ä¿ç•™ {self.elite_size} å€‹ç²¾è‹±å€‹é«”")

        # ç”Ÿæˆå­ä»£
        while len(new_population) < self.population_size:
            # é¸æ“‡çˆ¶æ¯
            if self.selection_method == "tournament":
                parents = self.operators.tournament_selection(population, num_select=2)
            else:  # roulette
                parents = self.operators.roulette_selection(population, num_select=2)

            # äº¤å‰
            child1, child2 = self.operators.uniform_crossover(
                parents[0], parents[1], self.crossover_rate
            )

            # çªè®Š
            child1 = self.operators.adaptive_mutation(
                child1, self.mutation_rate, generation, self.generations
            )
            child2 = self.operators.adaptive_mutation(
                child2, self.mutation_rate, generation, self.generations
            )

            # è¨­ç½®ä¸–ä»£
            child1.generation = generation + 1
            child2.generation = generation + 1

            new_population.extend([child1, child2])

        # ç¢ºä¿ç¨®ç¾¤å¤§å°æ­£ç¢º
        return new_population[: self.population_size]

    def _check_convergence(self) -> float:
        """æª¢æŸ¥æ”¶æ–‚ç¨‹åº¦"""
        if len(self.history["best_fitness"]) < 5:
            return 0.0

        recent_fitness = self.history["best_fitness"][-5:]
        fitness_std = np.std(recent_fitness)

        # æ”¶æ–‚ç¨‹åº¦ï¼šæ¨™æº–å·®è¶Šå°ï¼Œæ”¶æ–‚ç¨‹åº¦è¶Šé«˜
        convergence = 1.0 - min(1.0, fitness_std * 10)
        return convergence

    def _should_early_stop(self, generation: int) -> bool:
        """æ—©åœåˆ¤æ–·"""
        if generation < 5:  # è‡³å°‘é‹è¡Œ5ä»£
            return False

        # å¦‚æœé€£çºŒ5ä»£æœ€ä½³é©æ‡‰åº¦æ²’æœ‰æå‡ï¼Œè€ƒæ…®æ—©åœ
        if len(self.history["best_fitness"]) >= 5:
            recent_best = self.history["best_fitness"][-5:]
            if all(abs(recent_best[i] - recent_best[0]) < 1e-4 for i in range(1, 5)):
                return True

        return False

    def plot_optimization_history(self):
        """ç¹ªè£½è©³ç´°çš„å„ªåŒ–æ­·å²"""
        if not self.history["best_fitness"]:
            logger.warning("æ²’æœ‰è¶³å¤ çš„å„ªåŒ–æ­·å²æ•¸æ“šé€²è¡Œç¹ªè£½")
            print("âš ï¸  å„ªåŒ–æ­·å²ä¸è¶³ï¼Œç„¡æ³•ç¹ªè£½åœ–è¡¨")
            return

        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            generations = range(1, len(self.history["best_fitness"]) + 1)

            # é©æ‡‰åº¦é€²åŒ–
            ax1.plot(
                generations,
                self.history["best_fitness"],
                "r-",
                linewidth=2,
                label="Best Fitness",
            )
            ax1.plot(
                generations,
                self.history["avg_fitness"],
                "b--",
                linewidth=2,
                label="Average Fitness",
            )
            ax1.set_title("Fitness Evolution", fontweight="bold", fontsize=14)
            ax1.set_xlabel("Generation")
            ax1.set_ylabel("Fitness")
            ax1.legend()
            ax1.grid(True, alpha=0.3)

            # æº–ç¢ºç‡é€²åŒ–
            ax2.plot(
                generations,
                self.history["best_accuracy"],
                "g-",
                linewidth=3,
                marker="o",
                markersize=5,
            )
            ax2.set_title("Best Accuracy Evolution", fontweight="bold", fontsize=14)
            ax2.set_xlabel("Generation")
            ax2.set_ylabel("Accuracy (%)")
            ax2.grid(True, alpha=0.3)
            ax2.set_ylim(0, 100)

            # ç¨®ç¾¤å¤šæ¨£æ€§å’Œæ”¶æ–‚
            ax3.plot(
                generations,
                self.history["population_diversity"],
                "purple",
                linewidth=2,
                label="Diversity",
            )
            ax3_twin = ax3.twinx()
            ax3_twin.plot(
                generations,
                self.history["convergence_data"],
                "orange",
                linewidth=2,
                label="Convergence",
            )
            ax3.set_title("Diversity & Convergence", fontweight="bold", fontsize=14)
            ax3.set_xlabel("Generation")
            ax3.set_ylabel("Diversity", color="purple")
            ax3_twin.set_ylabel("Convergence", color="orange")
            ax3.grid(True, alpha=0.3)

            # åƒæ•¸æ•¸é‡é€²åŒ–
            param_counts = [
                ind.parameters / 1e6 for ind in self.history["best_individuals"]
            ]
            ax4.plot(
                generations,
                param_counts,
                "brown",
                linewidth=2,
                marker="s",
                markersize=4,
            )
            ax4.set_title("Best Model Size Evolution", fontweight="bold", fontsize=14)
            ax4.set_xlabel("Generation")
            ax4.set_ylabel("Parameters (M)")
            ax4.grid(True, alpha=0.3)

            plt.tight_layout()
            plt.savefig(
                "advanced_genetic_optimization_history.png",
                dpi=300,
                bbox_inches="tight",
            )
            plt.show()

            # é¡å¤–çµ±è¨ˆåœ–
            self._plot_gene_evolution()

        except Exception as e:
            logger.error(f"ç¹ªè£½å„ªåŒ–æ­·å²æ™‚å‡ºéŒ¯: {e}")
            print(f"âš ï¸  ç„¡æ³•ç¹ªè£½å„ªåŒ–æ­·å²: {e}")

    def _plot_gene_evolution(self):
        """ç¹ªè£½åŸºå› é€²åŒ–æ­·å²"""
        if not self.history["best_individuals"]:
            return

        # é¸æ“‡æ¶æ§‹åŸºå› é€²è¡Œå¯è¦–åŒ–
        key_genes = ["depth", "dim", "ff_mult", "prob_survival"]
        available_genes = [
            g for g in key_genes if g in self.history["best_individuals"][0].genes
        ]

        if not available_genes:
            return

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()

        generations = range(1, len(self.history["best_individuals"]) + 1)

        for i, gene_name in enumerate(available_genes[:4]):
            gene_values = [
                ind.genes[gene_name] for ind in self.history["best_individuals"]
            ]
            axes[i].plot(
                generations, gene_values, linewidth=2, marker="o", markersize=4
            )
            axes[i].set_title(f"{gene_name.title()} Evolution", fontweight="bold")
            axes[i].set_xlabel("Generation")
            axes[i].set_ylabel(gene_name.title())
            axes[i].grid(True, alpha=0.3)

            # ä¸éœ€è¦å°æ•¸å°ºåº¦ï¼Œå› ç‚ºç¾åœ¨åªæœ‰æ¶æ§‹åƒæ•¸

        plt.tight_layout()
        plt.savefig("gene_evolution_history.png", dpi=300, bbox_inches="tight")
        plt.show()

    def save_results(self, best_individual: Individual, filename: str = None):
        """ä¿å­˜è©³ç´°çµæœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"advanced_genetic_optimization_{timestamp}.json"

        # è½‰æ› Individual å°è±¡ç‚ºå­—å…¸æ ¼å¼
        serializable_history = {
            "best_fitness": [float(x) for x in self.history["best_fitness"]],
            "avg_fitness": [float(x) for x in self.history["avg_fitness"]],
            "best_accuracy": [float(x) for x in self.history["best_accuracy"]],
            "population_diversity": [
                float(x) for x in self.history["population_diversity"]
            ],
            "best_individuals": [
                ind.to_dict() for ind in self.history["best_individuals"]
            ],
            "convergence_data": [float(x) for x in self.history["convergence_data"]],
        }

        results = {
            "best_individual": best_individual.to_dict(),
            "optimization_config": {
                "population_size": self.population_size,
                "generations": self.generations,
                "mutation_rate": self.mutation_rate,
                "crossover_rate": self.crossover_rate,
                "selection_method": self.selection_method,
                "fitness_weights": self.evaluator.weights,
            },
            "statistics": {
                "total_evaluations": int(self.statistics["total_evaluations"]),
                "cache_hits": int(self.statistics["cache_hits"]),
                "final_diversity": float(
                    self.history["population_diversity"][-1]
                    if self.history["population_diversity"]
                    else 0.0
                ),
            },
            "history": serializable_history,
            "gene_ranges": {
                k: {
                    kk: (list(vv) if isinstance(vv, tuple) else vv)
                    for kk, vv in v.items()
                    if kk != "log_scale" and not callable(vv)
                }
                for k, v in self.config.gene_ranges.items()
            },
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"è©³ç´°çµæœå·²ä¿å­˜åˆ°: {filename}")

        # åŒæ™‚ä¿å­˜äºŒé€²åˆ¶æ ¼å¼ä»¥ä¿ç•™å®Œæ•´å°è±¡
        pickle_filename = filename.replace(".json", ".pkl")
        with open(pickle_filename, "wb") as f:
            pickle.dump(
                {
                    "best_individual": best_individual,
                    "optimizer": self,
                    "results": results,
                },
                f,
            )

        logger.info(f"å®Œæ•´å°è±¡å·²ä¿å­˜åˆ°: {pickle_filename}")


# å…¼å®¹æ€§åŒ…è£å™¨
class GeneticOptimizerGMLP(AdvancedGeneticOptimizerGMLP):
    """å…¼å®¹æ€§åŒ…è£å™¨ï¼Œä¿æŒåŸæœ‰æ¥å£"""

    pass


def run_genetic_optimization():
    """é‹è¡Œé€²éšéºå‚³ç®—æ³•å„ªåŒ–"""
    import sys
    import os

    sys.path.append(os.path.dirname(os.path.abspath(__file__)))

    try:
        from model_16 import load_cifar10_data_enhanced
    except ImportError:
        logger.error("ç„¡æ³•å°å…¥ model_16 æ¨¡çµ„ï¼Œè«‹ç¢ºèªæ–‡ä»¶å­˜åœ¨")
        return

    print("ğŸ§¬ é€²éšéºå‚³ç®—æ³•å„ªåŒ– gMLP æ¨¡å‹")
    print("=" * 60)

    # ç”¨æˆ¶é…ç½® - ä½¿ç”¨æ›´ä¿å®ˆçš„é è¨­å€¼
    try:
        population_size = int(input("ç¨®ç¾¤å¤§å° (é è¨­=8): ") or "8")  # æ¸›å°‘ç¨®ç¾¤å¤§å°
        generations = int(input("é€²åŒ–ä¸–ä»£æ•¸ (é è¨­=5): ") or "5")  # æ¸›å°‘ä¸–ä»£æ•¸
        mutation_rate = float(input("çªè®Šç‡ (é è¨­=0.3): ") or "0.3")

        selection_method = (
            input("é¸æ“‡æ–¹æ³• (tournament/roulette, é è¨­=tournament): ").strip()
            or "tournament"
        )
        parallel_eval = input("ä½¿ç”¨å¹³è¡Œè©•ä¼°? (y/n, é è¨­=n): ").strip().lower() == "y"

        print(f"\nğŸ¯ é€²éšå„ªåŒ–é…ç½®:")
        print(f"   ç¨®ç¾¤å¤§å°: {population_size}")
        print(f"   ä¸–ä»£æ•¸: {generations}")
        print(f"   çªè®Šç‡: {mutation_rate}")
        print(f"   é¸æ“‡æ–¹æ³•: {selection_method}")
        print(f"   å¹³è¡Œè©•ä¼°: {parallel_eval}")

    except ValueError:
        logger.warning("è¼¸å…¥éŒ¯èª¤ï¼Œä½¿ç”¨é è¨­é…ç½®")
        population_size, generations, mutation_rate = 8, 5, 0.3
        selection_method, parallel_eval = "tournament", False

    # åŠ è¼‰æ•¸æ“š
    print("\nğŸ“¦ åŠ è¼‰æ•¸æ“š...")
    trainloader, testloader, classes = load_cifar10_data_enhanced(
        quick_test=True, use_mixup_transform=False
    )

    # è¨­ç½®è¨­å‚™
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   ğŸ–¥ï¸  ä½¿ç”¨è¨­å‚™: {device}")

    # å‰µå»ºé€²éšå„ªåŒ–å™¨
    optimizer = AdvancedGeneticOptimizerGMLP(
        population_size=population_size,
        generations=generations,
        mutation_rate=mutation_rate,
        crossover_rate=0.7,
        elite_ratio=0.2,
        selection_method=selection_method,
        parallel_evaluation=parallel_eval,
    )

    # é–‹å§‹å„ªåŒ–
    try:
        start_time = time.time()
        best_individual = optimizer.optimize(trainloader, testloader, device)
        end_time = time.time()

        print(f"\nâ±ï¸ ç¸½å„ªåŒ–æ™‚é–“: {(end_time - start_time)/60:.1f} åˆ†é˜")

        # ç¹ªè£½è©³ç´°æ­·å²
        print("\nğŸ“ˆ ç”Ÿæˆå„ªåŒ–æ­·å²åœ–è¡¨...")
        optimizer.plot_optimization_history()

        # ä¿å­˜çµæœ
        print("\nğŸ’¾ ä¿å­˜å„ªåŒ–çµæœ...")
        optimizer.save_results(best_individual)

        # è©¢å•æ˜¯å¦é€²è¡Œå®Œæ•´è¨“ç·´
        print(f"\n" + "=" * 60)
        if best_individual and best_individual.fitness > 0:
            use_best = input("ğŸ¯ æ˜¯å¦ä½¿ç”¨æœ€ä½³é…ç½®é€²è¡Œå®Œæ•´è¨“ç·´? (y/n): ").strip().lower()

            if use_best in ["y", "yes"]:
                print("\nğŸš€ é–‹å§‹ä½¿ç”¨æœ€ä½³é…ç½®é€²è¡Œå®Œæ•´è¨“ç·´...")
                train_with_best_config(best_individual, trainloader, testloader, device)
        else:
            print("âš ï¸  æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä½³é…ç½®ï¼Œè·³éå®Œæ•´è¨“ç·´")

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  å„ªåŒ–å·²è¢«ç”¨æˆ¶ä¸­æ–·")
        if "optimizer" in locals() and hasattr(optimizer, "history"):
            print("ğŸ”„ å˜—è©¦ç¹ªè£½å·²æœ‰çš„å„ªåŒ–æ­·å²...")
            try:
                optimizer.plot_optimization_history()
            except Exception as e:
                print(f"âš ï¸  ç„¡æ³•ç¹ªè£½æ­·å²åœ–è¡¨: {e}")
        print("ğŸ’¡ æç¤º: æ‚¨å¯ä»¥èª¿æ•´åƒæ•¸å¾Œé‡æ–°é‹è¡Œ")
    except Exception as e:
        logger.error(f"å„ªåŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        print(f"\nâŒ å„ªåŒ–å¤±æ•—: {e}")
        print("ğŸ’¡ å»ºè­°æª¢æŸ¥:")
        print("   - ç¢ºèª model_16.py æ–‡ä»¶å­˜åœ¨ä¸”å¯å°å…¥")
        print("   - ç¢ºèª g_mlp_pytorch åº«å·²æ­£ç¢ºå®‰è£")
        print("   - æª¢æŸ¥ç³»çµ±å…§å­˜æ˜¯å¦å……è¶³")
        import traceback

        traceback.print_exc()


def train_with_best_config(
    best_individual: Individual, trainloader, testloader, device
):
    """ä½¿ç”¨æœ€ä½³é…ç½®é€²è¡Œå®Œæ•´è¨“ç·´ - æ”¹é€²ç‰ˆ"""
    try:
        from model_16 import (
            create_custom_gmlp_model,
            train_enhanced,
            evaluate_custom_model,
            plot_enhanced_training_history,
        )
    except ImportError:
        logger.error("ç„¡æ³•å°å…¥è¨“ç·´å‡½æ•¸")
        return

    print("ğŸ‹ï¸ ä½¿ç”¨é€²éšéºå‚³ç®—æ³•å„ªåŒ–çš„æœ€ä½³é…ç½®é€²è¡Œå®Œæ•´è¨“ç·´...")

    # è½‰æ›åŸºå› ç‚ºæ¨¡å‹é…ç½®
    model_config = {
        "depth": int(best_individual.genes["depth"]),
        "dim": int(best_individual.genes["dim"]),
        "ff_mult": int(best_individual.genes["ff_mult"]),
        "prob_survival": float(best_individual.genes["prob_survival"]),
        "attn_dim": int(
            best_individual.genes["dim"]
        ),  # ä½¿ç”¨ dim ä½œç‚º attn_dim çš„é è¨­å€¼
        "estimated_params": best_individual.parameters / 1e6,
    }

    # è½‰æ›åŸºå› ç‚ºè¨“ç·´é…ç½® - ä½¿ç”¨ model_16.py çš„é è¨­å€¼
    training_params = {
        # ä¾†è‡ªéºå‚³ç®—æ³•å„ªåŒ–çš„æ¶æ§‹åƒæ•¸å·²åœ¨ model_config ä¸­
        # é€™è£¡ä½¿ç”¨ model_16.py çš„é è¨­è¨“ç·´é…ç½®
        "lr": best_individual.genes["lr"],
        "weight_decay": best_individual.genes["weight_decay"],
        "epochs": 100,  # å®Œæ•´è¨“ç·´ä½¿ç”¨æ›´å¤šè¼ªæ•¸
        "use_mixup": best_individual.genes["use_mixup"],
        "alpha": best_individual.genes["alpha"],
        "batch_split": 1,  # model_16.py é è¨­å€¼
        "use_enhanced_transform": False,  # ä½¿ç”¨æ¨™æº–è®Šæ›
        "optimizer_type": best_individual.genes["optimizer_type"],
        "scheduler_type": best_individual.genes["scheduler_type"],
        "use_early_stopping": True,  # model_16.py é è¨­å€¼
        "patience": 10,  # model_16.py é è¨­å€¼
        "min_delta": 0.001,  # model_16.py é è¨­å€¼
    }

    print(f"ğŸ“Š æœ€ä½³æ¨¡å‹é…ç½®:")
    for k, v in model_config.items():
        print(f"     {k}: {v}")

    print(f"âš™ï¸  æœ€ä½³è¨“ç·´åƒæ•¸:")
    for k, v in training_params.items():
        print(f"     {k}: {v}")

    # å‰µå»ºä¸¦è¨“ç·´æ¨¡å‹
    model, device = create_custom_gmlp_model(model_config)

    train_result = train_enhanced(
        model, trainloader, testloader, device, training_params
    )

    (
        train_losses,
        train_accs,
        val_accs,
        val_losses,
        epoch_times,
        total_time,
        early_stopped,
        best_epoch,
    ) = train_result

    # å¯è¦–åŒ–çµæœ
    plot_enhanced_training_history(
        train_losses, train_accs, val_accs, val_losses, epoch_times
    )

    # è©•ä¼°æ¨¡å‹
    classes = [
        "Airplane",
        "Automobile",
        "Bird",
        "Cat",
        "Deer",
        "Dog",
        "Frog",
        "Horse",
        "Ship",
        "Truck",
    ]
    final_acc = evaluate_custom_model(model, testloader, device, classes)

    # çµæœåˆ†æ
    prediction_error = abs(final_acc - best_individual.accuracy)

    print(f"\nğŸ‰ é€²éšéºå‚³ç®—æ³•å„ªåŒ–è¨“ç·´å®Œæˆ:")
    print(f"   ğŸ“ˆ æœ€çµ‚æº–ç¢ºç‡: {final_acc:.2f}%")
    print(f"   â±ï¸  è¨“ç·´æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
    print(f"   ğŸ§¬ éºå‚³ç®—æ³•é æ¸¬: {best_individual.accuracy:.2f}%")
    print(f"   ğŸ“Š é æ¸¬èª¤å·®: {prediction_error:.2f}%")
    print(f"   ğŸ¯ é©æ‡‰åº¦åˆ†æ•¸: {best_individual.fitness:.4f}")
    print(f"   ğŸ“¦ æ¨¡å‹åƒæ•¸: {best_individual.parameters/1e6:.2f}M")

    if prediction_error < 5.0:
        print("   âœ… éºå‚³ç®—æ³•é æ¸¬æº–ç¢ºï¼")
    elif prediction_error < 10.0:
        print("   âš ï¸  éºå‚³ç®—æ³•é æ¸¬æœ‰ä¸€å®šèª¤å·®ï¼Œä»åœ¨å¯æ¥å—ç¯„åœ")
    else:
        print("   âŒ éºå‚³ç®—æ³•é æ¸¬èª¤å·®è¼ƒå¤§ï¼Œå¯èƒ½éœ€è¦èª¿æ•´è©•ä¼°ç­–ç•¥")

    # ä¿å­˜æœ€çµ‚çµæœ
    final_results = {
        "genetic_prediction": best_individual.accuracy,
        "final_accuracy": final_acc,
        "prediction_error": prediction_error,
        "training_time": total_time,
        "model_config": model_config,
        "training_params": training_params,
        "genetic_fitness": best_individual.fitness,
    }

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f"final_training_results_{timestamp}.json", "w") as f:
        json.dump(final_results, f, indent=2)

    print(f"   ğŸ“ æœ€çµ‚çµæœå·²ä¿å­˜")


if __name__ == "__main__":
    run_genetic_optimization()

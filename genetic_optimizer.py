"""
éºå‚³ç®—æ³•å„ªåŒ– gMLP æ¨¡å‹
æ”¯æ´è¶…åƒæ•¸å„ªåŒ–ã€æ¶æ§‹æœç´¢å’Œè¨“ç·´ç­–ç•¥å„ªåŒ–
"""

import random
import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Any
import matplotlib.pyplot as plt
import json
from datetime import datetime
from dataclasses import dataclass
from concurrent.futures import ProcessPoolExecutor, as_completed
import copy


@dataclass
class Individual:
    """å€‹é«”é¡åˆ¥ - ä»£è¡¨ä¸€å€‹ gMLP é…ç½®"""

    genes: Dict[str, Any]
    fitness: float = 0.0
    accuracy: float = 0.0
    training_time: float = 0.0
    parameters: int = 0

    def __hash__(self):
        return hash(str(sorted(self.genes.items())))


class GeneticOptimizerGMLP:
    """gMLP éºå‚³ç®—æ³•å„ªåŒ–å™¨"""

    def __init__(
        self,
        population_size: int = 20,
        generations: int = 10,
        mutation_rate: float = 0.3,
        crossover_rate: float = 0.7,
        elite_ratio: float = 0.2,
    ):

        self.population_size = population_size
        self.generations = generations
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.elite_size = int(population_size * elite_ratio)

        # åŸºå› ç·¨ç¢¼ç¯„åœå®šç¾©
        self.gene_ranges = {
            # æ¨¡å‹æ¶æ§‹åŸºå› 
            "depth": (4, 24),  # æ¨¡å‹æ·±åº¦
            "dim": (64, 256),  # åµŒå…¥ç¶­åº¦
            "ff_mult": (2, 6),  # FFN å€æ•¸
            "prob_survival": (0.8, 1.0),  # å­˜æ´»æ©Ÿç‡
            "attn_dim": (32, 128),  # æ³¨æ„åŠ›ç¶­åº¦
            # è¨“ç·´è¶…åƒæ•¸åŸºå› 
            "lr": (1e-4, 5e-2),  # å­¸ç¿’ç‡
            "weight_decay": (1e-6, 1e-1),  # æ¬Šé‡è¡°æ¸›
            "batch_size": (32, 128),  # æ‰¹æ¬¡å¤§å°
            "alpha": (0.05, 0.3),  # Mixup alpha
            # è¨“ç·´ç­–ç•¥åŸºå› 
            "use_mixup": [True, False],  # æ˜¯å¦ä½¿ç”¨ Mixup
            "label_smoothing": (0.0, 0.2),  # æ¨™ç±¤å¹³æ»‘
            "gradient_clip": (0.5, 2.0),  # æ¢¯åº¦è£å‰ª
        }

        # é©æ‡‰åº¦æ¬Šé‡
        self.fitness_weights = {
            "accuracy": 0.7,  # æº–ç¢ºç‡æ¬Šé‡
            "efficiency": 0.2,  # æ•ˆç‡æ¬Šé‡ (åƒæ•¸å°‘/è¨“ç·´å¿«)
            "stability": 0.1,  # ç©©å®šæ€§æ¬Šé‡
        }

        # æ­·å²è¨˜éŒ„
        self.history = {
            "best_fitness": [],
            "avg_fitness": [],
            "best_accuracy": [],
            "population_diversity": [],
            "best_individuals": [],
        }

    def create_individual(self) -> Individual:
        """å‰µå»ºä¸€å€‹éš¨æ©Ÿå€‹é«”"""
        genes = {}

        for gene_name, gene_range in self.gene_ranges.items():
            if isinstance(gene_range, tuple):
                if isinstance(gene_range[0], int):
                    genes[gene_name] = random.randint(gene_range[0], gene_range[1])
                elif isinstance(gene_range[0], float):
                    genes[gene_name] = random.uniform(gene_range[0], gene_range[1])
            elif isinstance(gene_range, list):
                genes[gene_name] = random.choice(gene_range)

        return Individual(genes=genes)

    def initialize_population(self) -> List[Individual]:
        """åˆå§‹åŒ–ç¨®ç¾¤"""
        print(f"ğŸ§¬ åˆå§‹åŒ–ç¨®ç¾¤ (å¤§å°: {self.population_size})...")

        population = []
        for i in range(self.population_size):
            individual = self.create_individual()
            population.append(individual)

        print(f"   âœ“ å‰µå»ºäº† {len(population)} å€‹å€‹é«”")
        return population

    def evaluate_fitness(
        self, individual: Individual, trainloader, testloader, device
    ) -> Individual:
        """è©•ä¼°å€‹é«”é©æ‡‰åº¦"""
        try:
            # 1. å‰µå»ºæ¨¡å‹
            model = self.create_model_from_genes(individual.genes)
            model = model.to(device)

            # 2. è¨ˆç®—åƒæ•¸æ•¸é‡
            total_params = sum(p.numel() for p in model.parameters())
            individual.parameters = total_params

            # 3. å¿«é€Ÿè¨“ç·´è©•ä¼° (æ¸›å°‘è¨“ç·´æ™‚é–“)
            accuracy, training_time = self.quick_train_evaluate(
                model, individual.genes, trainloader, testloader, device
            )

            individual.accuracy = accuracy
            individual.training_time = training_time

            # 4. è¨ˆç®—ç¶œåˆé©æ‡‰åº¦
            individual.fitness = self.calculate_fitness(
                accuracy, total_params, training_time
            )

            print(
                f"   ğŸ“Š å€‹é«”è©•ä¼°: æº–ç¢ºç‡={accuracy:.2f}%, åƒæ•¸={total_params/1e6:.2f}M, é©æ‡‰åº¦={individual.fitness:.4f}"
            )

        except Exception as e:
            print(f"   âŒ è©•ä¼°å¤±æ•—: {e}")
            individual.fitness = 0.0
            individual.accuracy = 0.0

        return individual

    def create_model_from_genes(self, genes: Dict) -> nn.Module:
        """æ ¹æ“šåŸºå› å‰µå»º gMLP æ¨¡å‹"""
        from g_mlp_pytorch import gMLPVision

        model = gMLPVision(
            image_size=32,
            patch_size=4,
            num_classes=10,
            dim=int(genes["dim"]),
            depth=int(genes["depth"]),
            ff_mult=int(genes["ff_mult"]),
            channels=3,
            prob_survival=float(genes["prob_survival"]),
            attn_dim=int(genes["attn_dim"]),
        )
        return model

    def quick_train_evaluate(
        self, model, genes: Dict, trainloader, testloader, device
    ) -> Tuple[float, float]:
        """å¿«é€Ÿè¨“ç·´è©•ä¼° (ç”¨æ–¼é©æ‡‰åº¦è©•ä¼°)"""
        import time
        from torch.optim import AdamW
        from torch.optim.lr_scheduler import CosineAnnealingLR

        # å¿«é€Ÿè¨“ç·´é…ç½®
        epochs = 5  # æ¸›å°‘è¨“ç·´è¼ªæ•¸ä»¥åŠ é€Ÿè©•ä¼°
        criterion = nn.CrossEntropyLoss(label_smoothing=genes["label_smoothing"])
        optimizer = AdamW(
            model.parameters(),
            lr=genes["lr"],
            weight_decay=genes["weight_decay"],
            betas=(0.9, 0.95),
        )
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)

        start_time = time.time()

        for epoch in range(epochs):
            model.train()
            for batch_idx, (inputs, targets) in enumerate(trainloader):
                if batch_idx >= 50:  # é™åˆ¶æ¯å€‹ epoch çš„æ‰¹æ¬¡æ•¸
                    break

                inputs, targets = inputs.to(device), targets.to(device)

                optimizer.zero_grad()

                # å¯é¸ Mixup
                if genes["use_mixup"]:
                    alpha = genes["alpha"]
                    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0
                    batch_size = inputs.size()[0]
                    index = torch.randperm(batch_size).to(device)
                    mixed_x = lam * inputs + (1 - lam) * inputs[index, :]
                    y_a, y_b = targets, targets[index]

                    outputs = model(mixed_x)
                    loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(
                        outputs, y_b
                    )
                else:
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)

                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    model.parameters(), max_norm=genes["gradient_clip"]
                )

                optimizer.step()

            scheduler.step()

        training_time = time.time() - start_time

        # è©•ä¼°æº–ç¢ºç‡
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(testloader):
                if batch_idx >= 20:  # é™åˆ¶è©•ä¼°æ‰¹æ¬¡æ•¸
                    break
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        accuracy = 100.0 * correct / total if total > 0 else 0.0

        return accuracy, training_time

    def calculate_fitness(
        self, accuracy: float, params: int, training_time: float
    ) -> float:
        """è¨ˆç®—é©æ‡‰åº¦å‡½æ•¸"""
        # æ­£è¦åŒ–æŒ‡æ¨™
        acc_norm = accuracy / 100.0  # æº–ç¢ºç‡æ­£è¦åŒ–åˆ° 0-1

        # æ•ˆç‡æŒ‡æ¨™ (åƒæ•¸è¶Šå°‘è¶Šå¥½ï¼Œè¨“ç·´æ™‚é–“è¶ŠçŸ­è¶Šå¥½)
        param_penalty = params / 5e6  # 5M åƒæ•¸ä½œç‚ºåŸºæº–
        time_penalty = training_time / 100.0  # 100ç§’ä½œç‚ºåŸºæº–
        efficiency = 1.0 / (1.0 + param_penalty + time_penalty)

        # ç©©å®šæ€§æŒ‡æ¨™ (å¯ä»¥æ ¹æ“šè¨“ç·´éç¨‹çš„æ–¹å·®è¨ˆç®—ï¼Œé€™è£¡ç°¡åŒ–)
        stability = min(1.0, accuracy / 80.0)  # 80% æº–ç¢ºç‡ä»¥ä¸Šèªç‚ºç©©å®š

        # ç¶œåˆé©æ‡‰åº¦
        fitness = (
            self.fitness_weights["accuracy"] * acc_norm
            + self.fitness_weights["efficiency"] * efficiency
            + self.fitness_weights["stability"] * stability
        )

        return fitness

    def selection(self, population: List[Individual]) -> List[Individual]:
        """é¸æ“‡æ“ä½œ - éŒ¦æ¨™è³½é¸æ“‡"""
        tournament_size = 3
        selected = []

        for _ in range(self.population_size - self.elite_size):
            tournament = random.sample(population, tournament_size)
            winner = max(tournament, key=lambda x: x.fitness)
            selected.append(copy.deepcopy(winner))

        return selected

    def crossover(
        self, parent1: Individual, parent2: Individual
    ) -> Tuple[Individual, Individual]:
        """äº¤å‰æ“ä½œ"""
        if random.random() > self.crossover_rate:
            return copy.deepcopy(parent1), copy.deepcopy(parent2)

        child1_genes = {}
        child2_genes = {}

        for gene_name in parent1.genes.keys():
            if random.random() < 0.5:
                child1_genes[gene_name] = parent1.genes[gene_name]
                child2_genes[gene_name] = parent2.genes[gene_name]
            else:
                child1_genes[gene_name] = parent2.genes[gene_name]
                child2_genes[gene_name] = parent1.genes[gene_name]

        return Individual(genes=child1_genes), Individual(genes=child2_genes)

    def mutation(self, individual: Individual) -> Individual:
        """çªè®Šæ“ä½œ"""
        if random.random() > self.mutation_rate:
            return individual

        # é¸æ“‡è¦çªè®Šçš„åŸºå› 
        gene_to_mutate = random.choice(list(individual.genes.keys()))
        gene_range = self.gene_ranges[gene_to_mutate]

        if isinstance(gene_range, tuple):
            if isinstance(gene_range[0], int):
                individual.genes[gene_to_mutate] = random.randint(
                    gene_range[0], gene_range[1]
                )
            elif isinstance(gene_range[0], float):
                # é«˜æ–¯çªè®Š
                current_value = individual.genes[gene_to_mutate]
                mutation_strength = (gene_range[1] - gene_range[0]) * 0.1
                new_value = current_value + random.gauss(0, mutation_strength)
                new_value = max(gene_range[0], min(gene_range[1], new_value))
                individual.genes[gene_to_mutate] = new_value
        elif isinstance(gene_range, list):
            individual.genes[gene_to_mutate] = random.choice(gene_range)

        return individual

    def calculate_diversity(self, population: List[Individual]) -> float:
        """è¨ˆç®—ç¨®ç¾¤å¤šæ¨£æ€§"""
        if len(population) < 2:
            return 0.0

        diversity_sum = 0.0
        count = 0

        for i in range(len(population)):
            for j in range(i + 1, len(population)):
                # è¨ˆç®—å…©å€‹å€‹é«”çš„åŸºå› å·®ç•°
                diff = 0
                for gene_name in population[i].genes.keys():
                    if population[i].genes[gene_name] != population[j].genes[gene_name]:
                        diff += 1
                diversity_sum += diff / len(population[i].genes)
                count += 1

        return diversity_sum / count if count > 0 else 0.0

    def optimize(self, trainloader, testloader, device) -> Individual:
        """ä¸»è¦å„ªåŒ–æµç¨‹"""
        print("ğŸ§¬ é–‹å§‹éºå‚³ç®—æ³•å„ªåŒ– gMLP...")
        print(f"   ğŸ“Š ç¨®ç¾¤å¤§å°: {self.population_size}")
        print(f"   ğŸ”„ ä¸–ä»£æ•¸: {self.generations}")
        print(f"   ğŸ¯ çªè®Šç‡: {self.mutation_rate}")
        print(f"   ğŸ’‘ äº¤å‰ç‡: {self.crossover_rate}")

        # åˆå§‹åŒ–ç¨®ç¾¤
        population = self.initialize_population()

        # é€²åŒ–å¾ªç’°
        for generation in range(self.generations):
            print(f"\nğŸ§¬ ç¬¬ {generation + 1}/{self.generations} ä¸–ä»£")

            # è©•ä¼°é©æ‡‰åº¦
            print("   ğŸ“Š è©•ä¼°ç¨®ç¾¤é©æ‡‰åº¦...")
            for i, individual in enumerate(population):
                print(f"   è©•ä¼°å€‹é«” {i+1}/{len(population)}")
                population[i] = self.evaluate_fitness(
                    individual, trainloader, testloader, device
                )

            # æ’åºç¨®ç¾¤
            population.sort(key=lambda x: x.fitness, reverse=True)

            # è¨˜éŒ„æ­·å²
            best_individual = population[0]
            avg_fitness = sum(ind.fitness for ind in population) / len(population)
            diversity = self.calculate_diversity(population)

            self.history["best_fitness"].append(best_individual.fitness)
            self.history["avg_fitness"].append(avg_fitness)
            self.history["best_accuracy"].append(best_individual.accuracy)
            self.history["population_diversity"].append(diversity)
            self.history["best_individuals"].append(copy.deepcopy(best_individual))

            print(f"   ğŸ† æœ€ä½³é©æ‡‰åº¦: {best_individual.fitness:.4f}")
            print(f"   ğŸ“ˆ æœ€ä½³æº–ç¢ºç‡: {best_individual.accuracy:.2f}%")
            print(f"   ğŸ”€ ç¨®ç¾¤å¤šæ¨£æ€§: {diversity:.4f}")
            print(f"   ğŸ“Š å¹³å‡é©æ‡‰åº¦: {avg_fitness:.4f}")

            if generation < self.generations - 1:
                # é¸æ“‡ã€äº¤å‰ã€çªè®Š
                print("   ğŸ§¬ é€²è¡Œé¸æ“‡ã€äº¤å‰å’Œçªè®Š...")

                # ä¿ç•™ç²¾è‹±
                new_population = population[: self.elite_size]

                # é¸æ“‡çˆ¶æ¯
                parents = self.selection(population)

                # äº¤å‰ç”¢ç”Ÿå­ä»£
                children = []
                for i in range(0, len(parents), 2):
                    if i + 1 < len(parents):
                        child1, child2 = self.crossover(parents[i], parents[i + 1])
                        children.extend([child1, child2])

                # çªè®Š
                for child in children:
                    self.mutation(child)

                new_population.extend(
                    children[: self.population_size - self.elite_size]
                )
                population = new_population

        # è¿”å›æœ€ä½³å€‹é«”
        best_individual = max(population, key=lambda x: x.fitness)

        print(f"\nğŸ‰ å„ªåŒ–å®Œæˆï¼")
        print(f"   ğŸ† æœ€ä½³é…ç½®: {best_individual.genes}")
        print(f"   ğŸ“ˆ æœ€ä½³æº–ç¢ºç‡: {best_individual.accuracy:.2f}%")
        print(f"   ğŸ¯ æœ€ä½³é©æ‡‰åº¦: {best_individual.fitness:.4f}")
        print(f"   ğŸ“¦ æ¨¡å‹åƒæ•¸: {best_individual.parameters/1e6:.2f}M")

        return best_individual

    def plot_optimization_history(self):
        """ç¹ªè£½å„ªåŒ–æ­·å²"""
        if not self.history["best_fitness"]:
            print("âŒ æ²’æœ‰å„ªåŒ–æ­·å²æ•¸æ“š")
            return

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))

        generations = range(1, len(self.history["best_fitness"]) + 1)

        # é©æ‡‰åº¦é€²åŒ–
        ax1.plot(
            generations,
            self.history["best_fitness"],
            "r-",
            linewidth=2,
            label="Best Fitness",
        )
        ax1.plot(
            generations,
            self.history["avg_fitness"],
            "b--",
            linewidth=2,
            label="Average Fitness",
        )
        ax1.set_title("Fitness Evolution", fontweight="bold")
        ax1.set_xlabel("Generation")
        ax1.set_ylabel("Fitness")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # æº–ç¢ºç‡é€²åŒ–
        ax2.plot(
            generations, self.history["best_accuracy"], "g-", linewidth=2, marker="o"
        )
        ax2.set_title("Best Accuracy Evolution", fontweight="bold")
        ax2.set_xlabel("Generation")
        ax2.set_ylabel("Accuracy (%)")
        ax2.grid(True, alpha=0.3)

        # ç¨®ç¾¤å¤šæ¨£æ€§
        ax3.plot(
            generations, self.history["population_diversity"], "purple", linewidth=2
        )
        ax3.set_title("Population Diversity", fontweight="bold")
        ax3.set_xlabel("Generation")
        ax3.set_ylabel("Diversity")
        ax3.grid(True, alpha=0.3)

        # åƒæ•¸æ•¸é‡é€²åŒ–
        param_counts = [
            ind.parameters / 1e6 for ind in self.history["best_individuals"]
        ]
        ax4.plot(generations, param_counts, "orange", linewidth=2, marker="s")
        ax4.set_title("Best Model Size Evolution", fontweight="bold")
        ax4.set_xlabel("Generation")
        ax4.set_ylabel("Parameters (M)")
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig("genetic_optimization_history.png", dpi=300, bbox_inches="tight")
        plt.show()

    def save_results(self, best_individual: Individual, filename: str = None):
        """ä¿å­˜å„ªåŒ–çµæœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"genetic_optimization_results_{timestamp}.json"

        results = {
            "best_individual": {
                "genes": best_individual.genes,
                "fitness": best_individual.fitness,
                "accuracy": best_individual.accuracy,
                "parameters": best_individual.parameters,
                "training_time": best_individual.training_time,
            },
            "optimization_config": {
                "population_size": self.population_size,
                "generations": self.generations,
                "mutation_rate": self.mutation_rate,
                "crossover_rate": self.crossover_rate,
                "fitness_weights": self.fitness_weights,
            },
            "history": self.history,
            "gene_ranges": {
                k: v for k, v in self.gene_ranges.items() if not isinstance(v, list)
            },  # JSON doesn't handle lists in dict well
        }

        with open(filename, "w") as f:
            json.dump(results, f, indent=2)

        print(f"ğŸ“ çµæœå·²ä¿å­˜åˆ°: {filename}")


def run_genetic_optimization():
    """é‹è¡Œéºå‚³ç®—æ³•å„ªåŒ–"""
    # å°å…¥å¿…è¦æ¨¡çµ„
    import sys
    import os

    sys.path.append(os.path.dirname(os.path.abspath(__file__)))

    from model_16 import load_cifar10_data_enhanced

    print("ğŸ§¬ éºå‚³ç®—æ³•å„ªåŒ– gMLP æ¨¡å‹")
    print("=" * 50)

    # ç”¨æˆ¶é…ç½®
    try:
        population_size = int(input("ç¨®ç¾¤å¤§å° (é è¨­=10): ") or "10")
        generations = int(input("é€²åŒ–ä¸–ä»£æ•¸ (é è¨­=5): ") or "5")
        mutation_rate = float(input("çªè®Šç‡ (é è¨­=0.3): ") or "0.3")

        print(f"\nğŸ¯ å„ªåŒ–é…ç½®:")
        print(f"   ç¨®ç¾¤å¤§å°: {population_size}")
        print(f"   ä¸–ä»£æ•¸: {generations}")
        print(f"   çªè®Šç‡: {mutation_rate}")

    except ValueError:
        print("âŒ è¼¸å…¥éŒ¯èª¤ï¼Œä½¿ç”¨é è¨­é…ç½®")
        population_size, generations, mutation_rate = 10, 5, 0.3

    # åŠ è¼‰æ•¸æ“š
    print("\nğŸ“¦ åŠ è¼‰æ•¸æ“š...")
    trainloader, testloader, classes = load_cifar10_data_enhanced(
        quick_test=True, use_mixup_transform=False
    )

    # è¨­ç½®è¨­å‚™
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   ğŸ–¥ï¸  ä½¿ç”¨è¨­å‚™: {device}")

    # å‰µå»ºå„ªåŒ–å™¨
    optimizer = GeneticOptimizerGMLP(
        population_size=population_size,
        generations=generations,
        mutation_rate=mutation_rate,
        crossover_rate=0.7,
        elite_ratio=0.2,
    )

    # é–‹å§‹å„ªåŒ–
    try:
        best_individual = optimizer.optimize(trainloader, testloader, device)

        # ç¹ªè£½æ­·å²
        optimizer.plot_optimization_history()

        # ä¿å­˜çµæœ
        optimizer.save_results(best_individual)

        # è©¢å•æ˜¯å¦ä½¿ç”¨æœ€ä½³é…ç½®é€²è¡Œå®Œæ•´è¨“ç·´
        print(f"\n" + "=" * 50)
        use_best = input("ğŸ¯ æ˜¯å¦ä½¿ç”¨æœ€ä½³é…ç½®é€²è¡Œå®Œæ•´è¨“ç·´? (y/n): ").strip().lower()

        if use_best in ["y", "yes"]:
            print("\nğŸš€ é–‹å§‹ä½¿ç”¨æœ€ä½³é…ç½®é€²è¡Œå®Œæ•´è¨“ç·´...")
            train_with_best_config(best_individual, trainloader, testloader, device)

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  å„ªåŒ–å·²ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ å„ªåŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")


def train_with_best_config(
    best_individual: Individual, trainloader, testloader, device
):
    """ä½¿ç”¨æœ€ä½³é…ç½®é€²è¡Œå®Œæ•´è¨“ç·´"""
    from model_16 import (
        create_custom_gmlp_model,
        train_enhanced,
        evaluate_custom_model,
        plot_enhanced_training_history,
    )

    print("ğŸ‹ï¸ ä½¿ç”¨éºå‚³ç®—æ³•å„ªåŒ–çš„æœ€ä½³é…ç½®é€²è¡Œå®Œæ•´è¨“ç·´...")

    # è½‰æ›åŸºå› ç‚ºæ¨¡å‹é…ç½®
    model_config = {
        "depth": int(best_individual.genes["depth"]),
        "dim": int(best_individual.genes["dim"]),
        "ff_mult": int(best_individual.genes["ff_mult"]),
        "prob_survival": float(best_individual.genes["prob_survival"]),
        "attn_dim": int(best_individual.genes["attn_dim"]),
        "estimated_params": best_individual.parameters / 1e6,
    }

    # è½‰æ›åŸºå› ç‚ºè¨“ç·´é…ç½®
    training_params = {
        "lr": float(best_individual.genes["lr"]),
        "weight_decay": float(best_individual.genes["weight_decay"]),
        "epochs": 30,  # å®Œæ•´è¨“ç·´ä½¿ç”¨æ›´å¤šè¼ªæ•¸
        "use_mixup": bool(best_individual.genes["use_mixup"]),
        "alpha": float(best_individual.genes["alpha"]),
        "batch_split": 1,
        "use_enhanced_transform": False,
        "optimizer_type": "AdamW",
        "scheduler_type": "CosineAnnealingLR",
        "use_early_stopping": True,
        "patience": 10,
        "min_delta": 0.001,
    }

    print(f"ğŸ“Š æœ€ä½³æ¨¡å‹é…ç½®: {model_config}")
    print(f"âš™ï¸  æœ€ä½³è¨“ç·´åƒæ•¸: {training_params}")

    # å‰µå»ºä¸¦è¨“ç·´æ¨¡å‹
    model, device = create_custom_gmlp_model(model_config)

    train_result = train_enhanced(
        model, trainloader, testloader, device, training_params
    )

    (
        train_losses,
        train_accs,
        val_accs,
        val_losses,
        epoch_times,
        total_time,
        early_stopped,
        best_epoch,
    ) = train_result

    # å¯è¦–åŒ–çµæœ
    plot_enhanced_training_history(
        train_losses, train_accs, val_accs, val_losses, epoch_times
    )

    # è©•ä¼°æ¨¡å‹
    classes = [
        "Airplane",
        "Automobile",
        "Bird",
        "Cat",
        "Deer",
        "Dog",
        "Frog",
        "Horse",
        "Ship",
        "Truck",
    ]
    final_acc = evaluate_custom_model(model, testloader, device, classes)

    print(f"\nğŸ‰ éºå‚³ç®—æ³•å„ªåŒ–è¨“ç·´å®Œæˆ:")
    print(f"   ğŸ“ˆ æœ€çµ‚æº–ç¢ºç‡: {final_acc:.2f}%")
    print(f"   â±ï¸  è¨“ç·´æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
    print(f"   ğŸ§¬ éºå‚³ç®—æ³•æº–ç¢ºç‡é æ¸¬: {best_individual.accuracy:.2f}%")
    print(f"   ğŸ“Š å¯¦éš› vs é æ¸¬å·®ç•°: {abs(final_acc - best_individual.accuracy):.2f}%")


if __name__ == "__main__":
    run_genetic_optimization()

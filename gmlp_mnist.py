"""
å°‡CNNåƒæ•¸è½‰æ›ç‚ºgMLPå¯ç”¨çš„è¨“ç·´é…ç½®
åŸºæ–¼åŸCNNç¨‹å¼çš„è¨“ç·´ç­–ç•¥å’Œæ•¸æ“šè™•ç†æ–¹å¼
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from g_mlp_pytorch import gMLPVision
from tqdm import tqdm
import multiprocessing
import time
import matplotlib.pyplot as plt
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device: ", device)


# åŸCNNçš„æ•¸æ“šå¢å¼·ç­–ç•¥ï¼Œé©é…gMLP
train_transform = transforms.Compose(
    [
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
        transforms.RandomPerspective(distortion_scale=0.5, p=0.5),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
    ]
)

test_transform = transforms.Compose(
    [
        transforms.ToTensor(),
    ]
)


# MNIST Dataset å–ä»£ CIFAR10
class MNIST_dataset(Dataset):
    def __init__(self, partition="train", transform=None):
        print("\nLoading MNIST ", partition, " Dataset...")
        self.partition = partition
        self.transform = transform
        if self.partition == "train":
            self.data = torchvision.datasets.MNIST(".data/", train=True, download=True)
        else:
            self.data = torchvision.datasets.MNIST(".data/", train=False, download=True)
        print("\tTotal Len.: ", len(self.data), "\n", 50 * "-")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        image = self.data[idx][0]
        image_tensor = self.transform(image)
        label = torch.tensor(self.data[idx][1])
        label = F.one_hot(label, num_classes=10).float()
        return {"img": image_tensor, "label": label}


def create_gmlp_model():
    """å‰µå»ºå°æ‡‰MNISTè¤‡é›œåº¦çš„gMLPæ¨¡å‹"""
    # MNIST: å–®é€šé“ç°éšåœ–åƒ 28x28
    model = gMLPVision(
        image_size=28,  # MNISTåœ–åƒå¤§å°
        patch_size=4,  # é©åˆ28x28çš„patchå¤§å°
        num_classes=10,  # MNISTé¡åˆ¥æ•¸
        dim=128,  # MNISTè¼ƒç°¡å–®ï¼Œç¶­åº¦å¯èª¿ä½
        depth=12,  # æ·±åº¦å¯èª¿ä½
        ff_mult=3,  # ç‰¹å¾µç¶­åº¦æ“´å±•å€æ•¸
        channels=1,  # MNISTç‚ºå–®é€šé“
        prob_survival=1.0,
    )
    return model


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def train_gmlp_model():
    """ä½¿ç”¨MNISTè³‡æ–™é›†è¨“ç·´gMLPï¼Œæ·»åŠ mixupåŠŸèƒ½"""

    use_mixup = False  # å·²å–æ¶ˆmixup
    print("ğŸ¨ Mixupé…ç½®: é—œé–‰")

    # MNISTè³‡æ–™é›†è¼‰å…¥
    train_dataset = MNIST_dataset(partition="train", transform=train_transform)
    test_dataset = MNIST_dataset(partition="test", transform=test_transform)

    batch_size = 100
    num_workers = max(1, multiprocessing.cpu_count() - 1)
    print("Num workers", num_workers)

    train_dataloader = DataLoader(
        train_dataset, batch_size, shuffle=True, num_workers=num_workers
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size, shuffle=False, num_workers=num_workers
    )

    net = create_gmlp_model()
    net.to(device)
    print("gMLP Model:")
    print(f"Parameters: {count_parameters(net):,}")

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(
        net.parameters(),
        lr=0.001,
        weight_decay=1e-4,
        betas=(0.9, 0.95),
    )
    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, factor=0.1, patience=10, min_lr=0.00001
    )
    epochs = 50  # MNISTè¼ƒç°¡å–®ï¼Œè¨“ç·´è¼ªæ•¸å¯æ¸›å°‘

    print("\n---- Start Training gMLP ----")
    best_accuracy = -1
    best_epoch = 0
    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []

    for epoch in range(epochs):
        start_time = time.time()
        train_loss, train_correct = 0, 0
        net.train()
        lam = 1.0
        with tqdm(
            iter(train_dataloader), desc="Epoch " + str(epoch), unit="batch"
        ) as tepoch:
            for batch in tepoch:
                images = batch["img"].to(device)
                labels = batch["label"].to(device)
                optimizer.zero_grad()
                outputs = net(images)
                loss = criterion(outputs, labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
                optimizer.step()
                labels_idx = torch.argmax(labels, dim=1)
                pred = torch.argmax(outputs, dim=1)
                train_correct += pred.eq(labels_idx).sum().item()
                train_loss += loss.item()
        train_loss /= len(train_dataloader.dataset) / batch_size
        train_accuracy = 100.0 * train_correct / len(train_dataloader.dataset)
        test_loss, test_correct = 0, 0
        net.eval()
        with torch.no_grad():
            with tqdm(
                iter(test_dataloader), desc="Test " + str(epoch), unit="batch"
            ) as tepoch:
                for batch in tepoch:
                    images = batch["img"].to(device)
                    labels = batch["label"].to(device)
                    outputs = net(images)
                    test_loss += criterion(outputs, labels)
                    labels_idx = torch.argmax(labels, dim=1)
                    pred = torch.argmax(outputs, dim=1)
                    test_correct += pred.eq(labels_idx).sum().item()
        lr_scheduler.step(test_loss)
        test_loss /= len(test_dataloader.dataset) / batch_size
        test_accuracy = 100.0 * test_correct / len(test_dataloader.dataset)
        train_losses.append(train_loss)
        train_accs.append(train_accuracy)
        test_losses.append(test_loss.item())
        test_accs.append(test_accuracy)
        epoch_time = time.time() - start_time
        print(
            "[Epoch {}] Train Loss: {:.6f} - Test Loss: {:.6f} - Train Accuracy: {:.2f}% - Test Accuracy: {:.2f}% - Time: {:.1f}s".format(
                epoch + 1,
                train_loss,
                test_loss,
                train_accuracy,
                test_accuracy,
                epoch_time,
            )
        )
        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy
            best_epoch = epoch
            torch.save(net.state_dict(), "best_gmlp_model.pt")
            print(f"   ğŸ’¾ New best model saved: {test_accuracy:.2f}%")
    print(f"\nBEST TEST ACCURACY: {best_accuracy:.2f}% in epoch {best_epoch + 1}")
    return train_losses, train_accs, test_losses, test_accs, best_accuracy


def plot_training_history(train_losses, train_accs, test_losses, test_accs):
    """ç¹ªè£½è¨“ç·´æ­·å²"""
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, "b-", label="Train Loss")
    plt.plot(test_losses, "r-", label="Test Loss")
    plt.title("gMLP Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(train_accs, "b-", label="Train Acc")
    plt.plot(test_accs, "r-", label="Test Acc")
    plt.title("gMLP Training Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig("gmlp_training_history.png", dpi=300, bbox_inches="tight")
    plt.show()


if __name__ == "__main__":
    print("ğŸš€ gMLPè¨“ç·´ - MNISTè³‡æ–™é›† (ç„¡Mixup)")
    print("ğŸ¯ æ¨¡å‹é…ç½®: depth=8, dim=128, ff_mult=2")
    print("ğŸ“š å„ªåŒ–å™¨: AdamW (æ›´é©åˆTransformer)")
    print("ğŸ“ˆ èª¿åº¦å™¨: ReduceLROnPlateau")
    print("ğŸ”¢ æ‰¹æ¬¡å¤§å°: 100")
    print("ğŸ¨ Mixupå¢å¼·: é—œé–‰")
    print("=" * 70)

    try:
        train_losses, train_accs, test_losses, test_accs, best_acc = train_gmlp_model()
        plot_training_history(train_losses, train_accs, test_losses, test_accs)

        print(f"\nğŸ‰ è¨“ç·´å®Œæˆ!")
        print(f"   â€¢ æœ€ä½³æº–ç¢ºç‡: {best_acc:.2f}%")
        print(f"   â€¢ æ¨¡å‹å·²ä¿å­˜: best_gmlp_model.pt")
        print(f"   â€¢ Mixupå¢å¼·: é—œé–‰")
        print(f"   â€¢ è¨“ç·´ç­–ç•¥: AdamW + æ¢¯åº¦è£å‰ª")
    except KeyboardInterrupt:
        print("\nâŒ è¨“ç·´è¢«ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

"""
å°‡CNNåƒæ•¸è½‰æ›ç‚ºgMLPå¯ç”¨çš„è¨“ç·´é…ç½®
åŸºæ–¼åŸCNNç¨‹å¼çš„è¨“ç·´ç­–ç•¥å’Œæ•¸æ“šè™•ç†æ–¹å¼
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from g_mlp_pytorch import gMLPVision
from tqdm import tqdm
import multiprocessing
import time
import matplotlib.pyplot as plt
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device: ", device)


# Mixup åŠŸèƒ½ - åƒè€ƒ model_16.py
def mixup_data(x, y, alpha=0.1, lam=1.0, count=0, device="cpu"):
    """Mixup æ•¸æ“šå¢å¼· - åŸºæ–¼ model_16.py å¯¦ç¾"""
    if count == 0:
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1.0

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Mixup æå¤±å‡½æ•¸ - åŸºæ–¼ model_16.py å¯¦ç¾"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


# åŸCNNçš„æ•¸æ“šå¢å¼·ç­–ç•¥ï¼Œé©é…gMLP
train_transform = transforms.Compose(
    [
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
        transforms.RandomPerspective(distortion_scale=0.5, p=0.5),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
    ]
)

test_transform = transforms.Compose(
    [
        transforms.ToTensor(),
    ]
)


class CIFAR10_dataset(Dataset):
    def __init__(self, partition="train", transform=None):
        print("\nLoading CIFAR10 ", partition, " Dataset...")
        self.partition = partition
        self.transform = transform
        if self.partition == "train":
            self.data = torchvision.datasets.CIFAR10(
                ".data/", train=True, download=True
            )
        else:
            self.data = torchvision.datasets.CIFAR10(
                ".data/", train=False, download=True
            )
        print("\tTotal Len.: ", len(self.data), "\n", 50 * "-")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Image
        image = self.data[idx][0]
        image_tensor = self.transform(image)

        # Label - ä¿æŒèˆ‡åŸç¨‹å¼ç›¸åŒçš„one-hotç·¨ç¢¼æ ¼å¼
        label = torch.tensor(self.data[idx][1])
        label = F.one_hot(label, num_classes=10).float()

        return {"img": image_tensor, "label": label}


def create_gmlp_model():
    """å‰µå»ºå°æ‡‰CNNè¤‡é›œåº¦çš„gMLPæ¨¡å‹"""
    # åŸºæ–¼CNNçš„æ·±åº¦å’Œè¤‡é›œåº¦èª¿æ•´gMLPåƒæ•¸
    model = gMLPVision(
        image_size=32,  # CIFAR-10åœ–åƒå¤§å°
        patch_size=4,  # é©åˆ32x32çš„patchå¤§å°
        num_classes=10,  # CIFAR-10é¡åˆ¥æ•¸
        dim=256,  # è¼ƒå¤§çš„ç¶­åº¦å°æ‡‰CNNçš„é€šé“æ•¸å¢é•·
        depth=12,  # å°æ‡‰CNNçš„9å±¤å·ç© + 3å±¤å…¨é€£æ¥çš„æ·±åº¦
        ff_mult=4,  # å°æ‡‰CNNä¸­ç‰¹å¾µç¶­åº¦çš„æ“´å±•å€æ•¸
        channels=3,  # RGBåœ–åƒ
        prob_survival=0.9,  # æ·»åŠ éš¨æ©Ÿæ·±åº¦ä»¥å°æ‡‰CNNçš„dropout
    )
    return model


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def train_gmlp_model():
    """ä½¿ç”¨CNNçš„è¨“ç·´ç­–ç•¥è¨“ç·´gMLPï¼Œæ·»åŠ mixupåŠŸèƒ½"""

    # Mixup é…ç½® - ä½¿ç”¨ model_16.py çš„é è¨­å€¼
    use_mixup = True  # å•Ÿç”¨ mixup
    mixup_alpha = 0.1  # model_16.py çš„é è¨­å€¼

    print(f"ğŸ¨ Mixupé…ç½®: {'å•Ÿç”¨' if use_mixup else 'é—œé–‰'}")
    if use_mixup:
        print(f"   ğŸ­ Alpha åƒæ•¸: {mixup_alpha}")

    # æ•¸æ“šè¼‰å…¥ï¼ˆä¿æŒåŸCNNçš„é…ç½®ï¼‰
    train_dataset = CIFAR10_dataset(partition="train", transform=train_transform)
    test_dataset = CIFAR10_dataset(partition="test", transform=test_transform)

    batch_size = 100  # ä¿æŒåŸCNNçš„batch size
    num_workers = multiprocessing.cpu_count() - 1
    print("Num workers", num_workers)

    train_dataloader = DataLoader(
        train_dataset, batch_size, shuffle=True, num_workers=num_workers
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size, shuffle=False, num_workers=num_workers
    )

    # å‰µå»ºgMLPæ¨¡å‹
    net = create_gmlp_model()
    net.to(device)
    print("gMLP Model:")
    print(f"Parameters: {count_parameters(net):,}")

    # è¨“ç·´è¶…åƒæ•¸ï¼ˆåŸºæ–¼åŸCNNé…ç½®èª¿æ•´ï¼‰
    criterion = nn.CrossEntropyLoss()

    # å°‡SGDæ”¹ç‚ºAdamWï¼ˆæ›´é©åˆTransformeræ¶æ§‹ï¼‰
    optimizer = optim.AdamW(
        net.parameters(),
        lr=0.001,  # è¼ƒä½çš„å­¸ç¿’ç‡é©åˆAdamW
        weight_decay=1e-4,  # èª¿æ•´æ¬Šé‡è¡°æ¸›
        betas=(0.9, 0.95),  # Transformerå¸¸ç”¨çš„betaå€¼
    )

    # ä¿æŒåŸæœ‰çš„å­¸ç¿’ç‡èª¿åº¦ç­–ç•¥
    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, factor=0.1, patience=10, min_lr=0.00001
    )

    epochs = 100  # ä¿æŒåŸè¨“ç·´è¼ªæ•¸

    print("\n---- Start Training gMLP ----")
    best_accuracy = -1
    best_epoch = 0

    # è¨“ç·´æ­·å²è¨˜éŒ„
    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []

    for epoch in range(epochs):
        start_time = time.time()

        # TRAIN NETWORK
        train_loss, train_correct = 0, 0
        net.train()
        lam = 1.0  # mixup lambda åˆå§‹å€¼

        with tqdm(
            iter(train_dataloader), desc="Epoch " + str(epoch), unit="batch"
        ) as tepoch:
            for batch in tepoch:
                # æ•¸æ“šè™•ç†ï¼ˆä¿æŒåŸæ ¼å¼ï¼‰
                images = batch["img"].to(device)
                labels = batch["label"].to(device)

                optimizer.zero_grad()

                # Mixup æ•¸æ“šå¢å¼·
                if use_mixup:
                    images, labels_a, labels_b, lam = mixup_data(
                        images, labels, mixup_alpha, lam, 0, device
                    )
                    outputs = net(images)
                    loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)
                else:
                    outputs = net(images)
                    loss = criterion(outputs, labels)

                # Backward
                loss.backward()

                # æ·»åŠ æ¢¯åº¦è£å‰ªï¼ˆé©åˆTransformerï¼‰
                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)

                optimizer.step()

                # çµ±è¨ˆï¼ˆé©é…mixupï¼‰
                if use_mixup:
                    # Mixup æ¨¡å¼ä¸‹çš„æº–ç¢ºç‡è¨ˆç®—
                    labels_idx_a = torch.argmax(labels_a, dim=1)
                    labels_idx_b = torch.argmax(labels_b, dim=1)
                    pred = torch.argmax(outputs, dim=1)

                    # åŸºæ–¼ lambda åŠ æ¬Šçš„æº–ç¢ºç‡è¨ˆç®—
                    correct_a = pred.eq(labels_idx_a).sum().item()
                    correct_b = pred.eq(labels_idx_b).sum().item()
                    train_correct += lam * correct_a + (1 - lam) * correct_b
                else:
                    labels_idx = torch.argmax(labels, dim=1)
                    pred = torch.argmax(outputs, dim=1)
                    train_correct += pred.eq(labels_idx).sum().item()

                train_loss += loss.item()

        train_loss /= len(train_dataloader.dataset) / batch_size
        train_accuracy = 100.0 * train_correct / len(train_dataloader.dataset)

        # TEST NETWORK
        test_loss, test_correct = 0, 0
        net.eval()
        with torch.no_grad():
            with tqdm(
                iter(test_dataloader), desc="Test " + str(epoch), unit="batch"
            ) as tepoch:
                for batch in tepoch:
                    images = batch["img"].to(device)
                    labels = batch["label"].to(device)

                    outputs = net(images)
                    test_loss += criterion(outputs, labels)

                    labels_idx = torch.argmax(labels, dim=1)
                    pred = torch.argmax(outputs, dim=1)
                    test_correct += pred.eq(labels_idx).sum().item()

        # å­¸ç¿’ç‡èª¿åº¦
        lr_scheduler.step(test_loss)

        test_loss /= len(test_dataloader.dataset) / batch_size
        test_accuracy = 100.0 * test_correct / len(test_dataloader.dataset)

        # è¨˜éŒ„è¨“ç·´æ­·å²
        train_losses.append(train_loss)
        train_accs.append(train_accuracy)
        test_losses.append(test_loss.item())
        test_accs.append(test_accuracy)

        epoch_time = time.time() - start_time

        print(
            "[Epoch {}] Train Loss: {:.6f} - Test Loss: {:.6f} - Train Accuracy: {:.2f}% - Test Accuracy: {:.2f}% - Time: {:.1f}s".format(
                epoch + 1,
                train_loss,
                test_loss,
                train_accuracy,
                test_accuracy,
                epoch_time,
            )
        )

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy
            best_epoch = epoch
            torch.save(net.state_dict(), "best_gmlp_model.pt")
            print(f"   ğŸ’¾ New best model saved: {test_accuracy:.2f}%")

    print(f"\nBEST TEST ACCURACY: {best_accuracy:.2f}% in epoch {best_epoch + 1}")

    return train_losses, train_accs, test_losses, test_accs, best_accuracy


def plot_training_history(train_losses, train_accs, test_losses, test_accs):
    """ç¹ªè£½è¨“ç·´æ­·å²"""
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, "b-", label="Train Loss")
    plt.plot(test_losses, "r-", label="Test Loss")
    plt.title("gMLP Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(train_accs, "b-", label="Train Acc")
    plt.plot(test_accs, "r-", label="Test Acc")
    plt.title("gMLP Training Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig("gmlp_training_history.png", dpi=300, bbox_inches="tight")
    plt.show()


if __name__ == "__main__":
    print("ğŸš€ gMLPè¨“ç·´ - ä½¿ç”¨CNNåƒæ•¸é…ç½® + Mixupå¢å¼·")
    print("ğŸ¯ æ¨¡å‹é…ç½®: depth=12, dim=256, ff_mult=4")
    print("ğŸ“š å„ªåŒ–å™¨: AdamW (æ›¿ä»£SGDï¼Œæ›´é©åˆTransformer)")
    print("ğŸ“ˆ èª¿åº¦å™¨: ReduceLROnPlateau (ä¿æŒåŸç­–ç•¥)")
    print("ğŸ”¢ æ‰¹æ¬¡å¤§å°: 100 (ä¿æŒåŸè¨­å®š)")
    print("ğŸ¨ Mixupå¢å¼·: å•Ÿç”¨ (alpha=0.1, åŸºæ–¼model_16.py)")
    print("=" * 70)

    try:
        train_losses, train_accs, test_losses, test_accs, best_acc = train_gmlp_model()
        plot_training_history(train_losses, train_accs, test_losses, test_accs)

        print(f"\nğŸ‰ è¨“ç·´å®Œæˆ!")
        print(f"   â€¢ æœ€ä½³æº–ç¢ºç‡: {best_acc:.2f}%")
        print(f"   â€¢ æ¨¡å‹å·²ä¿å­˜: best_gmlp_model.pt")
        print(f"   â€¢ Mixupå¢å¼·: å·²å•Ÿç”¨ (alpha=0.1)")
        print(f"   â€¢ è¨“ç·´ç­–ç•¥: AdamW + Mixup + æ¢¯åº¦è£å‰ª")

    except KeyboardInterrupt:
        print("\nâŒ è¨“ç·´è¢«ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

"""
CPUå„ªåŒ–ç‰ˆ gMLP åœ–åƒåˆ†é¡æ¸¬è©¦
å°ˆç‚ºCPUç’°å¢ƒå„ªåŒ–ï¼ŒåŒ…å«å¯è¦–åŒ–çµæœå’Œæº–ç¢ºç‡å„ªåŒ–æŠ€å·§
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
from g_mlp_pytorch import gMLPVision
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns


def load_cifar10_data_enhanced(quick_test=True):
    """åŠ è¼‰å¢å¼·çš„ CIFAR-10 æ•¸æ“šé›† - CPUå„ªåŒ–ç‰ˆ"""
    print("ğŸ“¦ åŠ è¼‰CPUå„ªåŒ–çš„ CIFAR-10 æ•¸æ“šé›†...")

    # CPUå„ªåŒ–çš„æ•¸æ“šå¢å¼·ç­–ç•¥ - è«–æ–‡å•Ÿç™¼ä½†é©é…CIFAR-10
    transform_train = transforms.Compose(
        [
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=8),
            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
            # è«–æ–‡ä¸­æ²’æœ‰ä½¿ç”¨RandomErasingï¼Œæ‰€ä»¥ç§»é™¤ä»¥ä¿æŒè«–æ–‡ä¸€è‡´æ€§
        ]
    )

    transform_test = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    trainset = torchvision.datasets.CIFAR10(
        root="./data", train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root="./data", train=False, download=True, transform=transform_test
    )

    if quick_test:
        # è«–æ–‡å•Ÿç™¼é…ç½®ï¼šä½¿ç”¨æ›´å¤šæ•¸æ“šä»¥ç¬¦åˆè«–æ–‡è¨“ç·´è¦æ¨¡
        trainset = Subset(trainset, range(15000))  # å¢åŠ åˆ°15Kæ¨£æœ¬æ›´æ¥è¿‘è«–æ–‡è¦æ¨¡
        testset = Subset(testset, range(3000))  # ç›¸æ‡‰å¢åŠ æ¸¬è©¦æ•¸æ“šåˆ°3K
        print("   ï¿½ è«–æ–‡å•Ÿç™¼æ¨¡å¼ï¼šå¤§è¦æ¨¡æ•¸æ“šé›†è¨“ç·´")

    # è«–æ–‡å•Ÿç™¼DataLoaderé…ç½®
    batch_size = 128  # æ›´æ¥è¿‘è«–æ–‡batch sizeä½†é©é…CPU (è«–æ–‡4096ï¼Œé€™è£¡128)
    num_workers = 0  # CPUå–®ç·šç¨‹é¿å…ç«¶çˆ­
    pin_memory = False  # CPUä¸éœ€è¦pin_memory

    trainloader = DataLoader(
        trainset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    testloader = DataLoader(
        testset,
        batch_size=batch_size,
        shuffle=False,  # æ¸¬è©¦ä¹Ÿç”¨åŒæ¨£batch_size
        num_workers=num_workers,
        pin_memory=pin_memory,
    )

    classes = [
        "Airplane",
        "Automobile",
        "Bird",
        "Cat",
        "Deer",
        "Dog",
        "Frog",
        "Horse",
        "Ship",
        "Truck",
    ]

    print(f"   âœ“ è¨“ç·´æ¨£æœ¬: {len(trainset)}")
    print(f"   âœ“ æ¸¬è©¦æ¨£æœ¬: {len(testset)}")
    print(f"   âœ“ é¡åˆ¥æ•¸: {len(classes)}")
    print(f"   âœ“ è«–æ–‡å•Ÿç™¼å„ªåŒ–: batch_size={batch_size}, å¤§è¦æ¨¡è¨“ç·´é…ç½®")

    return trainloader, testloader, classes


def create_optimized_gmlp_model(model_size="S"):
    """å‰µå»ºè«–æ–‡æ¨™æº–çš„ gMLP æ¨¡å‹æ¶æ§‹"""
    print(f"\nğŸ—ï¸ å‰µå»ºè«–æ–‡æ¨™æº– gMLP-{model_size} æ¨¡å‹...")

    # CPUå°ˆç”¨å„ªåŒ–è¨­ç½®
    torch.set_num_threads(4)  # è¨­ç½®4å€‹ç·šç¨‹
    print("   âš¡ CPUæ¨¡å¼ï¼šå·²è¨­ç½®4å€‹ç·šç¨‹")

    # è«–æ–‡æ¨™æº–æ¶æ§‹é…ç½® (åŸºæ–¼Table 1)
    if model_size == "Ti":  # gMLP-Ti (æœ€å°æ¨¡å‹)
        config = {
            "depth": 30,  # #L = 30
            "dim": 128,  # d_model = 128
            "ff_mult": 6,  # d_ffn / d_model = 768/128 = 6
            "prob_survival": 1.00,  # è«–æ–‡ä¸­Tiæ¨¡å‹ä¸ä½¿ç”¨éš¨æ©Ÿæ·±åº¦
            "params_target": 5.9,  # ç›®æ¨™åƒæ•¸é‡(M)
        }
    elif model_size == "S":  # gMLP-S (ä¸­ç­‰æ¨¡å‹)
        config = {
            "depth": 30,  # #L = 30
            "dim": 256,  # d_model = 256
            "ff_mult": 6,  # d_ffn / d_model = 1536/256 = 6
            "prob_survival": 0.95,  # è«–æ–‡éš¨æ©Ÿæ·±åº¦å­˜æ´»ç‡
            "params_target": 19.5,  # ç›®æ¨™åƒæ•¸é‡(M)
        }
    elif model_size == "B":  # gMLP-B (å¤§æ¨¡å‹)
        config = {
            "depth": 30,  # #L = 30
            "dim": 512,  # d_model = 512
            "ff_mult": 6,  # d_ffn / d_model = 3072/512 = 6
            "prob_survival": 0.80,  # è«–æ–‡éš¨æ©Ÿæ·±åº¦å­˜æ´»ç‡
            "params_target": 73.4,  # ç›®æ¨™åƒæ•¸é‡(M)
        }
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹å¤§å°: {model_size}")

    model = gMLPVision(
        # === æ ¸å¿ƒæ¶æ§‹åƒæ•¸ (åš´æ ¼æŒ‰ç…§è«–æ–‡Table 1) ===
        image_size=32,  # CIFAR-10åœ–åƒå°ºå¯¸ (è«–æ–‡ç”¨224ï¼Œé€™è£¡é©é…32)
        patch_size=4,  # é©é…CIFAR-10çš„patch size
        num_classes=10,  # CIFAR-10åˆ†é¡æ•¸é‡
        dim=config["dim"],  # d_model (è«–æ–‡æ¨™æº–)
        depth=config["depth"],  # å±¤æ•¸ #L (è«–æ–‡æ¨™æº–)
        # === ç¶²çµ¡çµæ§‹åƒæ•¸ ===
        ff_mult=config["ff_mult"],  # å‰é¥‹ç¶²çµ¡å€æ•¸ (è«–æ–‡è¨ˆç®—å¾—å‡º)
        channels=3,  # è¼¸å…¥é€šé“æ•¸
        # === æ­£å‰‡åŒ–åƒæ•¸ (è«–æ–‡æ¨™æº–) ===
        prob_survival=config["prob_survival"],  # éš¨æ©Ÿæ·±åº¦å­˜æ´»ç‡
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    total_params = sum(p.numel() for p in model.parameters())
    params_M = total_params / 1e6  # è½‰æ›ç‚ºç™¾è¬åƒæ•¸

    print(f"   âœ“ gMLP-{model_size} æ¨¡å‹å‰µå»ºå®Œæˆ")
    print(f"   âœ“ è¨­å‚™: {device}")
    print(f"   âœ“ å¯¦éš›åƒæ•¸æ•¸é‡: {total_params:,} ({params_M:.1f}M)")
    print(f"   âœ“ è«–æ–‡ç›®æ¨™åƒæ•¸: {config['params_target']}M")
    print(f"   âœ“ æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB")
    print(
        f"   âœ“ è«–æ–‡æ¶æ§‹: depth={config['depth']}, dim={config['dim']}, ff_mult={config['ff_mult']}"
    )
    print(f"   âœ“ éš¨æ©Ÿæ·±åº¦: prob_survival={config['prob_survival']}")

    return model, device


def train_model_with_scheduler(model, trainloader, testloader, device, epochs=300):
    """è«–æ–‡æ¨™æº–è¨“ç·´é…ç½® - åŸºæ–¼ImageNet-1Kè¶…åƒæ•¸"""
    print(f"\nğŸ‹ï¸ é–‹å§‹è«–æ–‡æ¨™æº–è¨“ç·´ ({epochs} å€‹ epochs)...")

    # è«–æ–‡æ¨™æº–è¨“ç·´é…ç½® (åŸºæ–¼ImageNet-1Kè¶…åƒæ•¸)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # è«–æ–‡æ¨™æº–æ¨™ç±¤å¹³æ»‘
    optimizer = optim.AdamW(
        model.parameters(),
        lr=1e-3,  # è«–æ–‡å³°å€¼å­¸ç¿’ç‡
        weight_decay=0.05,  # è«–æ–‡æ¬Šé‡è¡°æ¸›
        betas=(0.9, 0.999),  # è«–æ–‡Adamåƒæ•¸
        eps=1e-6,  # è«–æ–‡Adam epsilon
    )

    # è«–æ–‡æ¨™æº–å­¸ç¿’ç‡èª¿åº¦å™¨ (Cosineé€€ç«ï¼Œ10K warmup stepsé©é…)
    total_steps = len(trainloader) * epochs
    warmup_steps = min(10000, total_steps // 10)  # è«–æ–‡10K warmupï¼Œä½†é©é…è¼ƒå°æ•¸æ“šé›†

    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=1e-3,  # è«–æ–‡å³°å€¼å­¸ç¿’ç‡1e-3
        epochs=epochs,
        steps_per_epoch=len(trainloader),
        pct_start=warmup_steps / total_steps,  # åŸºæ–¼warmup stepsçš„ç™¾åˆ†æ¯”
        anneal_strategy="cos",  # è«–æ–‡ä½¿ç”¨cosineé€€ç«
        final_div_factor=1000,  # å¤§å¹…è¡°æ¸›åˆ°åˆå§‹å€¼çš„1/1000
    )

    train_losses = []
    train_accs = []
    val_accs = []
    epoch_times = []  # è¨˜éŒ„æ¯å€‹epochçš„æ™‚é–“

    # è¨˜éŒ„ç¸½è¨“ç·´é–‹å§‹æ™‚é–“
    total_start_time = time.time()

    # è«–æ–‡æ¨™æº–æ—©åœé…ç½®
    best_val_acc = 0
    patience = 30  # è«–æ–‡ç´šåˆ¥é•·è¨“ç·´çš„è€å¿ƒå€¼
    patience_counter = 0

    # è«–æ–‡ä¸ä½¿ç”¨å‹•æ…‹éæ“¬åˆæª¢æ¸¬ï¼Œå°ˆæ³¨æ–¼æ¨™æº–è¨“ç·´
    print("   ğŸ“„ ä½¿ç”¨è«–æ–‡æ¨™æº–è¨“ç·´é…ç½®ï¼š300 epochs + æ¨™æº–æ­£å‰‡åŒ–")

    for epoch in range(epochs):
        # è¨˜éŒ„æ¯å€‹epoché–‹å§‹æ™‚é–“
        epoch_start_time = time.time()

        # è¨“ç·´éšæ®µ
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        print(
            f"\nEpoch {epoch + 1}/{epochs}, LR: {optimizer.param_groups[0]['lr']:.6f}"
        )

        # å‹•æ…‹èª¿æ•´æ•¸æ“šå¢å¼· - å¼·åŒ–é˜²éæ“¬åˆç­–ç•¥
        if epoch >= epochs * 0.4:  # å¾40%é–‹å§‹æ¸›å°‘æ•¸æ“šå¢å¼· (0.7->0.4)
            # é€æ­¥æ¸›å°‘éš¨æ©Ÿæ“¦é™¤æ¦‚ç‡
            for transform in trainloader.dataset.dataset.transform.transforms:
                if isinstance(transform, transforms.RandomErasing):
                    if epoch >= epochs * 0.4 and epoch < epochs * 0.7:
                        transform.p = 0.1  # ä¸­æœŸé™ä½
                    elif epoch >= epochs * 0.7:
                        transform.p = 0.03  # å¾ŒæœŸå¤§å¹…é™ä½

        for i, (inputs, labels) in enumerate(trainloader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            # è«–æ–‡æ¨™æº–æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                model.parameters(),
                max_norm=1.0,  # è«–æ–‡æ¢¯åº¦è£å‰ªé–¾å€¼1.0
            )
            optimizer.step()
            scheduler.step()  # OneCycleLRéœ€è¦æ¯å€‹batchæ›´æ–°

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            if (i + 1) % 15 == 0 or (i + 1) == len(
                trainloader
            ):  # é¡¯ç¤ºé€²åº¦ä¸”åŒ…å«æœ€å¾Œæ‰¹æ¬¡
                acc = 100.0 * correct / total
                current_lr = optimizer.param_groups[0]["lr"]
                print(
                    f"   æ‰¹æ¬¡ {i+1:3d}/{len(trainloader)}: æå¤± = {running_loss/(i+1):.4f}, "
                    f"æº–ç¢ºç‡ = {acc:.2f}%, å­¸ç¿’ç‡ = {current_lr:.6f}"
                )

        epoch_loss = running_loss / len(trainloader)
        epoch_acc = 100.0 * correct / total
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)

        # é©—è­‰éšæ®µ
        val_acc = quick_validate(model, testloader, device)
        val_accs.append(val_acc)

        # OneCycleLRä¸éœ€è¦æ‰‹å‹•step

        # è¨˜éŒ„æ¯å€‹epochçµæŸæ™‚é–“
        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        epoch_times.append(epoch_duration)

        print(
            f"Epoch {epoch + 1} å®Œæˆ: è¨“ç·´æº–ç¢ºç‡ = {epoch_acc:.2f}%, é©—è­‰æº–ç¢ºç‡ = {val_acc:.2f}%, æ™‚é–“ = {epoch_duration:.2f}s"
        )

        # è«–æ–‡æ¨™æº–è¨“ç·´é€²åº¦å ±å‘Š (ç§»é™¤éæ“¬åˆç›£æ§ï¼Œå°ˆæ³¨æ¨™æº–è¨“ç·´)
        train_val_diff = epoch_acc - val_acc
        if train_val_diff > 10:
            print(f"   ğŸ“Š è¨“ç·´-é©—è­‰å·®ç•°: {train_val_diff:.2f}%")

        # é•·è¨“ç·´é€²åº¦æç¤º
        if epoch == epochs // 10:
            print(f"   ğŸ”„ å·²å®Œæˆ10%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        elif epoch == epochs // 4:
            print(f"   ğŸ”„ å·²å®Œæˆ25%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        elif epoch == epochs // 2:
            print(f"   ğŸ”„ å·²å®Œæˆ50%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        elif epoch == epochs * 3 // 4:
            print(f"   ğŸ”„ å·²å®Œæˆ75%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        elif epoch == epochs * 9 // 10:
            print(f"   ğŸ”„ å·²å®Œæˆ90%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")

        # è«–æ–‡æ¨™æº–æ—©åœæ©Ÿåˆ¶
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), "best_model_checkpoint_paper.pth")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"   æ—©åœï¼šé©—è­‰æº–ç¢ºç‡ {patience} å€‹epochæœªæå‡")
                break

    # è¨ˆç®—ç¸½è¨“ç·´æ™‚é–“
    total_end_time = time.time()
    total_training_time = total_end_time - total_start_time

    print(f"\nâ±ï¸ è«–æ–‡æ¨™æº–è¨“ç·´æ™‚é–“çµ±è¨ˆ:")
    print(
        f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.2f}s ({total_training_time/60:.2f}min)"
    )
    print(f"   â€¢ å¹³å‡æ¯epochæ™‚é–“: {np.mean(epoch_times):.2f}s")
    print(f"   â€¢ æœ€å¿«epochæ™‚é–“: {np.min(epoch_times):.2f}s")
    print(f"   â€¢ æœ€æ…¢epochæ™‚é–“: {np.max(epoch_times):.2f}s")
    print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")

    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    if best_val_acc > 0:
        model.load_state_dict(torch.load("best_model_checkpoint_paper.pth"))
        print("   â€¢ å·²è¼‰å…¥æœ€ä½³è«–æ–‡æ¨™æº–æ¨¡å‹æ¬Šé‡")

    return train_losses, train_accs, val_accs, epoch_times, total_training_time


def quick_validate(model, testloader, device):
    """å¿«é€Ÿé©—è­‰"""
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return 100.0 * correct / total


def evaluate_model_with_visualization(model, testloader, device, classes):
    """è©•ä¼°æ¨¡å‹ä¸¦ç”Ÿæˆå¯è¦–åŒ–çµæœ"""
    print("\nğŸ“Š è©•ä¼°CPUå„ªåŒ–æ¨¡å‹ä¸¦ç”Ÿæˆå¯è¦–åŒ–...")

    model.eval()
    all_predictions = []
    all_labels = []
    correct = 0
    total = 0
    class_correct = [0] * 10
    class_total = [0] * 10

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)

            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            for i in range(labels.size(0)):
                label = labels[i]
                class_correct[label] += (predicted[i] == label).item()
                class_total[label] += 1

    overall_acc = 100.0 * correct / total
    print(f"   âœ“ CPUæ¨¡å‹æ•´é«”æº–ç¢ºç‡: {overall_acc:.2f}%")

    # 1. å„é¡åˆ¥æº–ç¢ºç‡æ¢å½¢åœ–
    plt.figure(figsize=(15, 10))

    plt.subplot(2, 2, 1)
    class_accs = []
    for i in range(10):
        if class_total[i] > 0:
            acc = 100.0 * class_correct[i] / class_total[i]
            class_accs.append(acc)
        else:
            class_accs.append(0)

    bars = plt.bar(classes, class_accs, color=plt.cm.tab10(np.arange(10)))
    plt.title(
        "CPU-Optimized gMLP: Accuracy of Each Category", fontsize=14, fontweight="bold"
    )
    plt.ylabel("Accuracy (%)")
    plt.xticks(rotation=45)
    plt.grid(axis="y", alpha=0.3)

    # åœ¨æŸ±ç‹€åœ–ä¸Šæ·»åŠ æ•¸å€¼
    for bar, acc in zip(bars, class_accs):
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{acc:.1f}%",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # 2. æ··æ·†çŸ©é™£
    plt.subplot(2, 2, 2)
    cm = confusion_matrix(all_labels, all_predictions)
    sns.heatmap(
        cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes
    )
    plt.title("CPU-Optimized Confusion Matrix", fontsize=14, fontweight="bold")
    plt.xlabel("Predicted Category")
    plt.ylabel("True Category")

    # 3. æ¨™æº–åŒ–æ··æ·†çŸ©é™£
    plt.subplot(2, 2, 3)
    cm_normalized = confusion_matrix(all_labels, all_predictions, normalize="true")
    sns.heatmap(
        cm_normalized,
        annot=True,
        fmt=".2f",
        cmap="Blues",
        xticklabels=classes,
        yticklabels=classes,
    )
    plt.title(
        "CPU-Optimized Normalized Confusion Matrix", fontsize=14, fontweight="bold"
    )
    plt.xlabel("Predicted Category")
    plt.ylabel("True Category")

    # 4. é¡åˆ¥åˆ†ä½ˆ
    plt.subplot(2, 2, 4)
    unique, counts = np.unique(all_labels, return_counts=True)
    plt.pie(
        counts,
        labels=[classes[i] for i in unique],
        autopct="%1.1f%%",
        colors=plt.cm.tab10(np.arange(len(unique))),
    )
    plt.title("CPU Test Set Category Distribution", fontsize=14, fontweight="bold")

    plt.tight_layout()
    plt.savefig("gmlp_cpu_evaluation_results.png", dpi=300, bbox_inches="tight")
    plt.show()

    # æ‰“å°è©³ç´°å ±å‘Š
    print(f"\nğŸ“‹ CPUå„ªåŒ–æ¨¡å‹è©³ç´°åˆ†é¡å ±å‘Š:")
    target_names = [f"{i}_{classes[i]}" for i in range(10)]
    report = classification_report(
        all_labels, all_predictions, target_names=target_names, digits=3
    )
    print(report)

    return overall_acc


def plot_training_history(train_losses, train_accs, val_accs, epoch_times=None):
    """ç¹ªè£½CPUè¨“ç·´æ­·å²"""
    print("\nğŸ“ˆ ç¹ªè£½CPUè¨“ç·´æ­·å²...")

    # èª¿æ•´åœ–ç‰‡å¤§å°ä»¥å®¹ç´æ™‚é–“åœ–è¡¨
    if epoch_times is not None:
        plt.figure(figsize=(20, 5))
        subplot_count = 4
    else:
        plt.figure(figsize=(15, 5))
        subplot_count = 3

    # æå¤±æ›²ç·š
    plt.subplot(1, subplot_count, 1)
    plt.plot(train_losses, "b-", linewidth=2, label="CPU Training Loss")
    plt.title("CPU-Optimized Training Loss Curve", fontsize=14, fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # æº–ç¢ºç‡æ›²ç·š
    plt.subplot(1, subplot_count, 2)
    plt.plot(train_accs, "g-", linewidth=2, label="CPU Training Accuracy")
    plt.plot(val_accs, "r-", linewidth=2, label="CPU Validation Accuracy")
    plt.title("CPU-Optimized Accuracy Curve", fontsize=14, fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # æº–ç¢ºç‡å·®ç•°
    plt.subplot(1, subplot_count, 3)
    diff = np.array(train_accs) - np.array(val_accs)
    plt.plot(diff, "purple", linewidth=2, label="CPU Train-Val Difference")
    plt.title("CPU Overfitting Monitor", fontsize=14, fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy Difference (%)")
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color="black", linestyle="--", alpha=0.5)
    plt.legend()

    # æ™‚é–“çµ±è¨ˆåœ–ï¼ˆå¦‚æœæä¾›äº†æ™‚é–“æ•¸æ“šï¼‰
    if epoch_times is not None:
        plt.subplot(1, subplot_count, 4)
        plt.plot(epoch_times, "orange", linewidth=2, marker="o", label="CPU Epoch Time")
        plt.title("CPU Training Time per Epoch", fontsize=14, fontweight="bold")
        plt.xlabel("Epoch")
        plt.ylabel("Time (seconds)")
        plt.grid(True, alpha=0.3)
        plt.legend()

    plt.tight_layout()
    plt.savefig("gmlp_cpu_training_history.png", dpi=300, bbox_inches="tight")
    plt.show()


def visualize_sample_predictions(model, testloader, device, classes, num_samples=12):
    """å¯è¦–åŒ–CPUæ¨¡å‹æ¨£æœ¬é æ¸¬çµæœ"""
    print(f"\nğŸ” å¯è¦–åŒ–CPUæ¨¡å‹ {num_samples} å€‹æ¨£æœ¬é æ¸¬...")

    model.eval()

    # ç²å–ä¸€æ‰¹æ•¸æ“š
    dataiter = iter(testloader)
    images, labels = next(dataiter)
    images, labels = images.to(device), labels.to(device)

    with torch.no_grad():
        outputs = model(images)
        _, predictions = torch.max(outputs, 1)
        probabilities = torch.softmax(outputs, 1)

    # ç¹ªè£½çµæœ
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    fig.suptitle(
        "CPU-Optimized gMLP Prediction Results", fontsize=16, fontweight="bold"
    )

    for i in range(min(num_samples, len(images))):
        ax = axes[i // 4, i % 4]

        # åæ¨™æº–åŒ–åœ–åƒ
        img = images[i].cpu()
        mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)
        std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)
        img = img * std + mean
        img = torch.clamp(img, 0, 1)

        ax.imshow(img.permute(1, 2, 0))

        true_label = classes[labels[i]]
        pred_label = classes[predictions[i]]
        confidence = probabilities[i, predictions[i]].item()

        # è¨­ç½®é¡è‰²ï¼ˆæ­£ç¢º=ç¶ è‰²ï¼ŒéŒ¯èª¤=ç´…è‰²ï¼‰
        color = "green" if labels[i] == predictions[i] else "red"

        ax.set_title(
            f"True: {true_label}\nPred: {pred_label}\nConf: {confidence:.2f}",
            color=color,
            fontweight="bold",
        )
        ax.axis("off")

    plt.tight_layout()
    plt.savefig("gmlp_cpu_sample_predictions.png", dpi=300, bbox_inches="tight")
    plt.show()


def compare_model_architectures():
    """æ¯”è¼ƒä¸åŒgMLPæ¨¡å‹æ¶æ§‹çš„è¦æ ¼"""
    print("\nğŸ“‹ è«–æ–‡æ¨™æº– gMLP æ¨¡å‹æ¶æ§‹æ¯”è¼ƒ:")
    print("=" * 80)
    print(
        f"{'æ¨¡å‹':<8} {'æ·±åº¦':<6} {'ç¶­åº¦':<8} {'FFNå€æ•¸':<8} {'éš¨æ©Ÿæ·±åº¦':<10} {'ç›®æ¨™åƒæ•¸(M)':<12}"
    )
    print("-" * 80)

    models_config = {
        "Ti": {
            "depth": 30,
            "dim": 128,
            "ff_mult": 6,
            "prob_survival": 1.00,
            "params": 5.9,
        },
        "S": {
            "depth": 30,
            "dim": 256,
            "ff_mult": 6,
            "prob_survival": 0.95,
            "params": 19.5,
        },
        "B": {
            "depth": 30,
            "dim": 512,
            "ff_mult": 6,
            "prob_survival": 0.80,
            "params": 73.4,
        },
    }

    for name, config in models_config.items():
        print(
            f"gMLP-{name:<3} {config['depth']:<6} {config['dim']:<8} {config['ff_mult']:<8} "
            f"{config['prob_survival']:<10.2f} {config['params']:<12.1f}"
        )

    print("-" * 80)
    print("ğŸ’¡ å»ºè­°:")
    print("   â€¢ gMLP-Ti: å¿«é€Ÿå¯¦é©—å’Œæ¦‚å¿µé©—è­‰")
    print("   â€¢ gMLP-S:  å¹³è¡¡æ€§èƒ½å’Œè¨ˆç®—è³‡æº (æ¨è–¦)")
    print("   â€¢ gMLP-B:  è¿½æ±‚æœ€ä½³æ€§èƒ½ (éœ€è¦æ›´å¤šè³‡æº)")
    print("=" * 80)


def main():
    print("ğŸ–¼ï¸ è«–æ–‡æ¨™æº– gMLP åœ–åƒåˆ†é¡æ¸¬è©¦")
    print("=" * 60)
    print("ğŸ“„ åŸºæ–¼å®˜æ–¹ImageNet-1Kè¶…åƒæ•¸é…ç½®")
    print("=" * 60)

    # é¡¯ç¤ºæ¨¡å‹æ¶æ§‹æ¯”è¼ƒ
    compare_model_architectures()

    try:
        # 1. åŠ è¼‰è«–æ–‡æ¨™æº–æ•¸æ“š
        trainloader, testloader, classes = load_cifar10_data_enhanced(quick_test=True)

        # 2. å‰µå»ºè«–æ–‡æ¨™æº–æ¨¡å‹ (å¯é¸æ“‡æ¨¡å‹å¤§å°)
        model_size = "S"  # é¸æ“‡ 'Ti', 'S', æˆ– 'B'
        model, device = create_optimized_gmlp_model(model_size=model_size)

        # 3. è«–æ–‡æ¨™æº–è¨“ç·´
        train_losses, train_accs, val_accs, epoch_times, total_training_time = (
            train_model_with_scheduler(
                model,
                trainloader,
                testloader,
                device,
                epochs=300,  # è«–æ–‡æ¨™æº–300 epochs
            )
        )

        # 4. ç¹ªè£½è¨“ç·´æ­·å²
        plot_training_history(train_losses, train_accs, val_accs, epoch_times)

        # 5. è©³ç´°è©•ä¼°èˆ‡å¯è¦–åŒ–
        accuracy = evaluate_model_with_visualization(model, testloader, device, classes)

        # 6. å¯è¦–åŒ–é æ¸¬æ¨£æœ¬
        visualize_sample_predictions(model, testloader, device, classes)

        # 7. ä¿å­˜è«–æ–‡æ¨™æº–æ¨¡å‹
        torch.save(model.state_dict(), "gmlp_paper_model.pth")
        print("\nğŸ’¾ è«–æ–‡æ¨™æº–æ¨¡å‹å·²ä¿å­˜ç‚º 'gmlp_paper_model.pth'")

        print("\n" + "=" * 60)
        print("âœ… è«–æ–‡æ¨™æº–æ¸¬è©¦å®Œæˆï¼")
        print(f"\nğŸ“ˆ è«–æ–‡æ¨™æº–æœ€çµ‚çµæœ:")
        print(f"   â€¢ æœ€çµ‚æ¸¬è©¦æº–ç¢ºç‡: {accuracy:.2f}%")
        print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(val_accs):.2f}%")
        print(f"   â€¢ è¨“ç·´-é©—è­‰å·®ç•°: {train_accs[-1] - val_accs[-1]:.2f}%")
        print(
            f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.2f}s ({total_training_time/60:.2f}min)"
        )
        print(f"   â€¢ å¹³å‡æ¯epochæ™‚é–“: {np.mean(epoch_times):.2f}s")

        print(f"\nğŸ“„ è«–æ–‡é…ç½®ç‰¹æ€§:")
        print(f"   â€¢ æ¨¡å‹æ¶æ§‹: gMLP-{model_size}")
        print(f"   â€¢ æ¨¡å‹åƒæ•¸: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   â€¢ è«–æ–‡æ¨™æº–: depth=30, dim=256, ff_mult=6, prob_survival=0.95")
        print(f"   â€¢ æ•¸æ“šé‡: 15,000è¨“ç·´æ¨£æœ¬, 3,000æ¸¬è©¦æ¨£æœ¬")
        print(f"   â€¢ æ‰¹æ¬¡å¤§å°: 128 (æ¥è¿‘è«–æ–‡è¦æ¨¡)")
        print(f"   â€¢ è¨“ç·´ç­–ç•¥: 300 epochs + cosineé€€ç« + 10K warmup")

        print(f"\nğŸ¯ è«–æ–‡æ¨™æº–ç’°å¢ƒå»ºè­°:")
        if accuracy < 70:
            print(f"   â€¢ 300 epochså¾Œæº–ç¢ºåº¦ä»éœ€æå‡ï¼Œè€ƒæ…®èª¿æ•´æ¨¡å‹æ¶æ§‹")
            print(f"   â€¢ å¯èƒ½éœ€è¦æ›´å¤§çš„æ•¸æ“šé›†æˆ–æ›´é•·çš„è¨“ç·´")
            print(f"   â€¢ æª¢æŸ¥æ•¸æ“šé è™•ç†æ˜¯å¦èˆ‡è«–æ–‡ä¸€è‡´")
        elif accuracy < 80:
            print(f"   â€¢ è«–æ–‡æ¨™æº–æ¨¡å‹è¡¨ç¾è‰¯å¥½ï¼")
            print(f"   â€¢ 300å€‹epochså……åˆ†åˆ©ç”¨äº†è«–æ–‡é…ç½®")
            print(f"   â€¢ å¯è€ƒæ…®å¾®èª¿è¶…åƒæ•¸é€²ä¸€æ­¥å„ªåŒ–")
        else:
            print(f"   â€¢ è«–æ–‡æ¨™æº–æ¨¡å‹è¡¨ç¾å„ªç§€ï¼")
            print(f"   â€¢ æˆåŠŸå¾©ç¾è«–æ–‡ç´šåˆ¥çš„è¨“ç·´æ•ˆæœ")
            print(f"   â€¢ é©åˆç™¼è¡¨æˆ–å¯¦éš›æ‡‰ç”¨éƒ¨ç½²")

        # æ³›åŒ–èƒ½åŠ›åˆ†æ
        overfitting_diff = train_accs[-1] - val_accs[-1]
        if overfitting_diff > 15:
            print(f"\nâš ï¸  éœ€è¦æ³¨æ„:")
            print(f"   â€¢ è¨“ç·´-é©—è­‰å·®ç•°è¼ƒå¤§ ({overfitting_diff:.2f}%)")
            print(f"   â€¢ å¯èƒ½éœ€è¦æ›´å¤šæ•¸æ“šæˆ–æ›´å¼·æ­£å‰‡åŒ–")
        elif overfitting_diff > 8:
            print(f"\nğŸ”¶ æ³›åŒ–è¡¨ç¾:")
            print(f"   â€¢ è¨“ç·´-é©—è­‰å·®ç•°é©ä¸­ ({overfitting_diff:.2f}%)")
            print(f"   â€¢ è«–æ–‡é…ç½®å–å¾—è‰¯å¥½å¹³è¡¡")
        else:
            print(f"\nâœ… å„ªç§€æ³›åŒ–:")
            print(f"   â€¢ è¨“ç·´-é©—è­‰å·®ç•°å¾ˆå° ({overfitting_diff:.2f}%)")
            print(f"   â€¢ è«–æ–‡é…ç½®å¯¦ç¾å‡ºè‰²æ³›åŒ–èƒ½åŠ›")

        print(f"\nğŸš€ è«–æ–‡æ¨™æº–ç¸½çµ:")
        print(f"   â€¢ è¨“ç·´ç­–ç•¥: 300 epochs + è«–æ–‡æ¨™æº–è¶…åƒæ•¸")
        print(f"   â€¢ æ­£å‰‡åŒ–: æ¨™ç±¤å¹³æ»‘0.1 + æ¬Šé‡è¡°æ¸›0.05 + éš¨æ©Ÿæ·±åº¦0.8")
        print(f"   â€¢ å­¸ç¿’ç‡: å³°å€¼1e-3 + cosineé€€ç« + 10K warmup")
        print(f"   â€¢ å„ªåŒ–å™¨: AdamW + æ¢¯åº¦è£å‰ª1.0 + epsilon 1e-6")
        print(f"   â€¢ ç›®æ¨™: å¾©ç¾è«–æ–‡ImageNet-1Kç´šåˆ¥çš„è¨“ç·´æ•ˆæœ")

    except Exception as e:
        print(f"âŒ è«–æ–‡æ¨™æº–æ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()

"""
CPUå„ªåŒ–ç‰ˆ gMLP åœ–åƒåˆ†é¡æ¸¬è©¦
å°ˆç‚ºCPUç’°å¢ƒå„ªåŒ–ï¼ŒåŒ…å«å¯è¦–åŒ–çµæœå’Œæº–ç¢ºç‡å„ªåŒ–æŠ€å·§
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
from g_mlp_pytorch import gMLPVision
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns


def load_cifar10_data_enhanced(quick_test=True):
    """åŠ è¼‰å¢å¼·çš„ CIFAR-10 æ•¸æ“šé›† - CPUå„ªåŒ–ç‰ˆ"""
    print("ğŸ“¦ åŠ è¼‰CPUå„ªåŒ–çš„ CIFAR-10 æ•¸æ“šé›†...")

    # CPUå„ªåŒ–çš„æ•¸æ“šå¢å¼·ç­–ç•¥ - è«–æ–‡å•Ÿç™¼ä½†é©é…CIFAR-10
    transform_train = transforms.Compose(
        [
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=8),
            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
            # è«–æ–‡ä¸­æ²’æœ‰ä½¿ç”¨RandomErasingï¼Œæ‰€ä»¥ç§»é™¤ä»¥ä¿æŒè«–æ–‡ä¸€è‡´æ€§
        ]
    )

    transform_test = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    trainset = torchvision.datasets.CIFAR10(
        root="./data", train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root="./data", train=False, download=True, transform=transform_test
    )

    if quick_test:
        # è«–æ–‡å•Ÿç™¼é…ç½®ï¼šä½¿ç”¨æ›´å¤šæ•¸æ“šä»¥ç¬¦åˆè«–æ–‡è¨“ç·´è¦æ¨¡
        trainset = Subset(trainset, range(15000))  # å¢åŠ åˆ°15Kæ¨£æœ¬æ›´æ¥è¿‘è«–æ–‡è¦æ¨¡
        testset = Subset(testset, range(3000))  # ç›¸æ‡‰å¢åŠ æ¸¬è©¦æ•¸æ“šåˆ°3K
        print("   ï¿½ è«–æ–‡å•Ÿç™¼æ¨¡å¼ï¼šå¤§è¦æ¨¡æ•¸æ“šé›†è¨“ç·´")

    # è«–æ–‡å•Ÿç™¼DataLoaderé…ç½®
    batch_size = 128  # æ›´æ¥è¿‘è«–æ–‡batch sizeä½†é©é…CPU (è«–æ–‡4096ï¼Œé€™è£¡128)
    num_workers = 0  # CPUå–®ç·šç¨‹é¿å…ç«¶çˆ­
    pin_memory = False  # CPUä¸éœ€è¦pin_memory

    trainloader = DataLoader(
        trainset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    testloader = DataLoader(
        testset,
        batch_size=batch_size,
        shuffle=False,  # æ¸¬è©¦ä¹Ÿç”¨åŒæ¨£batch_size
        num_workers=num_workers,
        pin_memory=pin_memory,
    )

    classes = [
        "Airplane",
        "Automobile",
        "Bird",
        "Cat",
        "Deer",
        "Dog",
        "Frog",
        "Horse",
        "Ship",
        "Truck",
    ]

    print(f"   âœ“ è¨“ç·´æ¨£æœ¬: {len(trainset)}")
    print(f"   âœ“ æ¸¬è©¦æ¨£æœ¬: {len(testset)}")
    print(f"   âœ“ é¡åˆ¥æ•¸: {len(classes)}")
    print(f"   âœ“ è«–æ–‡å•Ÿç™¼å„ªåŒ–: batch_size={batch_size}, å¤§è¦æ¨¡è¨“ç·´é…ç½®")

    return trainloader, testloader, classes


def create_optimized_gmlp_model(model_size="S"):
    """å‰µå»ºè«–æ–‡æ¨™æº–çš„ gMLP æ¨¡å‹æ¶æ§‹"""
    print(f"\nğŸ—ï¸ å‰µå»ºè«–æ–‡æ¨™æº– gMLP-{model_size} æ¨¡å‹...")

    # CPUå°ˆç”¨å„ªåŒ–è¨­ç½®
    torch.set_num_threads(4)  # è¨­ç½®4å€‹ç·šç¨‹
    print("   âš¡ CPUæ¨¡å¼ï¼šå·²è¨­ç½®4å€‹ç·šç¨‹")

    # è«–æ–‡æ¨™æº–æ¶æ§‹é…ç½® (åŸºæ–¼Table 1)
    if model_size == "Ti":  # gMLP-Ti (æœ€å°æ¨¡å‹)
        config = {
            "depth": 30,  # #L = 30
            "dim": 128,  # d_model = 128
            "ff_mult": 6,  # d_ffn / d_model = 768/128 = 6
            "prob_survival": 1.00,  # è«–æ–‡ä¸­Tiæ¨¡å‹ä¸ä½¿ç”¨éš¨æ©Ÿæ·±åº¦
            "params_target": 5.9,  # ç›®æ¨™åƒæ•¸é‡(M)
        }
    elif model_size == "S":  # gMLP-S (ä¸­ç­‰æ¨¡å‹)
        config = {
            "depth": 30,  # #L = 30
            "dim": 256,  # d_model = 256
            "ff_mult": 6,  # d_ffn / d_model = 1536/256 = 6
            "prob_survival": 0.95,  # è«–æ–‡éš¨æ©Ÿæ·±åº¦å­˜æ´»ç‡
            "params_target": 19.5,  # ç›®æ¨™åƒæ•¸é‡(M)
        }
    elif model_size == "B":  # gMLP-B (å¤§æ¨¡å‹)
        config = {
            "depth": 30,  # #L = 30
            "dim": 512,  # d_model = 512
            "ff_mult": 6,  # d_ffn / d_model = 3072/512 = 6
            "prob_survival": 0.80,  # è«–æ–‡éš¨æ©Ÿæ·±åº¦å­˜æ´»ç‡
            "params_target": 73.4,  # ç›®æ¨™åƒæ•¸é‡(M)
        }
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹å¤§å°: {model_size}")

    model = gMLPVision(
        # === æ ¸å¿ƒæ¶æ§‹åƒæ•¸ (åš´æ ¼æŒ‰ç…§è«–æ–‡Table 1) ===
        image_size=32,  # CIFAR-10åœ–åƒå°ºå¯¸ (è«–æ–‡ç”¨224ï¼Œé€™è£¡é©é…32)
        patch_size=4,  # é©é…CIFAR-10çš„patch size
        num_classes=10,  # CIFAR-10åˆ†é¡æ•¸é‡
        dim=config["dim"],  # d_model (è«–æ–‡æ¨™æº–)
        depth=config["depth"],  # å±¤æ•¸ #L (è«–æ–‡æ¨™æº–)
        # === ç¶²çµ¡çµæ§‹åƒæ•¸ ===
        ff_mult=config["ff_mult"],  # å‰é¥‹ç¶²çµ¡å€æ•¸ (è«–æ–‡è¨ˆç®—å¾—å‡º)
        channels=3,  # è¼¸å…¥é€šé“æ•¸
        # === æ­£å‰‡åŒ–åƒæ•¸ (è«–æ–‡æ¨™æº–) ===
        prob_survival=config["prob_survival"],  # éš¨æ©Ÿæ·±åº¦å­˜æ´»ç‡
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    total_params = sum(p.numel() for p in model.parameters())
    params_M = total_params / 1e6  # è½‰æ›ç‚ºç™¾è¬åƒæ•¸

    print(f"   âœ“ gMLP-{model_size} æ¨¡å‹å‰µå»ºå®Œæˆ")
    print(f"   âœ“ è¨­å‚™: {device}")
    print(f"   âœ“ å¯¦éš›åƒæ•¸æ•¸é‡: {total_params:,} ({params_M:.1f}M)")
    print(f"   âœ“ è«–æ–‡ç›®æ¨™åƒæ•¸: {config['params_target']}M")
    print(f"   âœ“ æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB")
    print(
        f"   âœ“ è«–æ–‡æ¶æ§‹: depth={config['depth']}, dim={config['dim']}, ff_mult={config['ff_mult']}"
    )
    print(f"   âœ“ éš¨æ©Ÿæ·±åº¦: prob_survival={config['prob_survival']}")

    return model, device


def train_model_with_scheduler(model, trainloader, testloader, device, epochs=300):
    """è«–æ–‡æ¨™æº–è¨“ç·´é…ç½® - åŸºæ–¼ImageNet-1Kè¶…åƒæ•¸"""
    print(f"\nğŸ‹ï¸ é–‹å§‹è«–æ–‡æ¨™æº–è¨“ç·´ ({epochs} å€‹ epochs)...")

    # è«–æ–‡æ¨™æº–è¨“ç·´é…ç½® (åŸºæ–¼ImageNet-1Kè¶…åƒæ•¸)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # è«–æ–‡æ¨™æº–æ¨™ç±¤å¹³æ»‘
    optimizer = optim.AdamW(
        model.parameters(),
        lr=1e-3,  # è«–æ–‡å³°å€¼å­¸ç¿’ç‡
        weight_decay=0.05,  # è«–æ–‡æ¬Šé‡è¡°æ¸›
        betas=(0.9, 0.999),  # è«–æ–‡Adamåƒæ•¸
        eps=1e-6,  # è«–æ–‡Adam epsilon
    )

    # è«–æ–‡æ¨™æº–å­¸ç¿’ç‡èª¿åº¦å™¨ (Cosineé€€ç«ï¼Œ10K warmup stepsé©é…)
    total_steps = len(trainloader) * epochs
    warmup_steps = min(10000, total_steps // 10)  # è«–æ–‡10K warmupï¼Œä½†é©é…è¼ƒå°æ•¸æ“šé›†

    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=1e-3,  # è«–æ–‡å³°å€¼å­¸ç¿’ç‡1e-3
        epochs=epochs,
        steps_per_epoch=len(trainloader),
        pct_start=warmup_steps / total_steps,  # åŸºæ–¼warmup stepsçš„ç™¾åˆ†æ¯”
        anneal_strategy="cos",  # è«–æ–‡ä½¿ç”¨cosineé€€ç«
        final_div_factor=1000,  # å¤§å¹…è¡°æ¸›åˆ°åˆå§‹å€¼çš„1/1000
    )

    train_losses = []
    train_accs = []
    val_accs = []
    epoch_times = []  # è¨˜éŒ„æ¯å€‹epochçš„æ™‚é–“

    # è¨˜éŒ„ç¸½è¨“ç·´é–‹å§‹æ™‚é–“
    total_start_time = time.time()

    # è«–æ–‡æ¨™æº–æ—©åœé…ç½®
    best_val_acc = 0
    patience = 30  # è«–æ–‡ç´šåˆ¥é•·è¨“ç·´çš„è€å¿ƒå€¼
    patience_counter = 0

    # è«–æ–‡ä¸ä½¿ç”¨å‹•æ…‹éæ“¬åˆæª¢æ¸¬ï¼Œå°ˆæ³¨æ–¼æ¨™æº–è¨“ç·´
    print("   ğŸ“„ ä½¿ç”¨è«–æ–‡æ¨™æº–è¨“ç·´é…ç½®ï¼š300 epochs + æ¨™æº–æ­£å‰‡åŒ–")

    for epoch in range(epochs):
        # è¨˜éŒ„æ¯å€‹epoché–‹å§‹æ™‚é–“
        epoch_start_time = time.time()

        # è¨“ç·´éšæ®µ
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        print(
            f"\nEpoch {epoch + 1}/{epochs}, LR: {optimizer.param_groups[0]['lr']:.6f}"
        )

        # å‹•æ…‹èª¿æ•´æ•¸æ“šå¢å¼· - å¼·åŒ–é˜²éæ“¬åˆç­–ç•¥
        if epoch >= epochs * 0.4:  # å¾40%é–‹å§‹æ¸›å°‘æ•¸æ“šå¢å¼· (0.7->0.4)
            # é€æ­¥æ¸›å°‘éš¨æ©Ÿæ“¦é™¤æ¦‚ç‡
            for transform in trainloader.dataset.dataset.transform.transforms:
                if isinstance(transform, transforms.RandomErasing):
                    if epoch >= epochs * 0.4 and epoch < epochs * 0.7:
                        transform.p = 0.1  # ä¸­æœŸé™ä½
                    elif epoch >= epochs * 0.7:
                        transform.p = 0.03  # å¾ŒæœŸå¤§å¹…é™ä½

        for i, (inputs, labels) in enumerate(trainloader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            # è«–æ–‡æ¨™æº–æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                model.parameters(),
                max_norm=1.0,  # è«–æ–‡æ¢¯åº¦è£å‰ªé–¾å€¼1.0
            )
            optimizer.step()
            scheduler.step()  # OneCycleLRéœ€è¦æ¯å€‹batchæ›´æ–°

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            if (i + 1) % 15 == 0 or (i + 1) == len(
                trainloader
            ):  # é¡¯ç¤ºé€²åº¦ä¸”åŒ…å«æœ€å¾Œæ‰¹æ¬¡
                acc = 100.0 * correct / total
                current_lr = optimizer.param_groups[0]["lr"]
                print(
                    f"   æ‰¹æ¬¡ {i+1:3d}/{len(trainloader)}: æå¤± = {running_loss/(i+1):.4f}, "
                    f"æº–ç¢ºç‡ = {acc:.2f}%, å­¸ç¿’ç‡ = {current_lr:.6f}"
                )

        epoch_loss = running_loss / len(trainloader)
        epoch_acc = 100.0 * correct / total
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)

        # é©—è­‰éšæ®µ
        val_acc = quick_validate(model, testloader, device)
        val_accs.append(val_acc)

        # OneCycleLRä¸éœ€è¦æ‰‹å‹•step

        # è¨˜éŒ„æ¯å€‹epochçµæŸæ™‚é–“
        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        epoch_times.append(epoch_duration)

        print(
            f"Epoch {epoch + 1} å®Œæˆ: è¨“ç·´æº–ç¢ºç‡ = {epoch_acc:.2f}%, é©—è­‰æº–ç¢ºç‡ = {val_acc:.2f}%, æ™‚é–“ = {epoch_duration:.2f}s"
        )

        # è«–æ–‡æ¨™æº–è¨“ç·´é€²åº¦å ±å‘Š (ç§»é™¤éæ“¬åˆç›£æ§ï¼Œå°ˆæ³¨æ¨™æº–è¨“ç·´)
        train_val_diff = epoch_acc - val_acc
        if train_val_diff > 10:
            print(f"   ğŸ“Š è¨“ç·´-é©—è­‰å·®ç•°: {train_val_diff:.2f}%")

        # é•·è¨“ç·´é€²åº¦æç¤º
        if epoch == epochs // 10:
            print(f"   ğŸ”„ å·²å®Œæˆ10%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        elif epoch == epochs // 4:
            print(f"   ğŸ”„ å·²å®Œæˆ25%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        elif epoch == epochs // 2:
            print(f"   ğŸ”„ å·²å®Œæˆ50%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        elif epoch == epochs * 3 // 4:
            print(f"   ğŸ”„ å·²å®Œæˆ75%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        elif epoch == epochs * 9 // 10:
            print(f"   ğŸ”„ å·²å®Œæˆ90%è¨“ç·´ï¼Œç›®å‰æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")

        # è«–æ–‡æ¨™æº–æ—©åœæ©Ÿåˆ¶
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), "best_model_checkpoint_paper.pth")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"   æ—©åœï¼šé©—è­‰æº–ç¢ºç‡ {patience} å€‹epochæœªæå‡")
                break

    # è¨ˆç®—ç¸½è¨“ç·´æ™‚é–“
    total_end_time = time.time()
    total_training_time = total_end_time - total_start_time

    print(f"\nâ±ï¸ è«–æ–‡æ¨™æº–è¨“ç·´æ™‚é–“çµ±è¨ˆ:")
    print(
        f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.2f}s ({total_training_time/60:.2f}min)"
    )
    print(f"   â€¢ å¹³å‡æ¯epochæ™‚é–“: {np.mean(epoch_times):.2f}s")
    print(f"   â€¢ æœ€å¿«epochæ™‚é–“: {np.min(epoch_times):.2f}s")
    print(f"   â€¢ æœ€æ…¢epochæ™‚é–“: {np.max(epoch_times):.2f}s")
    print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")

    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    if best_val_acc > 0:
        model.load_state_dict(torch.load("best_model_checkpoint_paper.pth"))
        print("   â€¢ å·²è¼‰å…¥æœ€ä½³è«–æ–‡æ¨™æº–æ¨¡å‹æ¬Šé‡")

    return train_losses, train_accs, val_accs, epoch_times, total_training_time


def quick_validate(model, testloader, device):
    """å¿«é€Ÿé©—è­‰"""
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return 100.0 * correct / total


def evaluate_model_with_visualization(model, testloader, device, classes):
    """è©•ä¼°æ¨¡å‹ä¸¦ç”Ÿæˆå¯è¦–åŒ–çµæœ"""
    print("\nğŸ“Š è©•ä¼°CPUå„ªåŒ–æ¨¡å‹ä¸¦ç”Ÿæˆå¯è¦–åŒ–...")

    model.eval()
    all_predictions = []
    all_labels = []
    correct = 0
    total = 0
    class_correct = [0] * 10
    class_total = [0] * 10

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)

            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            for i in range(labels.size(0)):
                label = labels[i]
                class_correct[label] += (predicted[i] == label).item()
                class_total[label] += 1

    overall_acc = 100.0 * correct / total
    print(f"   âœ“ CPUæ¨¡å‹æ•´é«”æº–ç¢ºç‡: {overall_acc:.2f}%")

    # 1. å„é¡åˆ¥æº–ç¢ºç‡æ¢å½¢åœ–
    plt.figure(figsize=(15, 10))

    plt.subplot(2, 2, 1)
    class_accs = []
    for i in range(10):
        if class_total[i] > 0:
            acc = 100.0 * class_correct[i] / class_total[i]
            class_accs.append(acc)
        else:
            class_accs.append(0)

    bars = plt.bar(classes, class_accs, color=plt.cm.tab10(np.arange(10)))
    plt.title(
        "CPU-Optimized gMLP: Accuracy of Each Category", fontsize=14, fontweight="bold"
    )
    plt.ylabel("Accuracy (%)")
    plt.xticks(rotation=45)
    plt.grid(axis="y", alpha=0.3)

    # åœ¨æŸ±ç‹€åœ–ä¸Šæ·»åŠ æ•¸å€¼
    for bar, acc in zip(bars, class_accs):
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{acc:.1f}%",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # 2. æ··æ·†çŸ©é™£
    plt.subplot(2, 2, 2)
    cm = confusion_matrix(all_labels, all_predictions)
    sns.heatmap(
        cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes
    )
    plt.title("CPU-Optimized Confusion Matrix", fontsize=14, fontweight="bold")
    plt.xlabel("Predicted Category")
    plt.ylabel("True Category")

    # 3. æ¨™æº–åŒ–æ··æ·†çŸ©é™£
    plt.subplot(2, 2, 3)
    cm_normalized = confusion_matrix(all_labels, all_predictions, normalize="true")
    sns.heatmap(
        cm_normalized,
        annot=True,
        fmt=".2f",
        cmap="Blues",
        xticklabels=classes,
        yticklabels=classes,
    )
    plt.title(
        "CPU-Optimized Normalized Confusion Matrix", fontsize=14, fontweight="bold"
    )
    plt.xlabel("Predicted Category")
    plt.ylabel("True Category")

    # 4. é¡åˆ¥åˆ†ä½ˆ
    plt.subplot(2, 2, 4)
    unique, counts = np.unique(all_labels, return_counts=True)
    plt.pie(
        counts,
        labels=[classes[i] for i in unique],
        autopct="%1.1f%%",
        colors=plt.cm.tab10(np.arange(len(unique))),
    )
    plt.title("CPU Test Set Category Distribution", fontsize=14, fontweight="bold")

    plt.tight_layout()
    plt.savefig("gmlp_cpu_evaluation_results.png", dpi=300, bbox_inches="tight")
    plt.show()

    # æ‰“å°è©³ç´°å ±å‘Š
    print(f"\nğŸ“‹ CPUå„ªåŒ–æ¨¡å‹è©³ç´°åˆ†é¡å ±å‘Š:")
    target_names = [f"{i}_{classes[i]}" for i in range(10)]
    report = classification_report(
        all_labels, all_predictions, target_names=target_names, digits=3
    )
    print(report)

    return overall_acc


def plot_training_history(train_losses, train_accs, val_accs, epoch_times=None):
    """ç¹ªè£½CPUè¨“ç·´æ­·å²"""
    print("\nğŸ“ˆ ç¹ªè£½CPUè¨“ç·´æ­·å²...")

    # èª¿æ•´åœ–ç‰‡å¤§å°ä»¥å®¹ç´æ™‚é–“åœ–è¡¨
    if epoch_times is not None:
        plt.figure(figsize=(20, 5))
        subplot_count = 4
    else:
        plt.figure(figsize=(15, 5))
        subplot_count = 3

    # æå¤±æ›²ç·š
    plt.subplot(1, subplot_count, 1)
    plt.plot(train_losses, "b-", linewidth=2, label="CPU Training Loss")
    plt.title("CPU-Optimized Training Loss Curve", fontsize=14, fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # æº–ç¢ºç‡æ›²ç·š
    plt.subplot(1, subplot_count, 2)
    plt.plot(train_accs, "g-", linewidth=2, label="CPU Training Accuracy")
    plt.plot(val_accs, "r-", linewidth=2, label="CPU Validation Accuracy")
    plt.title("CPU-Optimized Accuracy Curve", fontsize=14, fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # æº–ç¢ºç‡å·®ç•°
    plt.subplot(1, subplot_count, 3)
    diff = np.array(train_accs) - np.array(val_accs)
    plt.plot(diff, "purple", linewidth=2, label="CPU Train-Val Difference")
    plt.title("CPU Overfitting Monitor", fontsize=14, fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy Difference (%)")
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color="black", linestyle="--", alpha=0.5)
    plt.legend()

    # æ™‚é–“çµ±è¨ˆåœ–ï¼ˆå¦‚æœæä¾›äº†æ™‚é–“æ•¸æ“šï¼‰
    if epoch_times is not None:
        plt.subplot(1, subplot_count, 4)
        plt.plot(epoch_times, "orange", linewidth=2, marker="o", label="CPU Epoch Time")
        plt.title("CPU Training Time per Epoch", fontsize=14, fontweight="bold")
        plt.xlabel("Epoch")
        plt.ylabel("Time (seconds)")
        plt.grid(True, alpha=0.3)
        plt.legend()

    plt.tight_layout()
    plt.savefig("gmlp_cpu_training_history.png", dpi=300, bbox_inches="tight")
    plt.show()


def visualize_sample_predictions(model, testloader, device, classes, num_samples=12):
    """å¯è¦–åŒ–CPUæ¨¡å‹æ¨£æœ¬é æ¸¬çµæœ"""
    print(f"\nğŸ” å¯è¦–åŒ–CPUæ¨¡å‹ {num_samples} å€‹æ¨£æœ¬é æ¸¬...")

    model.eval()

    # ç²å–ä¸€æ‰¹æ•¸æ“š
    dataiter = iter(testloader)
    images, labels = next(dataiter)
    images, labels = images.to(device), labels.to(device)

    with torch.no_grad():
        outputs = model(images)
        _, predictions = torch.max(outputs, 1)
        probabilities = torch.softmax(outputs, 1)

    # ç¹ªè£½çµæœ
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    fig.suptitle(
        "CPU-Optimized gMLP Prediction Results", fontsize=16, fontweight="bold"
    )

    for i in range(min(num_samples, len(images))):
        ax = axes[i // 4, i % 4]

        # åæ¨™æº–åŒ–åœ–åƒ
        img = images[i].cpu()
        mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)
        std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)
        img = img * std + mean
        img = torch.clamp(img, 0, 1)

        ax.imshow(img.permute(1, 2, 0))

        true_label = classes[labels[i]]
        pred_label = classes[predictions[i]]
        confidence = probabilities[i, predictions[i]].item()

        # è¨­ç½®é¡è‰²ï¼ˆæ­£ç¢º=ç¶ è‰²ï¼ŒéŒ¯èª¤=ç´…è‰²ï¼‰
        color = "green" if labels[i] == predictions[i] else "red"

        ax.set_title(
            f"True: {true_label}\nPred: {pred_label}\nConf: {confidence:.2f}",
            color=color,
            fontweight="bold",
        )
        ax.axis("off")

    plt.tight_layout()
    plt.savefig("gmlp_cpu_sample_predictions.png", dpi=300, bbox_inches="tight")
    plt.show()


def compare_model_architectures():
    """æ¯”è¼ƒä¸åŒgMLPæ¨¡å‹æ¶æ§‹çš„è¦æ ¼"""
    print("\nğŸ“‹ è«–æ–‡æ¨™æº– gMLP æ¨¡å‹æ¶æ§‹æ¯”è¼ƒ:")
    print("=" * 80)
    print(
        f"{'æ¨¡å‹':<8} {'æ·±åº¦':<6} {'ç¶­åº¦':<8} {'FFNå€æ•¸':<8} {'éš¨æ©Ÿæ·±åº¦':<10} {'ç›®æ¨™åƒæ•¸(M)':<12}"
    )
    print("-" * 80)

    models_config = {
        "Ti": {
            "depth": 30,
            "dim": 128,
            "ff_mult": 6,
            "prob_survival": 1.00,
            "params": 5.9,
        },
        "S": {
            "depth": 30,
            "dim": 256,
            "ff_mult": 6,
            "prob_survival": 0.95,
            "params": 19.5,
        },
        "B": {
            "depth": 30,
            "dim": 512,
            "ff_mult": 6,
            "prob_survival": 0.80,
            "params": 73.4,
        },
    }

    for name, config in models_config.items():
        print(
            f"gMLP-{name:<3} {config['depth']:<6} {config['dim']:<8} {config['ff_mult']:<8} "
            f"{config['prob_survival']:<10.2f} {config['params']:<12.1f}"
        )

    print("-" * 80)
    print("ğŸ’¡ å»ºè­°:")
    print("   â€¢ gMLP-Ti: å¿«é€Ÿå¯¦é©—å’Œæ¦‚å¿µé©—è­‰")
    print("   â€¢ gMLP-S:  å¹³è¡¡æ€§èƒ½å’Œè¨ˆç®—è³‡æº (æ¨è–¦)")
    print("   â€¢ gMLP-B:  è¿½æ±‚æœ€ä½³æ€§èƒ½ (éœ€è¦æ›´å¤šè³‡æº)")
    print("=" * 80)


def main():
    print("ğŸ–¼ï¸ è«–æ–‡æ¨™æº– gMLP åœ–åƒåˆ†é¡æ¸¬è©¦")
    print("=" * 60)
    print("ğŸ“„ åŸºæ–¼å®˜æ–¹ImageNet-1Kè¶…åƒæ•¸é…ç½®")
    print("=" * 60)

    # é¡¯ç¤ºæ¨¡å‹æ¶æ§‹æ¯”è¼ƒ
    compare_model_architectures()

    try:
        # 1. åŠ è¼‰è«–æ–‡æ¨™æº–æ•¸æ“š
        trainloader, testloader, classes = load_cifar10_data_enhanced(quick_test=True)

        # 2. å‰µå»ºè«–æ–‡æ¨™æº–æ¨¡å‹ (å¯é¸æ“‡æ¨¡å‹å¤§å°)
        model_size = "S"  # é¸æ“‡ 'Ti', 'S', æˆ– 'B'
        model, device = create_optimized_gmlp_model(model_size=model_size)

        # 3. è«–æ–‡æ¨™æº–è¨“ç·´
        train_losses, train_accs, val_accs, epoch_times, total_training_time = (
            train_model_with_scheduler(
                model,
                trainloader,
                testloader,
                device,
                epochs=300,  # è«–æ–‡æ¨™æº–300 epochs
            )
        )

        # 4. ç¹ªè£½è¨“ç·´æ­·å²
        plot_training_history(train_losses, train_accs, val_accs, epoch_times)

        # 5. è©³ç´°è©•ä¼°èˆ‡å¯è¦–åŒ–
        accuracy = evaluate_model_with_visualization(model, testloader, device, classes)

        # 6. å¯è¦–åŒ–é æ¸¬æ¨£æœ¬
        visualize_sample_predictions(model, testloader, device, classes)

        # 7. ä¿å­˜è«–æ–‡æ¨™æº–æ¨¡å‹
        torch.save(model.state_dict(), "gmlp_paper_model.pth")
        print("\nğŸ’¾ è«–æ–‡æ¨™æº–æ¨¡å‹å·²ä¿å­˜ç‚º 'gmlp_paper_model.pth'")

        print("\n" + "=" * 60)
        print("âœ… è«–æ–‡æ¨™æº–æ¸¬è©¦å®Œæˆï¼")
        print(f"\nğŸ“ˆ è«–æ–‡æ¨™æº–æœ€çµ‚çµæœ:")
        print(f"   â€¢ æœ€çµ‚æ¸¬è©¦æº–ç¢ºç‡: {accuracy:.2f}%")
        print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(val_accs):.2f}%")
        print(f"   â€¢ è¨“ç·´-é©—è­‰å·®ç•°: {train_accs[-1] - val_accs[-1]:.2f}%")
        print(
            f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.2f}s ({total_training_time/60:.2f}min)"
        )
        print(f"   â€¢ å¹³å‡æ¯epochæ™‚é–“: {np.mean(epoch_times):.2f}s")

        print(f"\nğŸ“„ è«–æ–‡é…ç½®ç‰¹æ€§:")
        print(f"   â€¢ æ¨¡å‹æ¶æ§‹: gMLP-{model_size}")
        print(f"   â€¢ æ¨¡å‹åƒæ•¸: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   â€¢ è«–æ–‡æ¨™æº–: depth=30, dim=256, ff_mult=6, prob_survival=0.95")
        print(f"   â€¢ æ•¸æ“šé‡: 15,000è¨“ç·´æ¨£æœ¬, 3,000æ¸¬è©¦æ¨£æœ¬")
        print(f"   â€¢ æ‰¹æ¬¡å¤§å°: 128 (æ¥è¿‘è«–æ–‡è¦æ¨¡)")
        print(f"   â€¢ è¨“ç·´ç­–ç•¥: 300 epochs + cosineé€€ç« + 10K warmup")

        print(f"\nğŸ¯ è«–æ–‡æ¨™æº–ç’°å¢ƒå»ºè­°:")
        if accuracy < 70:
            print(f"   â€¢ 300 epochså¾Œæº–ç¢ºåº¦ä»éœ€æå‡ï¼Œè€ƒæ…®èª¿æ•´æ¨¡å‹æ¶æ§‹")
            print(f"   â€¢ å¯èƒ½éœ€è¦æ›´å¤§çš„æ•¸æ“šé›†æˆ–æ›´é•·çš„è¨“ç·´")
            print(f"   â€¢ æª¢æŸ¥æ•¸æ“šé è™•ç†æ˜¯å¦èˆ‡è«–æ–‡ä¸€è‡´")
        elif accuracy < 80:
            print(f"   â€¢ è«–æ–‡æ¨™æº–æ¨¡å‹è¡¨ç¾è‰¯å¥½ï¼")
            print(f"   â€¢ 300å€‹epochså……åˆ†åˆ©ç”¨äº†è«–æ–‡é…ç½®")
            print(f"   â€¢ å¯è€ƒæ…®å¾®èª¿è¶…åƒæ•¸é€²ä¸€æ­¥å„ªåŒ–")
        else:
            print(f"   â€¢ è«–æ–‡æ¨™æº–æ¨¡å‹è¡¨ç¾å„ªç§€ï¼")
            print(f"   â€¢ æˆåŠŸå¾©ç¾è«–æ–‡ç´šåˆ¥çš„è¨“ç·´æ•ˆæœ")
            print(f"   â€¢ é©åˆç™¼è¡¨æˆ–å¯¦éš›æ‡‰ç”¨éƒ¨ç½²")

        # æ³›åŒ–èƒ½åŠ›åˆ†æ
        overfitting_diff = train_accs[-1] - val_accs[-1]
        if overfitting_diff > 15:
            print(f"\nâš ï¸  éœ€è¦æ³¨æ„:")
            print(f"   â€¢ è¨“ç·´-é©—è­‰å·®ç•°è¼ƒå¤§ ({overfitting_diff:.2f}%)")
            print(f"   â€¢ å¯èƒ½éœ€è¦æ›´å¤šæ•¸æ“šæˆ–æ›´å¼·æ­£å‰‡åŒ–")
        elif overfitting_diff > 8:
            print(f"\nğŸ”¶ æ³›åŒ–è¡¨ç¾:")
            print(f"   â€¢ è¨“ç·´-é©—è­‰å·®ç•°é©ä¸­ ({overfitting_diff:.2f}%)")
            print(f"   â€¢ è«–æ–‡é…ç½®å–å¾—è‰¯å¥½å¹³è¡¡")
        else:
            print(f"\nâœ… å„ªç§€æ³›åŒ–:")
            print(f"   â€¢ è¨“ç·´-é©—è­‰å·®ç•°å¾ˆå° ({overfitting_diff:.2f}%)")
            print(f"   â€¢ è«–æ–‡é…ç½®å¯¦ç¾å‡ºè‰²æ³›åŒ–èƒ½åŠ›")

        print(f"\nğŸš€ è«–æ–‡æ¨™æº–ç¸½çµ:")
        print(f"   â€¢ è¨“ç·´ç­–ç•¥: 300 epochs + è«–æ–‡æ¨™æº–è¶…åƒæ•¸")
        print(f"   â€¢ æ­£å‰‡åŒ–: æ¨™ç±¤å¹³æ»‘0.1 + æ¬Šé‡è¡°æ¸›0.05 + éš¨æ©Ÿæ·±åº¦0.8")
        print(f"   â€¢ å­¸ç¿’ç‡: å³°å€¼1e-3 + cosineé€€ç« + 10K warmup")
        print(f"   â€¢ å„ªåŒ–å™¨: AdamW + æ¢¯åº¦è£å‰ª1.0 + epsilon 1e-6")
        print(f"   â€¢ ç›®æ¨™: å¾©ç¾è«–æ–‡ImageNet-1Kç´šåˆ¥çš„è¨“ç·´æ•ˆæœ")

    except Exception as e:
        print(f"âŒ è«–æ–‡æ¨™æº–æ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()

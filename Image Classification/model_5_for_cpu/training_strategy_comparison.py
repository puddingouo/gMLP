<<<<<<< HEAD
"""
è¨“ç·´ç­–ç•¥å°æ¯”å¯¦é©—
æ¯”è¼ƒã€Œå°‘é‡epoch+å¤§æ•¸æ“šé‡ã€vsã€Œå¤šé‡epoch+å°æ•¸æ“šé‡ã€çš„æ•ˆæœ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
from g_mlp_pytorch import gMLPVision
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report


def load_data_for_strategy(strategy="large_data"):
    """æ ¹æ“šç­–ç•¥åŠ è¼‰ä¸åŒæ•¸æ“šé…ç½®"""
    print(f"ğŸ“¦ åŠ è¼‰æ•¸æ“š - ç­–ç•¥: {strategy}")

    # ç°¡åŒ–çš„æ•¸æ“šå¢å¼·
    transform_train = transforms.Compose(
        [
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    transform_test = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    trainset = torchvision.datasets.CIFAR10(
        root="./data", train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root="./data", train=False, download=True, transform=transform_test
    )

    if strategy == "large_data":
        # ç­–ç•¥Aï¼šå¤§æ•¸æ“šé‡ + å°‘epoch
        trainset = Subset(trainset, range(4000))  # æ›´å¤šè¨“ç·´æ•¸æ“š
        testset = Subset(testset, range(800))
        batch_size = 64  # è¼ƒå¤§æ‰¹æ¬¡
        print("   âœ“ ç­–ç•¥Aï¼šå¤§æ•¸æ“šé‡ç­–ç•¥")
        print(f"   âœ“ è¨“ç·´æ¨£æœ¬: {len(trainset)}, æ‰¹æ¬¡å¤§å°: {batch_size}")
    else:
        # ç­–ç•¥Bï¼šå°æ•¸æ“šé‡ + å¤šepoch
        trainset = Subset(trainset, range(1200))  # è¼ƒå°‘è¨“ç·´æ•¸æ“š
        testset = Subset(testset, range(300))
        batch_size = 32  # è¼ƒå°æ‰¹æ¬¡
        print("   âœ“ ç­–ç•¥Bï¼šå°æ•¸æ“šé‡ç­–ç•¥")
        print(f"   âœ“ è¨“ç·´æ¨£æœ¬: {len(trainset)}, æ‰¹æ¬¡å¤§å°: {batch_size}")

    trainloader = DataLoader(
        trainset, batch_size=batch_size, shuffle=True, num_workers=0
    )
    testloader = DataLoader(
        testset, batch_size=batch_size, shuffle=False, num_workers=0
    )

    classes = [
        "Airplane",
        "Automobile",
        "Bird",
        "Cat",
        "Deer",
        "Dog",
        "Frog",
        "Horse",
        "Ship",
        "Truck",
    ]

    return trainloader, testloader, classes


def create_model():
    """å‰µå»ºçµ±ä¸€çš„æ¨¡å‹é…ç½®"""
    torch.set_num_threads(4)

    model = gMLPVision(
        image_size=32,
        patch_size=8,
        num_classes=10,
        dim=128,  # è¼ƒå°æ¨¡å‹åŠ é€Ÿå°æ¯”
        depth=3,
        ff_mult=3,
        channels=3,
        prob_survival=0.8,
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    return model, device


def train_with_strategy(model, trainloader, testloader, device, strategy="large_data"):
    """æ ¹æ“šç­–ç•¥é€²è¡Œè¨“ç·´"""

    if strategy == "large_data":
        epochs = 6  # å°‘é‡epoch
        lr = 0.003  # ç¨é«˜å­¸ç¿’ç‡
        print(f"\nğŸ‹ï¸ ç­–ç•¥Aè¨“ç·´ï¼š{epochs} epochsï¼Œå¤§æ•¸æ“šé‡")
    else:
        epochs = 18  # å¤šé‡epoch
        lr = 0.002  # ç¨ä½å­¸ç¿’ç‡
        print(f"\nğŸ‹ï¸ ç­–ç•¥Bè¨“ç·´ï¼š{epochs} epochsï¼Œå°æ•¸æ“šé‡")

    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.005)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=3)

    train_losses = []
    train_accs = []
    val_accs = []
    epoch_times = []

    start_time = time.time()
    best_val_acc = 0
    patience = 4
    patience_counter = 0

    for epoch in range(epochs):
        epoch_start = time.time()

        # è¨“ç·´
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for i, (inputs, labels) in enumerate(trainloader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        train_loss = running_loss / len(trainloader)
        train_acc = 100.0 * correct / total

        # é©—è­‰
        model.eval()
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for inputs, labels in testloader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

        val_acc = 100.0 * val_correct / val_total

        train_losses.append(train_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        epoch_time = time.time() - epoch_start
        epoch_times.append(epoch_time)

        scheduler.step()

        print(
            f"Epoch {epoch+1:2d}/{epochs}: "
            f"è¨“ç·´æº–ç¢ºç‡={train_acc:5.2f}%, "
            f"é©—è­‰æº–ç¢ºç‡={val_acc:5.2f}%, "
            f"æ™‚é–“={epoch_time:4.1f}s"
        )

        # æ—©åœ
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"   æ—©åœæ–¼ç¬¬ {epoch+1} epoch")
                break

    total_time = time.time() - start_time

    return {
        "train_losses": train_losses,
        "train_accs": train_accs,
        "val_accs": val_accs,
        "epoch_times": epoch_times,
        "total_time": total_time,
        "best_val_acc": best_val_acc,
        "final_train_acc": train_accs[-1],
        "final_val_acc": val_accs[-1],
        "epochs_used": len(train_accs),
    }


def compare_strategies():
    """åŸ·è¡Œç­–ç•¥å°æ¯”å¯¦é©—"""
    print("ğŸ”¬ é–‹å§‹è¨“ç·´ç­–ç•¥å°æ¯”å¯¦é©—")
    print("=" * 60)

    results = {}

    # ç­–ç•¥Aï¼šå¤§æ•¸æ“šé‡ + å°‘epoch
    print("\n" + "=" * 30 + " ç­–ç•¥A " + "=" * 30)
    trainloader_a, testloader_a, classes = load_data_for_strategy("large_data")
    model_a, device = create_model()
    results["strategy_a"] = train_with_strategy(
        model_a, trainloader_a, testloader_a, device, "large_data"
    )

    # ç­–ç•¥Bï¼šå°æ•¸æ“šé‡ + å¤šepoch
    print("\n" + "=" * 30 + " ç­–ç•¥B " + "=" * 30)
    trainloader_b, testloader_b, classes = load_data_for_strategy("small_data")
    model_b, device = create_model()
    results["strategy_b"] = train_with_strategy(
        model_b, trainloader_b, testloader_b, device, "small_data"
    )

    return results


def plot_comparison(results):
    """ç¹ªè£½å°æ¯”çµæœ"""
    print("\nğŸ“ˆ ç¹ªè£½å°æ¯”çµæœ...")

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle(
        "è¨“ç·´ç­–ç•¥å°æ¯”ï¼šå¤§æ•¸æ“šé‡+å°‘epoch vs å°æ•¸æ“šé‡+å¤šepoch",
        fontsize=16,
        fontweight="bold",
    )

    # ç²å–æ•¸æ“š
    strategy_a = results["strategy_a"]
    strategy_b = results["strategy_b"]

    # 1. è¨“ç·´æå¤±å°æ¯”
    ax = axes[0, 0]
    epochs_a = range(1, len(strategy_a["train_losses"]) + 1)
    epochs_b = range(1, len(strategy_b["train_losses"]) + 1)
    ax.plot(
        epochs_a,
        strategy_a["train_losses"],
        "b-",
        linewidth=2,
        label="ç­–ç•¥A: å¤§æ•¸æ“š+å°‘epoch",
    )
    ax.plot(
        epochs_b,
        strategy_b["train_losses"],
        "r-",
        linewidth=2,
        label="ç­–ç•¥B: å°æ•¸æ“š+å¤šepoch",
    )
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Training Loss")
    ax.set_title("è¨“ç·´æå¤±å°æ¯”")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 2. è¨“ç·´æº–ç¢ºç‡å°æ¯”
    ax = axes[0, 1]
    ax.plot(
        epochs_a, strategy_a["train_accs"], "b-", linewidth=2, label="ç­–ç•¥A: è¨“ç·´æº–ç¢ºç‡"
    )
    ax.plot(
        epochs_a, strategy_a["val_accs"], "b--", linewidth=2, label="ç­–ç•¥A: é©—è­‰æº–ç¢ºç‡"
    )
    ax.plot(
        epochs_b, strategy_b["train_accs"], "r-", linewidth=2, label="ç­–ç•¥B: è¨“ç·´æº–ç¢ºç‡"
    )
    ax.plot(
        epochs_b, strategy_b["val_accs"], "r--", linewidth=2, label="ç­–ç•¥B: é©—è­‰æº–ç¢ºç‡"
    )
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Accuracy (%)")
    ax.set_title("æº–ç¢ºç‡å°æ¯”")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 3. æ¯epochæ™‚é–“å°æ¯”
    ax = axes[0, 2]
    ax.plot(
        epochs_a,
        strategy_a["epoch_times"],
        "b-",
        linewidth=2,
        marker="o",
        label="ç­–ç•¥A",
    )
    ax.plot(
        epochs_b,
        strategy_b["epoch_times"],
        "r-",
        linewidth=2,
        marker="s",
        label="ç­–ç•¥B",
    )
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Time (seconds)")
    ax.set_title("æ¯Epochè¨“ç·´æ™‚é–“")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 4. éæ“¬åˆç¨‹åº¦å°æ¯”
    ax = axes[1, 0]
    diff_a = np.array(strategy_a["train_accs"]) - np.array(strategy_a["val_accs"])
    diff_b = np.array(strategy_b["train_accs"]) - np.array(strategy_b["val_accs"])
    ax.plot(epochs_a, diff_a, "b-", linewidth=2, label="ç­–ç•¥A: éæ“¬åˆç¨‹åº¦")
    ax.plot(epochs_b, diff_b, "r-", linewidth=2, label="ç­–ç•¥B: éæ“¬åˆç¨‹åº¦")
    ax.axhline(y=0, color="black", linestyle="--", alpha=0.5)
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Train-Val Accuracy Difference (%)")
    ax.set_title("éæ“¬åˆç¨‹åº¦å°æ¯”")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 5. æ•ˆç‡å°æ¯”æ¢å½¢åœ–
    ax = axes[1, 1]
    strategies = ["ç­–ç•¥A\n(å¤§æ•¸æ“š+å°‘epoch)", "ç­–ç•¥B\n(å°æ•¸æ“š+å¤šepoch)"]
    times = [strategy_a["total_time"], strategy_b["total_time"]]
    accuracies = [strategy_a["best_val_acc"], strategy_b["best_val_acc"]]

    x = np.arange(len(strategies))
    width = 0.35

    ax2 = ax.twinx()
    bars1 = ax.bar(x - width / 2, times, width, label="ç¸½è¨“ç·´æ™‚é–“(s)", color="skyblue")
    bars2 = ax2.bar(
        x + width / 2, accuracies, width, label="æœ€ä½³é©—è­‰æº–ç¢ºç‡(%)", color="lightcoral"
    )

    ax.set_xlabel("ç­–ç•¥")
    ax.set_ylabel("æ™‚é–“ (seconds)", color="blue")
    ax2.set_ylabel("æº–ç¢ºç‡ (%)", color="red")
    ax.set_title("æ•ˆç‡ vs æº–ç¢ºç‡")
    ax.set_xticks(x)
    ax.set_xticklabels(strategies)

    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar, time in zip(bars1, times):
        ax.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{time:.1f}s",
            ha="center",
            va="bottom",
        )
    for bar, acc in zip(bars2, accuracies):
        ax2.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{acc:.1f}%",
            ha="center",
            va="bottom",
        )

    # 6. è©³ç´°æ•¸æ“šå°æ¯”è¡¨
    ax = axes[1, 2]
    ax.axis("off")

    # å‰µå»ºå°æ¯”è¡¨æ ¼
    comparison_data = [
        ["æŒ‡æ¨™", "ç­–ç•¥A (å¤§æ•¸æ“š)", "ç­–ç•¥B (å°æ•¸æ“š)"],
        ["è¨“ç·´æ¨£æœ¬æ•¸", "4000", "1200"],
        ["è¨“ç·´epochs", f"{strategy_a['epochs_used']}", f"{strategy_b['epochs_used']}"],
        [
            "ç¸½è¨“ç·´æ™‚é–“",
            f"{strategy_a['total_time']:.1f}s",
            f"{strategy_b['total_time']:.1f}s",
        ],
        [
            "æœ€ä½³é©—è­‰æº–ç¢ºç‡",
            f"{strategy_a['best_val_acc']:.2f}%",
            f"{strategy_b['best_val_acc']:.2f}%",
        ],
        [
            "æœ€çµ‚è¨“ç·´æº–ç¢ºç‡",
            f"{strategy_a['final_train_acc']:.2f}%",
            f"{strategy_b['final_train_acc']:.2f}%",
        ],
        [
            "æœ€çµ‚é©—è­‰æº–ç¢ºç‡",
            f"{strategy_a['final_val_acc']:.2f}%",
            f"{strategy_b['final_val_acc']:.2f}%",
        ],
        [
            "éæ“¬åˆç¨‹åº¦",
            f"{strategy_a['final_train_acc']-strategy_a['final_val_acc']:.2f}%",
            f"{strategy_b['final_train_acc']-strategy_b['final_val_acc']:.2f}%",
        ],
        [
            "å¹³å‡æ¯epochæ™‚é–“",
            f"{np.mean(strategy_a['epoch_times']):.1f}s",
            f"{np.mean(strategy_b['epoch_times']):.1f}s",
        ],
    ]

    table = ax.table(
        cellText=comparison_data[1:],
        colLabels=comparison_data[0],
        cellLoc="center",
        loc="center",
        bbox=[0, 0, 1, 1],
    )
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 2)

    # è¨­ç½®è¡¨æ ¼æ¨£å¼
    for i in range(len(comparison_data)):
        for j in range(3):
            if i == 0:  # æ¨™é¡Œè¡Œ
                table[(i, j)].set_facecolor("#4CAF50")
                table[(i, j)].set_text_props(weight="bold", color="white")
            elif j == 0:  # æŒ‡æ¨™åˆ—
                table[(i, j)].set_facecolor("#E8F5E8")
            else:
                table[(i, j)].set_facecolor("#F9F9F9")

    ax.set_title("è©³ç´°å°æ¯”æ•¸æ“š", fontweight="bold")

    plt.tight_layout()
    plt.savefig("training_strategy_comparison.png", dpi=300, bbox_inches="tight")
    plt.show()


def print_analysis(results):
    """æ‰“å°åˆ†æçµæœ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š è¨“ç·´ç­–ç•¥åˆ†æå ±å‘Š")
    print("=" * 60)

    strategy_a = results["strategy_a"]
    strategy_b = results["strategy_b"]

    print(f"\nğŸ” ç­–ç•¥Aï¼ˆå¤§æ•¸æ“š+å°‘epochï¼‰:")
    print(f"   â€¢ è¨“ç·´æ¨£æœ¬: 4000")
    print(f"   â€¢ ä½¿ç”¨epochs: {strategy_a['epochs_used']}")
    print(f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {strategy_a['total_time']:.1f}s")
    print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {strategy_a['best_val_acc']:.2f}%")
    print(
        f"   â€¢ éæ“¬åˆç¨‹åº¦: {strategy_a['final_train_acc']-strategy_a['final_val_acc']:.2f}%"
    )

    print(f"\nğŸ” ç­–ç•¥Bï¼ˆå°æ•¸æ“š+å¤šepochï¼‰:")
    print(f"   â€¢ è¨“ç·´æ¨£æœ¬: 1200")
    print(f"   â€¢ ä½¿ç”¨epochs: {strategy_b['epochs_used']}")
    print(f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {strategy_b['total_time']:.1f}s")
    print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {strategy_b['best_val_acc']:.2f}%")
    print(
        f"   â€¢ éæ“¬åˆç¨‹åº¦: {strategy_b['final_train_acc']-strategy_b['final_val_acc']:.2f}%"
    )

    print(f"\nğŸ’¡ ç­–ç•¥å„ªåŠ£åˆ†æ:")

    # æº–ç¢ºç‡æ¯”è¼ƒ
    if strategy_a["best_val_acc"] > strategy_b["best_val_acc"]:
        acc_winner = "ç­–ç•¥Aï¼ˆå¤§æ•¸æ“šï¼‰"
        acc_diff = strategy_a["best_val_acc"] - strategy_b["best_val_acc"]
    else:
        acc_winner = "ç­–ç•¥Bï¼ˆå°æ•¸æ“šï¼‰"
        acc_diff = strategy_b["best_val_acc"] - strategy_a["best_val_acc"]

    # æ™‚é–“æ¯”è¼ƒ
    if strategy_a["total_time"] < strategy_b["total_time"]:
        time_winner = "ç­–ç•¥Aï¼ˆå¤§æ•¸æ“šï¼‰"
        time_diff = strategy_b["total_time"] - strategy_a["total_time"]
    else:
        time_winner = "ç­–ç•¥Bï¼ˆå°æ•¸æ“šï¼‰"
        time_diff = strategy_a["total_time"] - strategy_b["total_time"]

    # éæ“¬åˆæ¯”è¼ƒ
    overfitting_a = abs(strategy_a["final_train_acc"] - strategy_a["final_val_acc"])
    overfitting_b = abs(strategy_b["final_train_acc"] - strategy_b["final_val_acc"])

    if overfitting_a < overfitting_b:
        overfitting_winner = "ç­–ç•¥Aï¼ˆå¤§æ•¸æ“šï¼‰"
        overfitting_diff = overfitting_b - overfitting_a
    else:
        overfitting_winner = "ç­–ç•¥Bï¼ˆå°æ•¸æ“šï¼‰"
        overfitting_diff = overfitting_a - overfitting_b

    print(f"\nğŸ† ç¶œåˆæ¯”è¼ƒ:")
    print(f"   â€¢ æº–ç¢ºç‡å„ªå‹: {acc_winner} (é ˜å…ˆ {acc_diff:.2f}%)")
    print(f"   â€¢ è¨“ç·´é€Ÿåº¦å„ªå‹: {time_winner} (å¿« {time_diff:.1f}s)")
    print(f"   â€¢ æ³›åŒ–èƒ½åŠ›å„ªå‹: {overfitting_winner} (éæ“¬åˆå°‘ {overfitting_diff:.2f}%)")

    print(f"\nğŸ¯ å¯¦ç”¨å»ºè­°:")
    print(f"   â€¢ å¦‚æœè¿½æ±‚æœ€é«˜æº–ç¢ºç‡: é¸æ“‡{acc_winner}")
    print(f"   â€¢ å¦‚æœæ³¨é‡è¨“ç·´æ•ˆç‡: é¸æ“‡{time_winner}")
    print(f"   â€¢ å¦‚æœè¦æ±‚æ³›åŒ–èƒ½åŠ›: é¸æ“‡{overfitting_winner}")

    # æ ¹æ“šçµæœçµ¦å‡ºå…·é«”å»ºè­°
    if (
        strategy_a["best_val_acc"] > strategy_b["best_val_acc"]
        and strategy_a["total_time"] < strategy_b["total_time"]
    ):
        print(f"\nâœ… çµè«–: ç­–ç•¥Aï¼ˆå¤§æ•¸æ“š+å°‘epochï¼‰åœ¨æº–ç¢ºç‡å’Œæ•ˆç‡ä¸Šéƒ½æ›´å„ªç§€ï¼")
    elif (
        strategy_b["best_val_acc"] > strategy_a["best_val_acc"]
        and strategy_b["total_time"] < strategy_a["total_time"]
    ):
        print(f"\nâœ… çµè«–: ç­–ç•¥Bï¼ˆå°æ•¸æ“š+å¤šepochï¼‰åœ¨æº–ç¢ºç‡å’Œæ•ˆç‡ä¸Šéƒ½æ›´å„ªç§€ï¼")
    else:
        print(f"\nâš–ï¸ çµè«–: å…©ç¨®ç­–ç•¥å„æœ‰å„ªåŠ£ï¼Œéœ€è¦æ ¹æ“šå…·é«”éœ€æ±‚é¸æ“‡")
        if abs(strategy_a["best_val_acc"] - strategy_b["best_val_acc"]) < 2:
            print(f"   æº–ç¢ºç‡ç›¸è¿‘ï¼Œå»ºè­°é¸æ“‡è¨“ç·´æ™‚é–“æ›´çŸ­çš„ç­–ç•¥")
        else:
            print(f"   å¦‚æœæº–ç¢ºç‡å·®è·è¼ƒå¤§ï¼Œå»ºè­°å„ªå…ˆè€ƒæ…®æº–ç¢ºç‡")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ”¬ gMLPè¨“ç·´ç­–ç•¥å°æ¯”å¯¦é©—")
    print("æ¯”è¼ƒã€Œå¤§æ•¸æ“šé‡+å°‘epochã€vsã€Œå°æ•¸æ“šé‡+å¤šepochã€")
    print("=" * 60)

    try:
        # åŸ·è¡Œå°æ¯”å¯¦é©—
        results = compare_strategies()

        # ç¹ªè£½å°æ¯”åœ–è¡¨
        plot_comparison(results)

        # æ‰“å°åˆ†æå ±å‘Š
        print_analysis(results)

        print(f"\nâœ… å°æ¯”å¯¦é©—å®Œæˆï¼")
        print(f"   â€¢ å°æ¯”åœ–è¡¨å·²ä¿å­˜: training_strategy_comparison.png")
        print(f"   â€¢ è©³ç´°åˆ†æå ±å‘Šå·²é¡¯ç¤º")

    except Exception as e:
        print(f"âŒ å¯¦é©—å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
=======
"""
è¨“ç·´ç­–ç•¥å°æ¯”å¯¦é©—
æ¯”è¼ƒã€Œå°‘é‡epoch+å¤§æ•¸æ“šé‡ã€vsã€Œå¤šé‡epoch+å°æ•¸æ“šé‡ã€çš„æ•ˆæœ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
from g_mlp_pytorch import gMLPVision
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report


def load_data_for_strategy(strategy="large_data"):
    """æ ¹æ“šç­–ç•¥åŠ è¼‰ä¸åŒæ•¸æ“šé…ç½®"""
    print(f"ğŸ“¦ åŠ è¼‰æ•¸æ“š - ç­–ç•¥: {strategy}")

    # ç°¡åŒ–çš„æ•¸æ“šå¢å¼·
    transform_train = transforms.Compose(
        [
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    transform_test = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    trainset = torchvision.datasets.CIFAR10(
        root="./data", train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root="./data", train=False, download=True, transform=transform_test
    )

    if strategy == "large_data":
        # ç­–ç•¥Aï¼šå¤§æ•¸æ“šé‡ + å°‘epoch
        trainset = Subset(trainset, range(4000))  # æ›´å¤šè¨“ç·´æ•¸æ“š
        testset = Subset(testset, range(800))
        batch_size = 64  # è¼ƒå¤§æ‰¹æ¬¡
        print("   âœ“ ç­–ç•¥Aï¼šå¤§æ•¸æ“šé‡ç­–ç•¥")
        print(f"   âœ“ è¨“ç·´æ¨£æœ¬: {len(trainset)}, æ‰¹æ¬¡å¤§å°: {batch_size}")
    else:
        # ç­–ç•¥Bï¼šå°æ•¸æ“šé‡ + å¤šepoch
        trainset = Subset(trainset, range(1200))  # è¼ƒå°‘è¨“ç·´æ•¸æ“š
        testset = Subset(testset, range(300))
        batch_size = 32  # è¼ƒå°æ‰¹æ¬¡
        print("   âœ“ ç­–ç•¥Bï¼šå°æ•¸æ“šé‡ç­–ç•¥")
        print(f"   âœ“ è¨“ç·´æ¨£æœ¬: {len(trainset)}, æ‰¹æ¬¡å¤§å°: {batch_size}")

    trainloader = DataLoader(
        trainset, batch_size=batch_size, shuffle=True, num_workers=0
    )
    testloader = DataLoader(
        testset, batch_size=batch_size, shuffle=False, num_workers=0
    )

    classes = [
        "Airplane",
        "Automobile",
        "Bird",
        "Cat",
        "Deer",
        "Dog",
        "Frog",
        "Horse",
        "Ship",
        "Truck",
    ]

    return trainloader, testloader, classes


def create_model():
    """å‰µå»ºçµ±ä¸€çš„æ¨¡å‹é…ç½®"""
    torch.set_num_threads(4)

    model = gMLPVision(
        image_size=32,
        patch_size=8,
        num_classes=10,
        dim=128,  # è¼ƒå°æ¨¡å‹åŠ é€Ÿå°æ¯”
        depth=3,
        ff_mult=3,
        channels=3,
        prob_survival=0.8,
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    return model, device


def train_with_strategy(model, trainloader, testloader, device, strategy="large_data"):
    """æ ¹æ“šç­–ç•¥é€²è¡Œè¨“ç·´"""

    if strategy == "large_data":
        epochs = 6  # å°‘é‡epoch
        lr = 0.003  # ç¨é«˜å­¸ç¿’ç‡
        print(f"\nğŸ‹ï¸ ç­–ç•¥Aè¨“ç·´ï¼š{epochs} epochsï¼Œå¤§æ•¸æ“šé‡")
    else:
        epochs = 18  # å¤šé‡epoch
        lr = 0.002  # ç¨ä½å­¸ç¿’ç‡
        print(f"\nğŸ‹ï¸ ç­–ç•¥Bè¨“ç·´ï¼š{epochs} epochsï¼Œå°æ•¸æ“šé‡")

    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.005)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=3)

    train_losses = []
    train_accs = []
    val_accs = []
    epoch_times = []

    start_time = time.time()
    best_val_acc = 0
    patience = 4
    patience_counter = 0

    for epoch in range(epochs):
        epoch_start = time.time()

        # è¨“ç·´
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for i, (inputs, labels) in enumerate(trainloader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        train_loss = running_loss / len(trainloader)
        train_acc = 100.0 * correct / total

        # é©—è­‰
        model.eval()
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for inputs, labels in testloader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

        val_acc = 100.0 * val_correct / val_total

        train_losses.append(train_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        epoch_time = time.time() - epoch_start
        epoch_times.append(epoch_time)

        scheduler.step()

        print(
            f"Epoch {epoch+1:2d}/{epochs}: "
            f"è¨“ç·´æº–ç¢ºç‡={train_acc:5.2f}%, "
            f"é©—è­‰æº–ç¢ºç‡={val_acc:5.2f}%, "
            f"æ™‚é–“={epoch_time:4.1f}s"
        )

        # æ—©åœ
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"   æ—©åœæ–¼ç¬¬ {epoch+1} epoch")
                break

    total_time = time.time() - start_time

    return {
        "train_losses": train_losses,
        "train_accs": train_accs,
        "val_accs": val_accs,
        "epoch_times": epoch_times,
        "total_time": total_time,
        "best_val_acc": best_val_acc,
        "final_train_acc": train_accs[-1],
        "final_val_acc": val_accs[-1],
        "epochs_used": len(train_accs),
    }


def compare_strategies():
    """åŸ·è¡Œç­–ç•¥å°æ¯”å¯¦é©—"""
    print("ğŸ”¬ é–‹å§‹è¨“ç·´ç­–ç•¥å°æ¯”å¯¦é©—")
    print("=" * 60)

    results = {}

    # ç­–ç•¥Aï¼šå¤§æ•¸æ“šé‡ + å°‘epoch
    print("\n" + "=" * 30 + " ç­–ç•¥A " + "=" * 30)
    trainloader_a, testloader_a, classes = load_data_for_strategy("large_data")
    model_a, device = create_model()
    results["strategy_a"] = train_with_strategy(
        model_a, trainloader_a, testloader_a, device, "large_data"
    )

    # ç­–ç•¥Bï¼šå°æ•¸æ“šé‡ + å¤šepoch
    print("\n" + "=" * 30 + " ç­–ç•¥B " + "=" * 30)
    trainloader_b, testloader_b, classes = load_data_for_strategy("small_data")
    model_b, device = create_model()
    results["strategy_b"] = train_with_strategy(
        model_b, trainloader_b, testloader_b, device, "small_data"
    )

    return results


def plot_comparison(results):
    """ç¹ªè£½å°æ¯”çµæœ"""
    print("\nğŸ“ˆ ç¹ªè£½å°æ¯”çµæœ...")

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle(
        "è¨“ç·´ç­–ç•¥å°æ¯”ï¼šå¤§æ•¸æ“šé‡+å°‘epoch vs å°æ•¸æ“šé‡+å¤šepoch",
        fontsize=16,
        fontweight="bold",
    )

    # ç²å–æ•¸æ“š
    strategy_a = results["strategy_a"]
    strategy_b = results["strategy_b"]

    # 1. è¨“ç·´æå¤±å°æ¯”
    ax = axes[0, 0]
    epochs_a = range(1, len(strategy_a["train_losses"]) + 1)
    epochs_b = range(1, len(strategy_b["train_losses"]) + 1)
    ax.plot(
        epochs_a,
        strategy_a["train_losses"],
        "b-",
        linewidth=2,
        label="ç­–ç•¥A: å¤§æ•¸æ“š+å°‘epoch",
    )
    ax.plot(
        epochs_b,
        strategy_b["train_losses"],
        "r-",
        linewidth=2,
        label="ç­–ç•¥B: å°æ•¸æ“š+å¤šepoch",
    )
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Training Loss")
    ax.set_title("è¨“ç·´æå¤±å°æ¯”")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 2. è¨“ç·´æº–ç¢ºç‡å°æ¯”
    ax = axes[0, 1]
    ax.plot(
        epochs_a, strategy_a["train_accs"], "b-", linewidth=2, label="ç­–ç•¥A: è¨“ç·´æº–ç¢ºç‡"
    )
    ax.plot(
        epochs_a, strategy_a["val_accs"], "b--", linewidth=2, label="ç­–ç•¥A: é©—è­‰æº–ç¢ºç‡"
    )
    ax.plot(
        epochs_b, strategy_b["train_accs"], "r-", linewidth=2, label="ç­–ç•¥B: è¨“ç·´æº–ç¢ºç‡"
    )
    ax.plot(
        epochs_b, strategy_b["val_accs"], "r--", linewidth=2, label="ç­–ç•¥B: é©—è­‰æº–ç¢ºç‡"
    )
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Accuracy (%)")
    ax.set_title("æº–ç¢ºç‡å°æ¯”")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 3. æ¯epochæ™‚é–“å°æ¯”
    ax = axes[0, 2]
    ax.plot(
        epochs_a,
        strategy_a["epoch_times"],
        "b-",
        linewidth=2,
        marker="o",
        label="ç­–ç•¥A",
    )
    ax.plot(
        epochs_b,
        strategy_b["epoch_times"],
        "r-",
        linewidth=2,
        marker="s",
        label="ç­–ç•¥B",
    )
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Time (seconds)")
    ax.set_title("æ¯Epochè¨“ç·´æ™‚é–“")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 4. éæ“¬åˆç¨‹åº¦å°æ¯”
    ax = axes[1, 0]
    diff_a = np.array(strategy_a["train_accs"]) - np.array(strategy_a["val_accs"])
    diff_b = np.array(strategy_b["train_accs"]) - np.array(strategy_b["val_accs"])
    ax.plot(epochs_a, diff_a, "b-", linewidth=2, label="ç­–ç•¥A: éæ“¬åˆç¨‹åº¦")
    ax.plot(epochs_b, diff_b, "r-", linewidth=2, label="ç­–ç•¥B: éæ“¬åˆç¨‹åº¦")
    ax.axhline(y=0, color="black", linestyle="--", alpha=0.5)
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Train-Val Accuracy Difference (%)")
    ax.set_title("éæ“¬åˆç¨‹åº¦å°æ¯”")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 5. æ•ˆç‡å°æ¯”æ¢å½¢åœ–
    ax = axes[1, 1]
    strategies = ["ç­–ç•¥A\n(å¤§æ•¸æ“š+å°‘epoch)", "ç­–ç•¥B\n(å°æ•¸æ“š+å¤šepoch)"]
    times = [strategy_a["total_time"], strategy_b["total_time"]]
    accuracies = [strategy_a["best_val_acc"], strategy_b["best_val_acc"]]

    x = np.arange(len(strategies))
    width = 0.35

    ax2 = ax.twinx()
    bars1 = ax.bar(x - width / 2, times, width, label="ç¸½è¨“ç·´æ™‚é–“(s)", color="skyblue")
    bars2 = ax2.bar(
        x + width / 2, accuracies, width, label="æœ€ä½³é©—è­‰æº–ç¢ºç‡(%)", color="lightcoral"
    )

    ax.set_xlabel("ç­–ç•¥")
    ax.set_ylabel("æ™‚é–“ (seconds)", color="blue")
    ax2.set_ylabel("æº–ç¢ºç‡ (%)", color="red")
    ax.set_title("æ•ˆç‡ vs æº–ç¢ºç‡")
    ax.set_xticks(x)
    ax.set_xticklabels(strategies)

    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar, time in zip(bars1, times):
        ax.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{time:.1f}s",
            ha="center",
            va="bottom",
        )
    for bar, acc in zip(bars2, accuracies):
        ax2.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{acc:.1f}%",
            ha="center",
            va="bottom",
        )

    # 6. è©³ç´°æ•¸æ“šå°æ¯”è¡¨
    ax = axes[1, 2]
    ax.axis("off")

    # å‰µå»ºå°æ¯”è¡¨æ ¼
    comparison_data = [
        ["æŒ‡æ¨™", "ç­–ç•¥A (å¤§æ•¸æ“š)", "ç­–ç•¥B (å°æ•¸æ“š)"],
        ["è¨“ç·´æ¨£æœ¬æ•¸", "4000", "1200"],
        ["è¨“ç·´epochs", f"{strategy_a['epochs_used']}", f"{strategy_b['epochs_used']}"],
        [
            "ç¸½è¨“ç·´æ™‚é–“",
            f"{strategy_a['total_time']:.1f}s",
            f"{strategy_b['total_time']:.1f}s",
        ],
        [
            "æœ€ä½³é©—è­‰æº–ç¢ºç‡",
            f"{strategy_a['best_val_acc']:.2f}%",
            f"{strategy_b['best_val_acc']:.2f}%",
        ],
        [
            "æœ€çµ‚è¨“ç·´æº–ç¢ºç‡",
            f"{strategy_a['final_train_acc']:.2f}%",
            f"{strategy_b['final_train_acc']:.2f}%",
        ],
        [
            "æœ€çµ‚é©—è­‰æº–ç¢ºç‡",
            f"{strategy_a['final_val_acc']:.2f}%",
            f"{strategy_b['final_val_acc']:.2f}%",
        ],
        [
            "éæ“¬åˆç¨‹åº¦",
            f"{strategy_a['final_train_acc']-strategy_a['final_val_acc']:.2f}%",
            f"{strategy_b['final_train_acc']-strategy_b['final_val_acc']:.2f}%",
        ],
        [
            "å¹³å‡æ¯epochæ™‚é–“",
            f"{np.mean(strategy_a['epoch_times']):.1f}s",
            f"{np.mean(strategy_b['epoch_times']):.1f}s",
        ],
    ]

    table = ax.table(
        cellText=comparison_data[1:],
        colLabels=comparison_data[0],
        cellLoc="center",
        loc="center",
        bbox=[0, 0, 1, 1],
    )
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 2)

    # è¨­ç½®è¡¨æ ¼æ¨£å¼
    for i in range(len(comparison_data)):
        for j in range(3):
            if i == 0:  # æ¨™é¡Œè¡Œ
                table[(i, j)].set_facecolor("#4CAF50")
                table[(i, j)].set_text_props(weight="bold", color="white")
            elif j == 0:  # æŒ‡æ¨™åˆ—
                table[(i, j)].set_facecolor("#E8F5E8")
            else:
                table[(i, j)].set_facecolor("#F9F9F9")

    ax.set_title("è©³ç´°å°æ¯”æ•¸æ“š", fontweight="bold")

    plt.tight_layout()
    plt.savefig("training_strategy_comparison.png", dpi=300, bbox_inches="tight")
    plt.show()


def print_analysis(results):
    """æ‰“å°åˆ†æçµæœ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š è¨“ç·´ç­–ç•¥åˆ†æå ±å‘Š")
    print("=" * 60)

    strategy_a = results["strategy_a"]
    strategy_b = results["strategy_b"]

    print(f"\nğŸ” ç­–ç•¥Aï¼ˆå¤§æ•¸æ“š+å°‘epochï¼‰:")
    print(f"   â€¢ è¨“ç·´æ¨£æœ¬: 4000")
    print(f"   â€¢ ä½¿ç”¨epochs: {strategy_a['epochs_used']}")
    print(f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {strategy_a['total_time']:.1f}s")
    print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {strategy_a['best_val_acc']:.2f}%")
    print(
        f"   â€¢ éæ“¬åˆç¨‹åº¦: {strategy_a['final_train_acc']-strategy_a['final_val_acc']:.2f}%"
    )

    print(f"\nğŸ” ç­–ç•¥Bï¼ˆå°æ•¸æ“š+å¤šepochï¼‰:")
    print(f"   â€¢ è¨“ç·´æ¨£æœ¬: 1200")
    print(f"   â€¢ ä½¿ç”¨epochs: {strategy_b['epochs_used']}")
    print(f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {strategy_b['total_time']:.1f}s")
    print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {strategy_b['best_val_acc']:.2f}%")
    print(
        f"   â€¢ éæ“¬åˆç¨‹åº¦: {strategy_b['final_train_acc']-strategy_b['final_val_acc']:.2f}%"
    )

    print(f"\nğŸ’¡ ç­–ç•¥å„ªåŠ£åˆ†æ:")

    # æº–ç¢ºç‡æ¯”è¼ƒ
    if strategy_a["best_val_acc"] > strategy_b["best_val_acc"]:
        acc_winner = "ç­–ç•¥Aï¼ˆå¤§æ•¸æ“šï¼‰"
        acc_diff = strategy_a["best_val_acc"] - strategy_b["best_val_acc"]
    else:
        acc_winner = "ç­–ç•¥Bï¼ˆå°æ•¸æ“šï¼‰"
        acc_diff = strategy_b["best_val_acc"] - strategy_a["best_val_acc"]

    # æ™‚é–“æ¯”è¼ƒ
    if strategy_a["total_time"] < strategy_b["total_time"]:
        time_winner = "ç­–ç•¥Aï¼ˆå¤§æ•¸æ“šï¼‰"
        time_diff = strategy_b["total_time"] - strategy_a["total_time"]
    else:
        time_winner = "ç­–ç•¥Bï¼ˆå°æ•¸æ“šï¼‰"
        time_diff = strategy_a["total_time"] - strategy_b["total_time"]

    # éæ“¬åˆæ¯”è¼ƒ
    overfitting_a = abs(strategy_a["final_train_acc"] - strategy_a["final_val_acc"])
    overfitting_b = abs(strategy_b["final_train_acc"] - strategy_b["final_val_acc"])

    if overfitting_a < overfitting_b:
        overfitting_winner = "ç­–ç•¥Aï¼ˆå¤§æ•¸æ“šï¼‰"
        overfitting_diff = overfitting_b - overfitting_a
    else:
        overfitting_winner = "ç­–ç•¥Bï¼ˆå°æ•¸æ“šï¼‰"
        overfitting_diff = overfitting_a - overfitting_b

    print(f"\nğŸ† ç¶œåˆæ¯”è¼ƒ:")
    print(f"   â€¢ æº–ç¢ºç‡å„ªå‹: {acc_winner} (é ˜å…ˆ {acc_diff:.2f}%)")
    print(f"   â€¢ è¨“ç·´é€Ÿåº¦å„ªå‹: {time_winner} (å¿« {time_diff:.1f}s)")
    print(f"   â€¢ æ³›åŒ–èƒ½åŠ›å„ªå‹: {overfitting_winner} (éæ“¬åˆå°‘ {overfitting_diff:.2f}%)")

    print(f"\nğŸ¯ å¯¦ç”¨å»ºè­°:")
    print(f"   â€¢ å¦‚æœè¿½æ±‚æœ€é«˜æº–ç¢ºç‡: é¸æ“‡{acc_winner}")
    print(f"   â€¢ å¦‚æœæ³¨é‡è¨“ç·´æ•ˆç‡: é¸æ“‡{time_winner}")
    print(f"   â€¢ å¦‚æœè¦æ±‚æ³›åŒ–èƒ½åŠ›: é¸æ“‡{overfitting_winner}")

    # æ ¹æ“šçµæœçµ¦å‡ºå…·é«”å»ºè­°
    if (
        strategy_a["best_val_acc"] > strategy_b["best_val_acc"]
        and strategy_a["total_time"] < strategy_b["total_time"]
    ):
        print(f"\nâœ… çµè«–: ç­–ç•¥Aï¼ˆå¤§æ•¸æ“š+å°‘epochï¼‰åœ¨æº–ç¢ºç‡å’Œæ•ˆç‡ä¸Šéƒ½æ›´å„ªç§€ï¼")
    elif (
        strategy_b["best_val_acc"] > strategy_a["best_val_acc"]
        and strategy_b["total_time"] < strategy_a["total_time"]
    ):
        print(f"\nâœ… çµè«–: ç­–ç•¥Bï¼ˆå°æ•¸æ“š+å¤šepochï¼‰åœ¨æº–ç¢ºç‡å’Œæ•ˆç‡ä¸Šéƒ½æ›´å„ªç§€ï¼")
    else:
        print(f"\nâš–ï¸ çµè«–: å…©ç¨®ç­–ç•¥å„æœ‰å„ªåŠ£ï¼Œéœ€è¦æ ¹æ“šå…·é«”éœ€æ±‚é¸æ“‡")
        if abs(strategy_a["best_val_acc"] - strategy_b["best_val_acc"]) < 2:
            print(f"   æº–ç¢ºç‡ç›¸è¿‘ï¼Œå»ºè­°é¸æ“‡è¨“ç·´æ™‚é–“æ›´çŸ­çš„ç­–ç•¥")
        else:
            print(f"   å¦‚æœæº–ç¢ºç‡å·®è·è¼ƒå¤§ï¼Œå»ºè­°å„ªå…ˆè€ƒæ…®æº–ç¢ºç‡")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ”¬ gMLPè¨“ç·´ç­–ç•¥å°æ¯”å¯¦é©—")
    print("æ¯”è¼ƒã€Œå¤§æ•¸æ“šé‡+å°‘epochã€vsã€Œå°æ•¸æ“šé‡+å¤šepochã€")
    print("=" * 60)

    try:
        # åŸ·è¡Œå°æ¯”å¯¦é©—
        results = compare_strategies()

        # ç¹ªè£½å°æ¯”åœ–è¡¨
        plot_comparison(results)

        # æ‰“å°åˆ†æå ±å‘Š
        print_analysis(results)

        print(f"\nâœ… å°æ¯”å¯¦é©—å®Œæˆï¼")
        print(f"   â€¢ å°æ¯”åœ–è¡¨å·²ä¿å­˜: training_strategy_comparison.png")
        print(f"   â€¢ è©³ç´°åˆ†æå ±å‘Šå·²é¡¯ç¤º")

    except Exception as e:
        print(f"âŒ å¯¦é©—å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
>>>>>>> 420764095488647da1ecd1309c810893dfec8ea4

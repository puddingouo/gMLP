"""
è‡ªå®šç¾© gMLP åœ–åƒåˆ†é¡æ¨¡å‹ - ç°¡åŒ–ç‰ˆæœ¬
åŸºæ–¼è«–æ–‡æ¶æ§‹ï¼Œæ”¯æ´å®Œå…¨è‡ªå®šç¾©æ¨¡å‹é…ç½®
é‡å°å¿«é€ŸåŸå‹é–‹ç™¼å’Œè³‡æºå—é™ç’°å¢ƒå„ªåŒ–
åƒ…ä½¿ç”¨ AdamW + CosineAnnealingLR ç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
from g_mlp_pytorch import gMLPVision
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import argparse
import os

# è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯¦é©—å¯é‡ç¾
torch.manual_seed(0)
np.random.seed(0)


def progress_bar(batch_idx, total_batches, msg):
    """é€²åº¦æ¢é¡¯ç¤º"""
    progress = (batch_idx + 1) / total_batches
    bar_length = 30
    filled_length = int(bar_length * progress)
    bar = "â–ˆ" * filled_length + "-" * (bar_length - filled_length)
    print(f"\r[{bar}] {progress:.0%} {msg}", end="", flush=True)
    if batch_idx == total_batches - 1:
        print()  # æ›è¡Œ


def get_lr(optimizer):
    """ç²å–ç•¶å‰å­¸ç¿’ç‡"""
    for param_group in optimizer.param_groups:
        return param_group["lr"]


def mixup_data(x, y, alpha=1.0, lam=1.0, count=0, device="cpu"):
    """Mixup æ•¸æ“šå¢å¼·"""
    if count == 0:
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1.0

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Mixup æå¤±å‡½æ•¸"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


def load_cifar10_data_enhanced(quick_test=True, use_mixup_transform=False):
    """åŠ è¼‰å¢å¼·ç‰ˆ CIFAR-10 æ•¸æ“šé›†"""
    print("ğŸ“¦ åŠ è¼‰å¢å¼·ç‰ˆ CIFAR-10 æ•¸æ“šé›†...")

    if use_mixup_transform:
        # ä½¿ç”¨å¢å¼·ç‰ˆè®Šæ›ç­–ç•¥
        transform_train = transforms.Compose(
            [
                transforms.RandomResizedCrop(
                    size=32, scale=(0.6, 1.0)
                ),  # èª¿æ•´ç‚º CIFAR-10 å°ºå¯¸
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
            ]
        )

        transform_test = transforms.Compose(
            [
                transforms.Resize(40),  # èª¿æ•´ç‚ºé©åˆ CIFAR-10
                transforms.CenterCrop(32),
                transforms.ToTensor(),
                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
            ]
        )
        print("   ğŸ¯ ä½¿ç”¨å¢å¼·ç‰ˆæ•¸æ“šè®Šæ› (ImageNet normalization)")
    else:
        # ä½¿ç”¨æ¨™æº–çš„ CIFAR-10 è®Šæ›ç­–ç•¥
        transform_train = transforms.Compose(
            [
                transforms.RandomCrop(32, padding=2),
                transforms.RandomHorizontalFlip(p=0.3),
                transforms.ToTensor(),
                transforms.Normalize(
                    (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)
                ),
            ]
        )

        transform_test = transforms.Compose(
            [
                transforms.ToTensor(),
                transforms.Normalize(
                    (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)
                ),
            ]
        )
        print("   ğŸ“Š ä½¿ç”¨æ¨™æº– CIFAR-10 æ•¸æ“šè®Šæ›")

    trainset = torchvision.datasets.CIFAR10(
        root="./data", train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root="./data", train=False, download=True, transform=transform_test
    )

    if quick_test:
        trainset = Subset(trainset, range(50000))
        testset = Subset(testset, range(10000))
        print("   ğŸš€ å¿«é€Ÿæ¨¡å¼ï¼šå®Œæ•´æ•¸æ“šé›†è¨“ç·´")

    batch_size = 64
    num_workers = 1
    pin_memory = False

    trainloader = DataLoader(
        trainset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    testloader = DataLoader(
        testset,
        batch_size=10,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )

    classes = [
        "Airplane",
        "Automobile",
        "Bird",
        "Cat",
        "Deer",
        "Dog",
        "Frog",
        "Horse",
        "Ship",
        "Truck",
    ]

    print(f"   âœ“ è¨“ç·´æ¨£æœ¬: {len(trainset)}")
    print(f"   âœ“ æ¸¬è©¦æ¨£æœ¬: {len(testset)}")
    print(f"   âœ“ è¨“ç·´æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   âœ“ æ¸¬è©¦æ‰¹æ¬¡å¤§å°: 10")

    return trainloader, testloader, classes


def get_training_parameters_enhanced():
    """ç²å–å¢å¼·ç‰ˆè¨“ç·´åƒæ•¸è¨­ç½® - åƒ… AdamW + CosineAnnealingLR"""
    print("\n" + "=" * 60)
    print("âš™ï¸  å¢å¼·ç‰ˆè¨“ç·´åƒæ•¸è¨­ç½®")
    print("=" * 60)

    # é è¨­åƒæ•¸
    default_lr = 0.01
    default_wd = 0.012
    default_epochs = 50
    default_alpha = 0.1
    default_batch_split = 1

    try:
        print(f"\nğŸ“š ä½¿ç”¨ AdamW + CosineAnnealingLR ç­–ç•¥ (å”¯ä¸€é¸é …)")

        # åŸºæœ¬åƒæ•¸è¨­ç½®
        lr_input = input(f"   ğŸ“š å­¸ç¿’ç‡ (é è¨­={default_lr}): ").strip()
        lr = float(lr_input) if lr_input else default_lr

        wd_input = input(f"   âš–ï¸  æ¬Šé‡è¡°æ¸› (é è¨­={default_wd}): ").strip()
        wd = float(wd_input) if wd_input else default_wd

        epochs_input = input(f"   ğŸ‹ï¸  è¨“ç·´è¼ªæ•¸ (é è¨­={default_epochs}): ").strip()
        epochs = int(epochs_input) if epochs_input.isdigit() else default_epochs

        # Mixup åƒæ•¸
        print("\nğŸ¨ Mixup æ•¸æ“šå¢å¼·:")
        print("   1. å•Ÿç”¨ Mixup (æ¨è–¦)")
        print("   2. é—œé–‰ Mixup")
        mixup_choice = input("   é¸æ“‡ (1/2, é è¨­=1): ").strip()
        use_mixup = True if mixup_choice != "2" else False

        alpha = default_alpha
        if use_mixup:
            alpha_input = input(
                f"   ğŸ­ Mixup alpha åƒæ•¸ (é è¨­={default_alpha}): "
            ).strip()
            alpha = float(alpha_input) if alpha_input else default_alpha

        # æ‰¹æ¬¡åˆ†å‰²
        batch_split_input = input(
            f"\nğŸ”¢ æ‰¹æ¬¡åˆ†å‰²å› å­ (é è¨­={default_batch_split}): "
        ).strip()
        batch_split = (
            int(batch_split_input)
            if batch_split_input.isdigit()
            else default_batch_split
        )

        # æ•¸æ“šè®Šæ›ç­–ç•¥
        print("\nğŸ”„ æ•¸æ“šè®Šæ›ç­–ç•¥:")
        print("   1. æ¨™æº– CIFAR-10 è®Šæ›")
        print("   2. å¢å¼·ç‰ˆè®Šæ› (ImageNet é¢¨æ ¼)")
        transform_choice = input("   é¸æ“‡ (1/2, é è¨­=1): ").strip()
        use_enhanced_transform = True if transform_choice == "2" else False

        # æ—©åœæ©Ÿåˆ¶è¨­ç½®
        print("\nâ¹ï¸ æ—©åœæ©Ÿåˆ¶è¨­ç½®:")
        print("   1. å•Ÿç”¨æ—©åœæ©Ÿåˆ¶ (æ¨è–¦)")
        print("   2. é—œé–‰æ—©åœæ©Ÿåˆ¶")
        early_stop_choice = input("   é¸æ“‡ (1/2, é è¨­=1): ").strip()
        use_early_stopping = True if early_stop_choice != "2" else False

        patience = 10
        min_delta = 0.001
        if use_early_stopping:
            patience_input = input(
                "   â° è€å¿ƒå€¼ - å¤šå°‘è¼ªç„¡æ”¹å–„å¾Œåœæ­¢ (é è¨­=10): "
            ).strip()
            patience = int(patience_input) if patience_input.isdigit() else 10

            min_delta_input = input("   ğŸ“ æœ€å°æ”¹å–„å¹…åº¦ (é è¨­=0.001): ").strip()
            min_delta = float(min_delta_input) if min_delta_input else 0.001

        print(f"\nâœ… è¨“ç·´åƒæ•¸ç¢ºèª:")
        print(f"   ğŸ“š å„ªåŒ–å™¨: AdamW")
        print(f"   ğŸ“š å­¸ç¿’ç‡: {lr}")
        print(f"   ğŸ“ˆ èª¿åº¦å™¨: CosineAnnealingLR")
        print(f"   âš–ï¸  æ¬Šé‡è¡°æ¸›: {wd}")
        print(f"   ğŸ‹ï¸  è¨“ç·´è¼ªæ•¸: {epochs}")
        print(f"   ğŸ¨ Mixup: {'å•Ÿç”¨' if use_mixup else 'é—œé–‰'}")
        if use_mixup:
            print(f"   ğŸ­ Alpha: {alpha}")
        print(f"   ğŸ”¢ æ‰¹æ¬¡åˆ†å‰²: {batch_split}")
        print(f"   ğŸ”„ è®Šæ›ç­–ç•¥: {'å¢å¼·ç‰ˆ' if use_enhanced_transform else 'æ¨™æº–'}")
        print(f"   â¹ï¸ æ—©åœæ©Ÿåˆ¶: {'å•Ÿç”¨' if use_early_stopping else 'é—œé–‰'}")
        if use_early_stopping:
            print(f"   â° è€å¿ƒå€¼: {patience}")
            print(f"   ğŸ“ æœ€å°æ”¹å–„: {min_delta}")

        return {
            "lr": lr,
            "weight_decay": wd,
            "epochs": epochs,
            "use_mixup": use_mixup,
            "alpha": alpha,
            "batch_split": batch_split,
            "use_enhanced_transform": use_enhanced_transform,
            "optimizer_type": "AdamW",
            "scheduler_type": "CosineAnnealingLR",
            "use_early_stopping": use_early_stopping,
            "patience": patience,
            "min_delta": min_delta,
        }

    except (ValueError, KeyboardInterrupt):
        print("   âš ï¸  ä½¿ç”¨é è¨­åƒæ•¸ (AdamW + CosineAnnealingLR ç­–ç•¥)")
        return {
            "lr": default_lr,
            "weight_decay": default_wd,
            "epochs": default_epochs,
            "use_mixup": True,
            "alpha": default_alpha,
            "batch_split": default_batch_split,
            "use_enhanced_transform": False,
            "optimizer_type": "AdamW",
            "scheduler_type": "CosineAnnealingLR",
            "use_early_stopping": True,
            "patience": 10,
            "min_delta": 0.001,
        }


def get_user_model_choice():
    """ç²å–ç”¨æˆ¶çš„è‡ªå®šç¾©æ¨¡å‹é…ç½®"""
    print("\n" + "=" * 80)
    print("ğŸ—ï¸  è‡ªå®šç¾© gMLP æ¨¡å‹é…ç½®")
    print("=" * 80)

    print("ğŸ“‹ åƒæ•¸å»ºè­°ç¯„åœ:")
    print("   â€¢ æ·±åº¦ (depth): 4-30 å±¤")
    print("   â€¢ ç¶­åº¦ (dim): 64-256")
    print("   â€¢ FFNå€æ•¸ (ff_mult): 2-8")
    print("   â€¢ å­˜æ´»æ©Ÿç‡ (prob_survival): 0.8-1.0")
    print("   â€¢ æ³¨æ„åŠ›ç¶­åº¦ (attn_dim): 64-128")

    while True:
        try:
            print("\nğŸ”§ è«‹è¼¸å…¥æ¨¡å‹åƒæ•¸:")

            depth = int(input("   ğŸ“ æ·±åº¦ (æ¨è–¦ 8-16): "))
            if depth < 1 or depth > 50:
                print("   âš ï¸ æ·±åº¦å»ºè­°åœ¨ 1-50 ä¹‹é–“")
                continue

            dim = int(input("   ğŸ“ ç¶­åº¦ (æ¨è–¦ 64-256): "))
            if dim < 32 or dim > 512:
                print("   âš ï¸ ç¶­åº¦å»ºè­°åœ¨ 32-512 ä¹‹é–“")
                continue

            ff_mult = int(input("   ğŸ”¢ FFNå€æ•¸ (æ¨è–¦ 2-6): "))
            if ff_mult < 1 or ff_mult > 12:
                print("   âš ï¸ FFNå€æ•¸å»ºè­°åœ¨ 1-12 ä¹‹é–“")
                continue

            prob_survival_input = input("   ğŸ¯ å­˜æ´»æ©Ÿç‡ (é è¨­ 1.0): ").strip()
            prob_survival = float(prob_survival_input) if prob_survival_input else 1.0
            if prob_survival < 0.1 or prob_survival > 1.0:
                print("   âš ï¸ å­˜æ´»æ©Ÿç‡å¿…é ˆåœ¨ 0.1-1.0 ä¹‹é–“")
                continue

            attn_dim_input = input("   ğŸ§  æ³¨æ„åŠ›ç¶­åº¦ (é è¨­ 64): ").strip()
            attn_dim = int(attn_dim_input) if attn_dim_input else 64
            if attn_dim < 32 or attn_dim > 256:
                print("   âš ï¸ æ³¨æ„åŠ›ç¶­åº¦å»ºè­°åœ¨ 32-256 ä¹‹é–“")
                continue

            # ä¼°ç®—åƒæ•¸æ•¸é‡
            estimated_params = estimate_gmlp_params(depth, dim, ff_mult, attn_dim)

            print(f"\nğŸ“Š æ¨¡å‹é…ç½®é è¦½:")
            print(f"   â€¢ æ·±åº¦: {depth} å±¤")
            print(f"   â€¢ ç¶­åº¦: {dim}")
            print(f"   â€¢ FFNå€æ•¸: {ff_mult}")
            print(f"   â€¢ å­˜æ´»æ©Ÿç‡: {prob_survival}")
            print(f"   â€¢ æ³¨æ„åŠ›ç¶­åº¦: {attn_dim}")
            print(f"   â€¢ é ä¼°åƒæ•¸: {estimated_params:.2f}M")

            # é ä¼°è¨“ç·´æ™‚é–“å’Œè¨˜æ†¶é«”
            if estimated_params < 0.5:
                time_est = "1-3åˆ†é˜"
                memory_est = "ä½"
            elif estimated_params < 1.0:
                time_est = "3-8åˆ†é˜"
                memory_est = "ä¸­ç­‰"
            elif estimated_params < 2.0:
                time_est = "8-15åˆ†é˜"
                memory_est = "è¼ƒé«˜"
            else:
                time_est = "15åˆ†é˜ä»¥ä¸Š"
                memory_est = "å¾ˆé«˜"

            print(f"   â€¢ é ä¼°è¨“ç·´æ™‚é–“: {time_est}")
            print(f"   â€¢ è¨˜æ†¶é«”éœ€æ±‚: {memory_est}")

            confirm = input(f"\n   ç¢ºèªä½¿ç”¨æ­¤é…ç½®å—? (y/n, é è¨­=y): ").strip().lower()
            if confirm in ["n", "no"]:
                print("   ğŸ”„ é‡æ–°é…ç½®...")
                continue

            return {
                "depth": depth,
                "dim": dim,
                "ff_mult": ff_mult,
                "prob_survival": prob_survival,
                "attn_dim": attn_dim,
                "estimated_params": estimated_params,
            }

        except ValueError:
            print("   âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
            continue
        except KeyboardInterrupt:
            print("\n   âŒ ç”¨æˆ¶ä¸­æ–·ï¼Œä½¿ç”¨é è¨­é…ç½®")
            return {
                "depth": 12,
                "dim": 128,
                "ff_mult": 3,
                "prob_survival": 1.0,
                "attn_dim": 64,
                "estimated_params": 0.65,
            }


def estimate_gmlp_params(depth, dim, ff_mult, attn_dim):
    """ä¼°ç®— gMLP æ¨¡å‹åƒæ•¸æ•¸é‡"""
    # Patch embedding: (patch_size^2 * channels * dim) + dim
    patch_embedding = (4 * 4 * 3 * dim) + dim

    # Each gMLP block
    block_params = (
        dim * 2  # Layer norm
        + dim * (dim * ff_mult * 2)
        + (dim * ff_mult * 2)  # Input projection
        + (32 // 4) ** 2
        + (32 // 4)  # Spatial gating (num_patches^2 + num_patches)
        + (dim * ff_mult) * dim
        + dim  # Output projection
    )

    total_blocks = depth * block_params

    # Final layer norm
    final_norm = dim * 2

    # Classifier
    classifier = dim * 10 + 10

    total_params = patch_embedding + total_blocks + final_norm + classifier
    return total_params / 1e6  # è½‰æ›ç‚ºç™¾è¬


def create_custom_gmlp_model(model_config):
    """å‰µå»ºè‡ªå®šç¾© gMLP æ¨¡å‹æ¶æ§‹"""
    print(f"\nğŸ—ï¸ å‰µå»ºè‡ªå®šç¾© gMLP æ¨¡å‹...")

    torch.set_num_threads(4)
    print("   âš¡ CPUæ¨¡å¼ï¼šå·²è¨­ç½®4å€‹ç·šç¨‹")

    model = gMLPVision(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=model_config["dim"],
        depth=model_config["depth"],
        ff_mult=model_config["ff_mult"],
        channels=3,
        prob_survival=model_config["prob_survival"],
        attn_dim=model_config["attn_dim"],
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    total_params = sum(p.numel() for p in model.parameters())
    params_M = total_params / 1e6

    print(f"\nâœ… è‡ªå®šç¾© gMLP æ¨¡å‹å‰µå»ºå®Œæˆ")
    print(f"   âœ“ è¨­å‚™: {device}")
    print(f"   âœ“ å¯¦éš›åƒæ•¸æ•¸é‡: {total_params:,} ({params_M:.2f}M)")
    print(f"   âœ“ é ä¼°åƒæ•¸æ•¸é‡: {model_config['estimated_params']:.2f}M")
    print(
        f"   âœ“ æ¶æ§‹é…ç½®: depth={model_config['depth']}, dim={model_config['dim']}, ff_mult={model_config['ff_mult']}, attn_dim={model_config['attn_dim']}"
    )

    return model, device


def train_enhanced(model, trainloader, testloader, device, training_params):
    """å¢å¼·ç‰ˆè¨“ç·´å‡½æ•¸ - åƒ… AdamW + CosineAnnealingLR ç­–ç•¥"""
    print(f"\nğŸ‹ï¸ é–‹å§‹å¢å¼·ç‰ˆè¨“ç·´ ({training_params['epochs']} å€‹ epochs)...")
    print(f"   ğŸ¨ Mixup: {'å•Ÿç”¨' if training_params['use_mixup'] else 'é—œé–‰'}")
    print(f"   ğŸ“š å„ªåŒ–å™¨: AdamW")
    print(f"   ğŸ“ˆ èª¿åº¦å™¨: CosineAnnealingLR")
    print(
        f"   â¹ï¸ æ—©åœæ©Ÿåˆ¶: {'å•Ÿç”¨' if training_params['use_early_stopping'] else 'é—œé–‰'}"
    )
    if training_params["use_early_stopping"]:
        print(f"   â° è€å¿ƒå€¼: {training_params['patience']} epochs")
        print(f"   ğŸ“ æœ€å°æ”¹å–„: {training_params['min_delta']}")

    # ğŸ¯ ä½¿ç”¨ AdamW + CosineAnnealingLR é…ç½®
    criterion = nn.CrossEntropyLoss(label_smoothing=0.08)
    optimizer = optim.AdamW(
        model.parameters(),
        lr=training_params["lr"],
        weight_decay=training_params["weight_decay"],
        betas=(0.9, 0.95),
    )
    lr_scheduler_obj = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=training_params["epochs"], eta_min=8e-6
    )

    train_losses = []
    train_accs = []
    val_accs = []
    val_losses = []
    epoch_times = []
    best_val_acc = 0

    # æ—©åœæ©Ÿåˆ¶è®Šæ•¸
    best_epoch = 0
    patience_counter = 0
    early_stopped = False

    total_start_time = time.time()

    for epoch in range(training_params["epochs"]):
        epoch_start_time = time.time()

        # è¨“ç·´éšæ®µ
        model.train()
        train_loss = 0
        correct = 0
        total = 0
        count = 0
        lam = 1.0

        print(
            f'\nEpoch: {epoch + 1}/{training_params["epochs"]}, LR: {get_lr(optimizer):.6f}'
        )
        optimizer.zero_grad()

        for batch_idx, (inputs, targets) in enumerate(trainloader):
            if count == training_params["batch_split"]:
                # ğŸ¯ æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.8)
                optimizer.step()
                optimizer.zero_grad()
                count = 0

            inputs, targets = inputs.to(device), targets.to(device)

            if training_params["use_mixup"]:
                inputs, targets_a, targets_b, lam = mixup_data(
                    inputs, targets, training_params["alpha"], lam, count, device
                )
                outputs = model(inputs)
                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)
            else:
                outputs = model(inputs)
                loss = criterion(outputs, targets)

            loss = loss / training_params["batch_split"]
            loss.backward()
            count += 1

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)

            if training_params["use_mixup"]:
                correct += (
                    lam * predicted.eq(targets_a.data).cpu().sum().float()
                    + (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float()
                )
            else:
                correct += predicted.eq(targets).sum().item()

            # é€²åº¦æ¢é¡¯ç¤º
            if batch_idx % 20 == 0 or batch_idx == len(trainloader) - 1:
                msg = f"Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})"
                progress_bar(batch_idx, len(trainloader), msg)

        epoch_loss = train_loss / len(trainloader)
        epoch_acc = 100.0 * correct / total
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)

        # é©—è­‰éšæ®µ
        val_acc, val_loss = test_model(model, testloader, device, criterion)
        val_accs.append(val_acc)
        val_losses.append(val_loss)

        # æ›´æ–°å­¸ç¿’ç‡
        lr_scheduler_obj.step()

        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        epoch_times.append(epoch_duration)

        print(
            f"Epoch {epoch + 1} å®Œæˆ: è¨“ç·´={epoch_acc:.2f}%, é©—è­‰={val_acc:.2f}%, æ™‚é–“={epoch_duration:.1f}s"
        )

        # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ—©åœæ©Ÿåˆ¶
        improved = False
        prev_best = best_val_acc

        if val_acc > best_val_acc + training_params["min_delta"]:
            best_val_acc = val_acc
            best_epoch = epoch
            patience_counter = 0
            improved = True

            model_name = "best_custom_gmlp_model.pth"
            torch.save(
                {
                    "model_state_dict": model.state_dict(),
                    "acc": val_acc,
                    "epoch": epoch,
                    "optimizer_type": "AdamW",
                    "scheduler_type": "CosineAnnealingLR",
                },
                model_name,
            )
            improvement = val_acc - prev_best
            print(
                f"   ğŸ’¾ æ–°æœ€ä½³æ¨¡å‹å·²ä¿å­˜: é©—è­‰æº–ç¢ºç‡ {best_val_acc:.2f}% (+{improvement:.3f}%)"
            )
        else:
            patience_counter += 1

        # æ—©åœæª¢æŸ¥
        if training_params["use_early_stopping"]:
            if patience_counter >= training_params["patience"]:
                early_stopped = True
                print(
                    f"\nâ¹ï¸ æ—©åœè§¸ç™¼ï¼é€£çºŒ {training_params['patience']} å€‹ epochs ç„¡æ”¹å–„"
                )
                print(
                    f"   ğŸ“ˆ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}% (ç¬¬ {best_epoch + 1} epoch)"
                )
                print(
                    f"   â° ç•¶å‰è€å¿ƒè¨ˆæ•¸: {patience_counter}/{training_params['patience']}"
                )
                break
            elif patience_counter > 0:
                print(
                    f"   â° æ—©åœè¨ˆæ•¸: {patience_counter}/{training_params['patience']} (æº–ç¢ºç‡æœªæ”¹å–„ â‰¥ {training_params['min_delta']})"
                )

        # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
        if not improved:
            print(
                f"   ğŸ“Š ç•¶å‰æº–ç¢ºç‡: {val_acc:.2f}% (æœ€ä½³: {best_val_acc:.2f}%, å·®è·: {best_val_acc - val_acc:.3f}%)"
            )

    total_end_time = time.time()
    total_training_time = total_end_time - total_start_time

    print(f"\nâ±ï¸ å¢å¼·ç‰ˆè¨“ç·´æ™‚é–“çµ±è¨ˆ:")
    print(
        f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.1f}s ({total_training_time/60:.1f}min)"
    )
    print(f"   â€¢ å¹³å‡æ¯epoch: {np.mean(epoch_times):.1f}s")
    print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}% (ç¬¬ {best_epoch + 1} epoch)")
    print(f"   â€¢ ä½¿ç”¨ç­–ç•¥: AdamW + CosineAnnealingLR")

    # æ—©åœæ©Ÿåˆ¶çµ±è¨ˆ
    if training_params["use_early_stopping"]:
        if early_stopped:
            print(f"   â€¢ æ—©åœç‹€æ…‹: â¹ï¸ æ—©åœè§¸ç™¼ (ç¬¬ {epoch + 1} epoch)")
            print(f"   â€¢ å¯¦éš›è¨“ç·´è¼ªæ•¸: {epoch + 1}/{training_params['epochs']}")
            print(
                f"   â€¢ ç¯€çœæ™‚é–“: {(training_params['epochs'] - epoch - 1) * np.mean(epoch_times):.1f}s"
            )
        else:
            print(f"   â€¢ æ—©åœç‹€æ…‹: âœ… å®Œæ•´è¨“ç·´å®Œæˆ")
            print(f"   â€¢ å¯¦éš›è¨“ç·´è¼ªæ•¸: {len(train_losses)}/{training_params['epochs']}")
    else:
        print(f"   â€¢ å¯¦éš›è¨“ç·´è¼ªæ•¸: {len(train_losses)}/{training_params['epochs']}")

    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    if best_val_acc > 0:
        checkpoint = torch.load("best_custom_gmlp_model.pth")
        model.load_state_dict(checkpoint["model_state_dict"])
        print(f"   â€¢ å·²è¼‰å…¥æœ€ä½³æ¨¡å‹æ¬Šé‡ (AdamW è¨“ç·´)")

    return (
        train_losses,
        train_accs,
        val_accs,
        val_losses,
        epoch_times,
        total_training_time,
        early_stopped,
        best_epoch,
    )


def test_model(model, testloader, device, criterion):
    """æ¸¬è©¦æ¨¡å‹æ€§èƒ½"""
    model.eval()
    test_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(testloader):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    accuracy = 100.0 * correct / total
    avg_loss = test_loss / len(testloader)

    return accuracy, avg_loss


def evaluate_custom_model(model, testloader, device, classes):
    """è©•ä¼°è‡ªå®šç¾©æ¨¡å‹"""
    print("\nğŸ“Š è©•ä¼°è‡ªå®šç¾©æ¨¡å‹...")

    model.eval()
    all_predictions = []
    all_labels = []
    correct = 0
    total = 0
    class_correct = [0] * 10
    class_total = [0] * 10

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)

            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            for i in range(labels.size(0)):
                label = labels[i]
                class_correct[label] += (predicted[i] == label).item()
                class_total[label] += 1

    overall_acc = 100.0 * correct / total
    print(f"   âœ“ æ•´é«”æº–ç¢ºç‡: {overall_acc:.2f}%")

    # çµæœå¯è¦–åŒ–
    plt.figure(figsize=(12, 8))

    # å„é¡åˆ¥æº–ç¢ºç‡
    plt.subplot(2, 2, 1)
    class_accs = [
        100.0 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0
        for i in range(10)
    ]
    bars = plt.bar(classes, class_accs, color=plt.cm.tab10(np.arange(10)))
    plt.title("Custom gMLP: Class Accuracy", fontweight="bold")
    plt.ylabel("Accuracy (%)")
    plt.xticks(rotation=45)
    plt.grid(axis="y", alpha=0.3)

    for bar, acc in zip(bars, class_accs):
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{acc:.1f}%",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # æ··æ·†çŸ©é™£
    plt.subplot(2, 2, 2)
    cm = confusion_matrix(all_labels, all_predictions)
    sns.heatmap(
        cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes
    )
    plt.title("Confusion Matrix", fontweight="bold")
    plt.xlabel("Predicted")
    plt.ylabel("True")

    # æº–ç¢ºç‡åˆ†ä½ˆ
    plt.subplot(2, 2, 3)
    plt.hist(class_accs, bins=8, alpha=0.7, color="skyblue", edgecolor="black")
    plt.title("Class Accuracy Distribution", fontweight="bold")
    plt.xlabel("Accuracy (%)")
    plt.ylabel("Count")
    plt.grid(axis="y", alpha=0.3)

    # æ¨¡å‹çµ±è¨ˆ
    plt.subplot(2, 2, 4)
    stats_text = f"""Custom Model Statistics:
â€¢ Overall Accuracy: {overall_acc:.2f}%
â€¢ Best Class: {classes[np.argmax(class_accs)]} ({max(class_accs):.1f}%)
â€¢ Worst Class: {classes[np.argmin(class_accs)]} ({min(class_accs):.1f}%)
â€¢ Average Class Accuracy: {np.mean(class_accs):.1f}%
â€¢ Standard Deviation: {np.std(class_accs):.1f}%"""

    plt.text(
        0.1,
        0.5,
        stats_text,
        fontsize=11,
        verticalalignment="center",
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"),
    )
    plt.axis("off")
    plt.title("Model Statistics", fontweight="bold")

    plt.tight_layout()
    plt.savefig("custom_gmlp_results.png", dpi=300, bbox_inches="tight")
    plt.show()

    return overall_acc


def plot_enhanced_training_history(
    train_losses, train_accs, val_accs, val_losses, epoch_times
):
    """ç¹ªè£½å¢å¼·ç‰ˆè¨“ç·´æ­·å²"""
    print("\nğŸ“ˆ ç¹ªè£½å¢å¼·ç‰ˆè¨“ç·´æ­·å²...")

    plt.figure(figsize=(16, 4))

    # æå¤±æ›²ç·šæ¯”è¼ƒ
    plt.subplot(1, 4, 1)
    plt.plot(train_losses, "b-", linewidth=2, label="Training Loss")
    plt.plot(val_losses, "r-", linewidth=2, label="Validation Loss")
    plt.title("Custom gMLP Loss Curves", fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # æº–ç¢ºç‡æ›²ç·š
    plt.subplot(1, 4, 2)
    plt.plot(train_accs, "g-", linewidth=2, label="Training Acc")
    plt.plot(val_accs, "r-", linewidth=2, label="Validation Acc")
    plt.title("Custom gMLP Accuracy Curves", fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # éæ“¬åˆç›£æ§
    plt.subplot(1, 4, 3)
    acc_diff = np.array(train_accs) - np.array(val_accs)
    plt.plot(acc_diff, "purple", linewidth=2, label="Accuracy Diff")
    plt.title("Overfitting Monitor", fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Training - Validation (%)")
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color="black", linestyle="--", alpha=0.5)
    plt.legend()

    # è¨“ç·´æ™‚é–“
    plt.subplot(1, 4, 4)
    plt.plot(epoch_times, "orange", linewidth=2, marker="o", markersize=3)
    plt.title("Training Time per Epoch", fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Time (seconds)")
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("custom_gmlp_training_history.png", dpi=300, bbox_inches="tight")
    plt.show()


def genetic_algorithm_optimization():
    """éºå‚³ç®—æ³•å„ªåŒ–å…¥å£"""
    from genetic_optimizer import run_genetic_optimization

    run_genetic_optimization()


def main():
    """ä¸»å‡½æ•¸ - æ·»åŠ éºå‚³ç®—æ³•é¸é …"""
    print("ğŸš€ è‡ªå®šç¾© gMLP åœ–åƒåˆ†é¡è¨“ç·´")
    print("ğŸ¯ ä½¿ç”¨ AdamW + CosineAnnealingLR ç­–ç•¥")
    print("=" * 70)

    print("\né¸æ“‡è¨“ç·´æ¨¡å¼:")
    print("1. æ‰‹å‹•é…ç½®è¨“ç·´")
    print("2. éºå‚³ç®—æ³•è‡ªå‹•å„ªåŒ– ğŸ§¬")

    choice = input("è«‹é¸æ“‡ (1/2, é è¨­=1): ").strip()

    if choice == "2":
        genetic_algorithm_optimization()
        return

    try:
        # åŸæœ‰çš„æ‰‹å‹•é…ç½®æµç¨‹
        model_config = get_user_model_choice()
        training_params = get_training_parameters_enhanced()

        trainloader, testloader, classes = load_cifar10_data_enhanced(
            quick_test=True,
            use_mixup_transform=training_params["use_enhanced_transform"],
        )

        model, device = create_custom_gmlp_model(model_config)

        print(f"\nğŸ¬ é–‹å§‹è¨“ç·´è‡ªå®šç¾©æ¨¡å‹...")
        train_result = train_enhanced(
            model, trainloader, testloader, device, training_params
        )
        (
            train_losses,
            train_accs,
            val_accs,
            val_losses,
            epoch_times,
            total_time,
            early_stopped,
            best_epoch,
        ) = train_result

        plot_enhanced_training_history(
            train_losses, train_accs, val_accs, val_losses, epoch_times
        )

        final_acc = evaluate_custom_model(model, testloader, device, classes)

        print(f"\nğŸ‰ è¨“ç·´å®Œæˆç¸½çµ:")
        print(
            f"   â€¢ æ¨¡å‹é…ç½®: {model_config['depth']}å±¤, {model_config['dim']}ç¶­åº¦, FFNÃ—{model_config['ff_mult']}"
        )
        print(f"   â€¢ æœ€çµ‚æº–ç¢ºç‡: {final_acc:.2f}%")
        print(f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
        print(f"   â€¢ å¹³å‡æ¯epoch: {np.mean(epoch_times):.1f} ç§’")
        print(f"   â€¢ å¯¦éš›è¨“ç·´è¼ªæ•¸: {len(train_losses)}/{training_params['epochs']}")
        print(f"   â€¢ å„ªåŒ–ç­–ç•¥: AdamW + CosineAnnealingLR")
        print(f"   â€¢ Mixupç‹€æ…‹: {'å•Ÿç”¨' if training_params['use_mixup'] else 'é—œé–‰'}")

        if training_params["use_early_stopping"]:
            if early_stopped:
                print(
                    f"   â€¢ æ—©åœç‹€æ…‹: â¹ï¸ æå‰åœæ­¢ (ç¯€çœ {training_params['epochs'] - len(train_losses)} epochs)"
                )
                print(f"   â€¢ æœ€ä½³epoch: ç¬¬ {best_epoch + 1} epoch")
            else:
                print(f"   â€¢ æ—©åœç‹€æ…‹: âœ… è¨“ç·´å®Œæˆ (æœªè§¸ç™¼æ—©åœ)")
        else:
            print(f"   â€¢ æ—©åœç‹€æ…‹: âŒ æœªå•Ÿç”¨")

        print(f"\n" + "=" * 70)
        continue_choice = input("ğŸ”„ æ˜¯å¦è¦è¨“ç·´å…¶ä»–æ¨¡å‹? (y/n): ").strip().lower()
        if continue_choice in ["y", "yes"]:
            print("\n" + "ğŸ”„ é‡æ–°é–‹å§‹..." + "\n")
            main()

    except KeyboardInterrupt:
        print("\n\nâŒ ç”¨æˆ¶ä¸­æ–·ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("   è«‹æª¢æŸ¥è¼¸å…¥ä¸¦é‡è©¦")


if __name__ == "__main__":
    main()

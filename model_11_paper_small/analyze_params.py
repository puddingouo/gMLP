"""
gMLP åƒæ•¸åˆ†æå·¥å…·
æ·±å…¥åˆ†æå¯¦éš›åƒæ•¸èˆ‡ç›®æ¨™åƒæ•¸çš„å·®ç•°
"""

from g_mlp_pytorch import gMLPVision
import torch
import math


def analyze_gmlp_parameters(model_size="XS"):
    """è©³ç´°åˆ†ægMLPæ¨¡å‹åƒæ•¸"""

    # æ¨¡å‹é…ç½®
    configs = {
        "Test": {"depth": 4, "dim": 64, "ff_mult": 2, "target": 0.1},
        "Nano": {"depth": 6, "dim": 64, "ff_mult": 2, "target": 0.3},
        "XS": {"depth": 8, "dim": 80, "ff_mult": 3, "target": 0.8},
        "S": {"depth": 12, "dim": 128, "ff_mult": 3, "target": 2.0},
        "M": {"depth": 16, "dim": 160, "ff_mult": 4, "target": 4.5},
        "L": {"depth": 30, "dim": 128, "ff_mult": 6, "target": 5.9},
    }

    config = configs[model_size]

    print(f"ğŸ” {model_size} æ¨¡å‹åƒæ•¸æ·±åº¦åˆ†æ")
    print("=" * 70)
    print(
        f"é…ç½®: depth={config['depth']}, dim={config['dim']}, ff_mult={config['ff_mult']}"
    )
    print(f"ç›®æ¨™åƒæ•¸: {config['target']}M")
    print("-" * 70)

    # å‰µå»ºæ¨¡å‹
    model = gMLPVision(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=config["dim"],
        depth=config["depth"],
        ff_mult=config["ff_mult"],
        channels=3,
        prob_survival=1.00,
    )

    # åˆ†ææ¯ä¸€å±¤åƒæ•¸
    print("\nğŸ“Š é€å±¤åƒæ•¸åˆ†æ:")
    total_params = 0
    layer_groups = {}

    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count

        # åˆ†çµ„çµ±è¨ˆ
        if "to_patch_embedding" in name:
            group = "Patch Embedding"
        elif "pos_emb" in name:
            group = "Position Embedding"
        elif "layers" in name:
            if "norm" in name:
                group = "Layer Normalization"
            elif "proj_in" in name or "proj_out" in name:
                group = "Linear Projections"
            elif "sgu" in name:
                group = "Spatial Gating Unit"
            else:
                group = "Other Layers"
        elif "to_logits" in name:
            group = "Classification Head"
        else:
            group = "Other"

        if group not in layer_groups:
            layer_groups[group] = 0
        layer_groups[group] += param_count

        if param_count > 500:  # åªé¡¯ç¤ºè¼ƒå¤§çš„åƒæ•¸
            print(f"   â€¢ {name:<40}: {param_count:>8,} ({param_count/1e6:.3f}M)")

    print("\nğŸ“ˆ åƒæ•¸åˆ†çµ„çµ±è¨ˆ:")
    for group, count in sorted(layer_groups.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_params) * 100
        print(f"   â€¢ {group:<25}: {count:>8,} ({count/1e6:.3f}M, {percentage:.1f}%)")

    print("\nğŸ§® åƒæ•¸é‡ç†è«–è¨ˆç®—:")

    # è¨ˆç®—å„éƒ¨åˆ†ç†è«–åƒæ•¸é‡
    patch_size = 4
    image_size = 32
    channels = 3
    num_classes = 10
    dim = config["dim"]
    depth = config["depth"]
    ff_mult = config["ff_mult"]

    # Patch embedding: (patch_size^2 * channels) * dim
    patch_emb_params = (patch_size * patch_size * channels) * dim
    print(
        f"   â€¢ Patch Embedding: {patch_size}Â²Ã—{channels}Ã—{dim} = {patch_emb_params:,}"
    )

    # Position embedding: (image_size/patch_size)^2 * dim
    num_patches = (image_size // patch_size) ** 2
    pos_emb_params = num_patches * dim
    print(f"   â€¢ Position Embedding: {num_patches}Ã—{dim} = {pos_emb_params:,}")

    # æ¯å±¤gMLPåƒæ•¸
    # Layer Norm: dim (weight) + dim (bias) = 2*dim
    norm_params_per_layer = 2 * dim

    # Linear projections: dim*dim*ff_mult + dim*ff_mult (proj_in) + dim*ff_mult*dim + dim (proj_out)
    proj_params_per_layer = (
        dim * (dim * ff_mult) + (dim * ff_mult) + (dim * ff_mult) * dim + dim
    )

    # SGU: è¤‡é›œåº¦è¼ƒä½ï¼Œä¸»è¦æ˜¯æ¬Šé‡çŸ©é™£
    sgu_params_per_layer = num_patches * num_patches + num_patches  # ç°¡åŒ–ä¼°ç®—

    layer_params = (
        norm_params_per_layer + proj_params_per_layer + sgu_params_per_layer
    ) * depth
    print(f"   â€¢ gMLPå±¤ (Ã—{depth}): ~{layer_params:,}")

    # Classification head: dim * num_classes
    head_params = dim * num_classes
    print(f"   â€¢ Classification Head: {dim}Ã—{num_classes} = {head_params:,}")

    # ç†è«–ç¸½è¨ˆ
    theoretical_total = patch_emb_params + pos_emb_params + layer_params + head_params
    print(f"   â€¢ ç†è«–ç¸½è¨ˆ: {theoretical_total:,} ({theoretical_total/1e6:.3f}M)")

    print("\nğŸ“Š æœ€çµ‚å°æ¯”:")
    actual_M = total_params / 1e6
    target_M = config["target"]
    theoretical_M = theoretical_total / 1e6

    print(f"   â€¢ å¯¦éš›åƒæ•¸: {total_params:,} ({actual_M:.3f}M)")
    print(f"   â€¢ ç›®æ¨™åƒæ•¸: {target_M:.1f}M")
    print(f"   â€¢ ç†è«–ä¼°ç®—: {theoretical_total:,} ({theoretical_M:.3f}M)")
    print(f"   â€¢ å¯¦éš› vs ç›®æ¨™: {((actual_M/target_M-1)*100):+.1f}%")
    print(f"   â€¢ å¯¦éš› vs ç†è«–: {((actual_M/theoretical_M-1)*100):+.1f}%")

    print("\nğŸ’¡ å·®ç•°åŸå› åˆ†æ:")
    if actual_M < target_M:
        print("   âœ… å¯¦éš›åƒæ•¸å°‘æ–¼ç›®æ¨™ - é€™æ˜¯å¥½äº‹ï¼")
        print("   ğŸ“‰ å¯èƒ½åŸå› :")
        print("      â€¢ gMLPVisionæ¶æ§‹æ¯”é æœŸæ›´ç²¾ç°¡")
        print("      â€¢ æŸäº›å±¤ä½¿ç”¨äº†åƒæ•¸å…±äº«æˆ–æ›´é«˜æ•ˆçš„å¯¦ç¾")
        print("      â€¢ ç›®æ¨™ä¼°ç®—å…¬å¼éæ–¼ä¿å®ˆ")
        print("      â€¢ patch_size=4 ç”¢ç”Ÿçš„patchesè¼ƒå°‘")
    else:
        print("   âš ï¸ å¯¦éš›åƒæ•¸å¤šæ–¼ç›®æ¨™")
        print("   ğŸ“ˆ å¯èƒ½éœ€è¦å„ªåŒ–æ¨¡å‹æ¶æ§‹")


if __name__ == "__main__":
    # åˆ†æä¸åŒè¦æ¨¡çš„æ¨¡å‹
    for size in ["Test", "Nano", "XS", "S"]:
        analyze_gmlp_parameters(size)
        print("\n" + "=" * 70 + "\n")

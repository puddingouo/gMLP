"""
è¶…ç¸®å°ç‰ˆ gMLP åœ–åƒåˆ†é¡æ¨¡å‹
åŸºæ–¼è«–æ–‡æ¶æ§‹ä½†å¤§å¹…ç¸®å°è¦æ¨¡ä»¥æé«˜è¨“ç·´æ•ˆç‡
é‡å°å¿«é€ŸåŸå‹é–‹ç™¼å’Œè³‡æºå—é™ç’°å¢ƒå„ªåŒ–
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
from g_mlp_pytorch import gMLPVision
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns


def load_cifar10_data_ultrafast(quick_test=True):
    """åŠ è¼‰è¶…å¿«é€Ÿ CIFAR-10 æ•¸æ“šé›† - é‡å°å¿«é€Ÿè¨“ç·´å„ªåŒ–"""
    print("ğŸ“¦ åŠ è¼‰è¶…å¿«é€Ÿ CIFAR-10 æ•¸æ“šé›†...")

    # ç°¡åŒ–çš„æ•¸æ“šå¢å¼·ç­–ç•¥ - æ¸›å°‘è¨ˆç®—é–‹éŠ·
    transform_train = transforms.Compose(
        [
            transforms.RandomCrop(32, padding=2),  # æ¸›å°‘padding
            transforms.RandomHorizontalFlip(p=0.3),  # é™ä½æ¦‚ç‡
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    transform_test = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    trainset = torchvision.datasets.CIFAR10(
        root="./data", train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root="./data", train=False, download=True, transform=transform_test
    )

    if quick_test:
        # è¶…å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨æ›´å°‘æ•¸æ“šé€²è¡Œå¿«é€ŸåŸå‹
        trainset = Subset(trainset, range(50000))  # æ¸›å°‘åˆ°30Kæ¨£æœ¬
        testset = Subset(testset, range(10000))  # æ¸›å°‘åˆ°5Kæ¨£æœ¬
        print("   ğŸš€ è¶…å¿«é€Ÿæ¨¡å¼ï¼šå°è¦æ¨¡æ•¸æ“šé›†è¨“ç·´")

    # å„ªåŒ–DataLoaderé…ç½®
    batch_size = 64  # æ¸›å°‘batch sizeä»¥é©é…å°æ¨¡å‹
    num_workers = 0
    pin_memory = False

    trainloader = DataLoader(
        trainset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    testloader = DataLoader(
        testset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )

    classes = [
        "Airplane",
        "Automobile",
        "Bird",
        "Cat",
        "Deer",
        "Dog",
        "Frog",
        "Horse",
        "Ship",
        "Truck",
    ]

    print(f"   âœ“ è¨“ç·´æ¨£æœ¬: {len(trainset)}")
    print(f"   âœ“ æ¸¬è©¦æ¨£æœ¬: {len(testset)}")
    print(f"   âœ“ Batchå¤§å°: {batch_size}")

    return trainloader, testloader, classes


def create_ultra_small_gmlp_model(model_size="L"):
    """å‰µå»ºè¶…ç¸®å°ç‰ˆ gMLP æ¨¡å‹æ¶æ§‹"""
    print(f"\nğŸ—ï¸ å‰µå»ºè¶…ç¸®å°ç‰ˆ gMLP-{model_size} æ¨¡å‹...")

    # CPUå°ˆç”¨å„ªåŒ–è¨­ç½®
    torch.set_num_threads(4)
    print("   âš¡ CPUæ¨¡å¼ï¼šå·²è¨­ç½®4å€‹ç·šç¨‹")

    # è¶…ç¸®å°ç‰ˆæ¶æ§‹é…ç½® - å¤§å¹…é™ä½è¤‡é›œåº¦
    if model_size == "Test":  # æ¸¬è©¦æ¨¡å‹ - æ–°å¢
        config = {
            "depth": 4,  # æ¥µå°‘å±¤æ•¸
            "dim": 64,  # æ¥µå°ç¶­åº¦
            "ff_mult": 2,  # æœ€å°FFNå€æ•¸
            "prob_survival": 1.00,
            "params_target": 0.1,
        }
    elif model_size == "Nano":  # æ¥µå°æ¨¡å‹ - æ–°å¢
        config = {
            "depth": 6,  # æ¥µå°‘å±¤æ•¸
            "dim": 64,  # æ¥µå°ç¶­åº¦
            "ff_mult": 2,  # æœ€å°FFNå€æ•¸
            "prob_survival": 1.00,
            "params_target": 0.3,
        }
    elif model_size == "XS":  # è¶…å°æ¨¡å‹
        config = {
            "depth": 8,  # æ¸›å°‘å±¤æ•¸
            "dim": 80,  # å°ç¶­åº¦
            "ff_mult": 3,  # å°FFNå€æ•¸
            "prob_survival": 1.00,
            "params_target": 0.8,
        }
    elif model_size == "S":  # å°æ¨¡å‹
        config = {
            "depth": 12,  # ä¸­ç­‰å±¤æ•¸
            "dim": 128,  # ä¸­ç­‰ç¶­åº¦
            "ff_mult": 3,  # ä¸­ç­‰FFNå€æ•¸
            "prob_survival": 0.98,
            "params_target": 2.0,
        }
    elif model_size == "M":  # ä¸­ç­‰æ¨¡å‹
        config = {
            "depth": 16,  # é©ä¸­å±¤æ•¸
            "dim": 160,  # é©ä¸­ç¶­åº¦
            "ff_mult": 4,  # é©ä¸­FFNå€æ•¸
            "prob_survival": 0.95,
            "params_target": 4.5,
        }
    elif model_size == "L":  # å¤§æ¨¡å‹ - æ–°å¢
        config = {
            "depth": 30,  # #L = 30
            "dim": 128,  # d_model = 128
            "ff_mult": 6,  # d_ffn / d_model = 768/128 = 6
            "prob_survival": 1.00,  # è«–æ–‡ä¸­Tiæ¨¡å‹ä¸ä½¿ç”¨éš¨æ©Ÿæ·±åº¦
            "params_target": 5.9,  # ç›®æ¨™åƒæ•¸é‡(M)
        }
    else:
        raise ValueError(
            f"ä¸æ”¯æ´çš„æ¨¡å‹å¤§å°: {model_size}ã€‚æ”¯æ´: Test, Nano, XS, S, M, L"
        )

    model = gMLPVision(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=config["dim"],
        depth=config["depth"],
        ff_mult=config["ff_mult"],
        channels=3,
        prob_survival=config["prob_survival"],
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    total_params = sum(p.numel() for p in model.parameters())
    params_M = total_params / 1e6

    # è©³ç´°åƒæ•¸åˆ†æ
    print(f"\nğŸ“Š è©³ç´°åƒæ•¸åˆ†æ:")
    total_analyzed = 0
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_analyzed += param_count
        if param_count > 1000:  # åªé¡¯ç¤ºä¸»è¦åƒæ•¸
            print(f"   â€¢ {name:<35}: {param_count:>8,} ({param_count/1e6:.3f}M)")

    print(f"   {'='*50}")
    print(f"   â€¢ {'ç¸½è¨ˆ':<35}: {total_analyzed:>8,} ({total_analyzed/1e6:.3f}M)")

    print(f"\nâœ… è¶…ç¸®å°ç‰ˆ gMLP-{model_size} æ¨¡å‹å‰µå»ºå®Œæˆ")
    print(f"   âœ“ è¨­å‚™: {device}")
    print(f"   âœ“ å¯¦éš›åƒæ•¸æ•¸é‡: {total_params:,} ({params_M:.2f}M)")
    print(f"   âœ“ ç›®æ¨™åƒæ•¸é æœŸ: {config['params_target']}M")
    print(
        f"   âœ“ åƒæ•¸å·®ç•°åˆ†æ: å¯¦éš›æ¯”é æœŸ {'å¤š' if params_M > config['params_target'] else 'å°‘'} {abs(params_M - config['params_target']):.2f}M"
    )
    print(f"   âœ“ æ¨¡å‹æª”æ¡ˆå¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB (FP32)")
    print(
        f"   âœ“ æ¶æ§‹é…ç½®: depth={config['depth']}, dim={config['dim']}, ff_mult={config['ff_mult']}"
    )

    return model, device


def train_ultra_fast(model, trainloader, testloader, device, epochs=50):
    """è¶…å¿«é€Ÿè¨“ç·´é…ç½® - é‡å°å¿«é€ŸåŸå‹é–‹ç™¼ + éæ“¬åˆæ—©åœ"""
    print(f"\nğŸ‹ï¸ é–‹å§‹è¶…å¿«é€Ÿè¨“ç·´ ({epochs} å€‹ epochs)...")
    print("   ğŸ›¡ï¸  å•Ÿç”¨éæ“¬åˆæ—©åœä¿è­·")

    # å¿«é€Ÿè¨“ç·´é…ç½®
    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)  # æ¸›å°‘æ¨™ç±¤å¹³æ»‘
    optimizer = optim.AdamW(
        model.parameters(),
        lr=3e-3,  # æé«˜å­¸ç¿’ç‡åŠ é€Ÿæ”¶æ–‚
        weight_decay=0.01,  # æ¸›å°‘æ¬Šé‡è¡°æ¸›
        betas=(0.9, 0.95),  # èª¿æ•´momentum
    )

    # ç°¡åŒ–å­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=epochs, eta_min=1e-5
    )

    train_losses = []
    train_accs = []
    val_accs = []
    val_losses = []  # æ–°å¢ï¼šè¨˜éŒ„é©—è­‰æå¤±
    epoch_times = []

    best_val_acc = 0
    patience = 15  # æ¸›å°‘è€å¿ƒå€¼
    patience_counter = 0

    # éæ“¬åˆæ—©åœé…ç½® - é©é…è¶…å°æ¨¡å‹ï¼ˆæ›´åš´æ ¼ï¼‰
    overfitting_patience = 6  # æ›´çŸ­çš„éæ“¬åˆå®¹å¿æœŸï¼ˆå¾8é™åˆ°6ï¼‰
    overfitting_counter = 0
    overfitting_threshold = 8.0  # æ›´ä½çš„éæ“¬åˆé–¾å€¼ï¼ˆå¾10.0é™åˆ°8.0ï¼‰
    min_epochs_before_overfitting_check = 8  # æ›´æ—©é–‹å§‹æª¢æ¸¬ï¼ˆå¾10é™åˆ°8ï¼‰

    total_start_time = time.time()

    for epoch in range(epochs):
        epoch_start_time = time.time()

        # è¨“ç·´éšæ®µ
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        print(
            f"\nEpoch {epoch + 1}/{epochs}, LR: {optimizer.param_groups[0]['lr']:.6f}"
        )

        for i, (inputs, labels) in enumerate(trainloader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            # æ¯50å€‹æ‰¹æ¬¡æˆ–æœ€å¾Œä¸€å€‹æ‰¹æ¬¡é¡¯ç¤ºé€²åº¦
            if (i + 1) % 50 == 0 or (i + 1) == len(trainloader):
                acc = 100.0 * correct / total
                print(
                    f"   æ‰¹æ¬¡ {i+1:3d}/{len(trainloader)}: æå¤±={running_loss/(i+1):.4f}, æº–ç¢ºç‡={acc:.2f}%"
                )

        epoch_loss = running_loss / len(trainloader)
        epoch_acc = 100.0 * correct / total
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)

        # é©—è­‰éšæ®µ
        val_acc, val_loss = quick_validate_with_loss(
            model, testloader, device, criterion
        )
        val_accs.append(val_acc)
        val_losses.append(val_loss)

        scheduler.step()

        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        epoch_times.append(epoch_duration)

        print(
            f"Epoch {epoch + 1} å®Œæˆ: è¨“ç·´={epoch_acc:.2f}%, é©—è­‰={val_acc:.2f}%, æ™‚é–“={epoch_duration:.1f}s"
        )

        # éæ“¬åˆæ—©åœæª¢æ¸¬
        train_val_diff = epoch_acc - val_acc
        if epoch >= min_epochs_before_overfitting_check:
            if train_val_diff > overfitting_threshold:
                overfitting_counter += 1
                print(
                    f"   âš ï¸  éæ“¬åˆè­¦å‘Š: å·®ç•° {train_val_diff:.2f}% > é–¾å€¼ {overfitting_threshold}% ({overfitting_counter}/{overfitting_patience})"
                )

                if overfitting_counter >= overfitting_patience:
                    print(
                        f"   ğŸ›‘ éæ“¬åˆæ—©åœ: é€£çºŒ {overfitting_patience} epochs è¨“ç·´-é©—è­‰å·®ç•°è¶…é {overfitting_threshold}%"
                    )
                    break
            else:
                overfitting_counter = 0  # é‡ç½®è¨ˆæ•¸å™¨

        # é¡¯ç¤ºè¨“ç·´-é©—è­‰å·®ç•°ï¼ˆç”¨æ–¼ç›£æ§ï¼‰
        if train_val_diff > 5:
            print(f"   ğŸ“Š è¨“ç·´-é©—è­‰å·®ç•°: {train_val_diff:.2f}%")

        # æ—©åœæ©Ÿåˆ¶
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save(model.state_dict(), "best_ultra_small_model.pth")
            print(f"   ğŸ’¾ æ–°æœ€ä½³æ¨¡å‹å·²ä¿å­˜: é©—è­‰æº–ç¢ºç‡ {best_val_acc:.2f}%")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"   â° æ€§èƒ½æ—©åœï¼šé©—è­‰æº–ç¢ºç‡ {patience} å€‹epochæœªæå‡")
                break

    total_end_time = time.time()
    total_training_time = total_end_time - total_start_time

    print(f"\nâ±ï¸ è¶…å¿«é€Ÿè¨“ç·´æ™‚é–“çµ±è¨ˆ:")
    print(
        f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.1f}s ({total_training_time/60:.1f}min)"
    )
    print(f"   â€¢ å¯¦éš›è¨“ç·´epochs: {len(train_losses)} / {epochs}")
    print(f"   â€¢ å¹³å‡æ¯epoch: {np.mean(epoch_times):.1f}s")
    print(f"   â€¢ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")

    # æ—©åœåŸå› åˆ†æ
    if len(train_losses) < epochs:
        final_train_val_diff = (
            train_accs[-1] - val_accs[-1] if train_accs and val_accs else 0
        )
        if overfitting_counter >= overfitting_patience:
            print(f"   â€¢ æ—©åœåŸå› : éæ“¬åˆæª¢æ¸¬ (å·®ç•°: {final_train_val_diff:.2f}%)")
        elif patience_counter >= patience:
            print(f"   â€¢ æ—©åœåŸå› : æ€§èƒ½åœæ»¯ ({patience} epochsç„¡æå‡)")
    else:
        print(f"   â€¢ è¨“ç·´ç‹€æ…‹: å®Œæ•´è¨“ç·´ ({epochs} epochs)")

    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    if best_val_acc > 0:
        model.load_state_dict(torch.load("best_ultra_small_model.pth"))
        print("   â€¢ å·²è¼‰å…¥æœ€ä½³æ¨¡å‹æ¬Šé‡")

    return (
        train_losses,
        train_accs,
        val_accs,
        val_losses,
        epoch_times,
        total_training_time,
    )


def quick_validate(model, testloader, device):
    """å¿«é€Ÿé©—è­‰"""
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return 100.0 * correct / total


def quick_validate_with_loss(model, testloader, device, criterion):
    """å¿«é€Ÿé©—è­‰ï¼ˆåŒæ™‚è¨ˆç®—æº–ç¢ºç‡å’Œæå¤±ï¼‰"""
    model.eval()
    correct = 0
    total = 0
    running_loss = 0.0
    num_batches = 0

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            num_batches += 1

            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    accuracy = 100.0 * correct / total
    avg_loss = running_loss / num_batches if num_batches > 0 else 0.0

    return accuracy, avg_loss


def evaluate_ultra_model(model, testloader, device, classes):
    """è©•ä¼°è¶…ç¸®å°ç‰ˆæ¨¡å‹"""
    print("\nğŸ“Š è©•ä¼°è¶…ç¸®å°ç‰ˆæ¨¡å‹...")

    model.eval()
    all_predictions = []
    all_labels = []
    correct = 0
    total = 0
    class_correct = [0] * 10
    class_total = [0] * 10

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)

            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            for i in range(labels.size(0)):
                label = labels[i]
                class_correct[label] += (predicted[i] == label).item()
                class_total[label] += 1

    overall_acc = 100.0 * correct / total
    print(f"   âœ“ æ•´é«”æº–ç¢ºç‡: {overall_acc:.2f}%")

    # ç°¡åŒ–çš„çµæœå¯è¦–åŒ–
    plt.figure(figsize=(12, 8))

    # å„é¡åˆ¥æº–ç¢ºç‡
    plt.subplot(2, 2, 1)
    class_accs = [
        100.0 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0
        for i in range(10)
    ]
    bars = plt.bar(classes, class_accs, color=plt.cm.tab10(np.arange(10)))
    plt.title("Ultra-Small gMLP: Class Accuracy", fontweight="bold")
    plt.ylabel("Accuracy (%)")
    plt.xticks(rotation=45)
    plt.grid(axis="y", alpha=0.3)

    for bar, acc in zip(bars, class_accs):
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{acc:.1f}%",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # æ··æ·†çŸ©é™£
    plt.subplot(2, 2, 2)
    cm = confusion_matrix(all_labels, all_predictions)
    sns.heatmap(
        cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes
    )
    plt.title("Confusion Matrix", fontweight="bold")
    plt.xlabel("Predicted")
    plt.ylabel("True")

    # æº–ç¢ºç‡åˆ†ä½ˆ
    plt.subplot(2, 2, 3)
    plt.hist(class_accs, bins=8, alpha=0.7, color="skyblue", edgecolor="black")
    plt.title("Class Accuracy Distribution", fontweight="bold")
    plt.xlabel("Accuracy (%)")
    plt.ylabel("Count")
    plt.grid(axis="y", alpha=0.3)

    # æ¨¡å‹çµ±è¨ˆ
    plt.subplot(2, 2, 4)
    stats_text = f"""Model Statistics:
â€¢ Overall Accuracy: {overall_acc:.2f}%
â€¢ Best Class: {classes[np.argmax(class_accs)]} ({max(class_accs):.1f}%)
â€¢ Worst Class: {classes[np.argmin(class_accs)]} ({min(class_accs):.1f}%)
â€¢ Average Class Accuracy: {np.mean(class_accs):.1f}%
â€¢ Standard Deviation: {np.std(class_accs):.1f}%"""

    plt.text(
        0.1,
        0.5,
        stats_text,
        fontsize=11,
        verticalalignment="center",
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"),
    )
    plt.axis("off")
    plt.title("Model Statistics", fontweight="bold")

    plt.tight_layout()
    plt.savefig("ultra_small_gmlp_results.png", dpi=300, bbox_inches="tight")
    plt.show()

    return overall_acc


def plot_ultra_training_history(
    train_losses, train_accs, val_accs, val_losses, epoch_times
):
    """ç¹ªè£½è¶…å¿«é€Ÿè¨“ç·´æ­·å²ï¼ˆåŒ…å«è¨“ç·´å’Œé©—è­‰æå¤±æ¯”è¼ƒï¼‰"""
    print("\nğŸ“ˆ ç¹ªè£½è¶…å¿«é€Ÿè¨“ç·´æ­·å²...")

    plt.figure(figsize=(16, 4))

    # æå¤±æ›²ç·šæ¯”è¼ƒï¼ˆè¨“ç·´ vs é©—è­‰ï¼‰
    plt.subplot(1, 4, 1)
    plt.plot(train_losses, "b-", linewidth=2, label="Training Loss")
    plt.plot(val_losses, "r-", linewidth=2, label="Validation Loss")
    plt.title("Loss Comparison", fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # æº–ç¢ºç‡æ›²ç·š
    plt.subplot(1, 4, 2)
    plt.plot(train_accs, "g-", linewidth=2, label="Training Acc")
    plt.plot(val_accs, "r-", linewidth=2, label="Validation Acc")
    plt.title("Accuracy Curves", fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # éæ“¬åˆç›£æ§ï¼ˆæº–ç¢ºç‡å·®ç•°ï¼‰
    plt.subplot(1, 4, 3)
    acc_diff = np.array(train_accs) - np.array(val_accs)
    plt.plot(acc_diff, "purple", linewidth=2, label="Accuracy Diff")
    plt.title("Overfitting Monitor", fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Training - Validation (%)")
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color="black", linestyle="--", alpha=0.5)
    plt.legend()

    # è¨“ç·´æ™‚é–“
    plt.subplot(1, 4, 4)
    plt.plot(epoch_times, "orange", linewidth=2, marker="o", markersize=3)
    plt.title("Training Time per Epoch", fontweight="bold")
    plt.xlabel("Epoch")
    plt.ylabel("Time (seconds)")
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("ultra_small_training_history.png", dpi=300, bbox_inches="tight")
    plt.show()


def compare_ultra_architectures():
    """æ¯”è¼ƒè¶…ç¸®å°ç‰ˆæ¨¡å‹æ¶æ§‹"""
    print("\nğŸ“‹ è¶…ç¸®å°ç‰ˆ gMLP æ¨¡å‹æ¶æ§‹æ¯”è¼ƒ:")
    print("=" * 85)
    print(
        f"{'æ¨¡å‹':<8} {'æ·±åº¦':<6} {'ç¶­åº¦':<8} {'FFNå€æ•¸':<8} {'åƒæ•¸(M)':<10} {'éæ“¬åˆé¢¨éšª':<12}"
    )
    print("-" * 85)

    configs = {
        "Test": {"depth": 4, "dim": 64, "ff_mult": 2, "params": 0.1, "risk": "æ¥µä½"},
        "Nano": {"depth": 6, "dim": 64, "ff_mult": 2, "params": 0.3, "risk": "å¾ˆä½"},
        "XS": {"depth": 8, "dim": 80, "ff_mult": 3, "params": 0.8, "risk": "ä½"},
        "S": {"depth": 12, "dim": 128, "ff_mult": 3, "params": 2.0, "risk": "ä¸­ç­‰"},
        "M": {"depth": 16, "dim": 160, "ff_mult": 4, "params": 4.5, "risk": "è¼ƒé«˜"},
        "L": {"depth": 30, "dim": 128, "ff_mult": 6, "params": 5.9, "risk": "å¾ˆé«˜"},
    }

    for model, config in configs.items():
        print(
            f"{model:<8} {config['depth']:<6} {config['dim']:<8} "
            f"{config['ff_mult']:<8} {config['params']:<10} {config['risk']:<12}"
        )

    print("-" * 85)
    print("ğŸ¯ é¸æ“‡å»ºè­°ï¼ˆé‡å°ä¸åŒæ•¸æ“šé‡ï¼‰:")
    print("   ğŸ“Š æ•¸æ“šé‡å°å‘é¸æ“‡:")
    print("     â€¢ < 25K æ¨£æœ¬: Test/Nano (æ¥µä½éæ“¬åˆé¢¨éšª)")
    print("     â€¢ 25K-40K æ¨£æœ¬: XS (æ¨è–¦ï¼Œå¹³è¡¡æ€§èƒ½) â­")
    print("     â€¢ 40K-50K æ¨£æœ¬: S (è¼ƒå¥½æ€§èƒ½)")
    print("     â€¢ > 50K æ¨£æœ¬: M (é«˜æ€§èƒ½ï¼Œéœ€ç›£æ§éæ“¬åˆ)")
    print("\n   â±ï¸ è¨“ç·´æ™‚é–“å°å‘é¸æ“‡:")
    print("     â€¢ Test: è¶…æ¥µé€Ÿæ¸¬è©¦ (<30ç§’) ğŸš€")
    print("     â€¢ Nano: æ¥µé€ŸåŸå‹é–‹ç™¼ (<1åˆ†é˜)")
    print("     â€¢ XS: å¿«é€Ÿå¯¦é©— (~2åˆ†é˜) â­æ¨è–¦")
    print("     â€¢ S: å¹³è¡¡è¨“ç·´ (~5åˆ†é˜)")
    print("     â€¢ M/L: é•·æ™‚é–“è¨“ç·´ (>10åˆ†é˜)")
    print("\n   ğŸ›¡ï¸ éæ“¬åˆé¢¨éšªæ§åˆ¶:")
    print("     â€¢ åš´æ ¼æ§åˆ¶: Test/Nano")
    print("     â€¢ å¹³è¡¡æ§åˆ¶: XS/S â­æ¨è–¦")
    print("     â€¢ éœ€è¦ç›£æ§: M/L (é…åˆæ—©åœ)")


def get_model_recommendation(train_samples, target_time_min=5, risk_tolerance="medium"):
    """æ™ºèƒ½æ¨¡å‹æ¨è–¦ç³»çµ±"""
    print(f"\nğŸ¤– æ™ºèƒ½æ¨¡å‹æ¨è–¦ç³»çµ±")
    print(f"   ğŸ“Š è¨“ç·´æ¨£æœ¬: {train_samples:,}")
    print(f"   â±ï¸ ç›®æ¨™æ™‚é–“: {target_time_min} åˆ†é˜")
    print(f"   ğŸ›¡ï¸ é¢¨éšªå®¹å¿: {risk_tolerance}")
    print("-" * 50)

    # åŸºæ–¼æ•¸æ“šé‡çš„åŸºç¤æ¨è–¦
    if train_samples < 25000:
        base_recommendation = "Nano"
        risk_level = "very_low"
    elif train_samples < 40000:
        base_recommendation = "XS"
        risk_level = "low"
    elif train_samples < 50000:
        base_recommendation = "S"
        risk_level = "medium"
    else:
        base_recommendation = "M"
        risk_level = "high"

    # æ™‚é–“ç´„æŸèª¿æ•´
    time_map = {"Test": 0.5, "Nano": 1, "XS": 2, "S": 5, "M": 10, "L": 15}
    if target_time_min < 2:
        time_recommendation = "Nano"
    elif target_time_min < 4:
        time_recommendation = "XS"
    elif target_time_min < 8:
        time_recommendation = "S"
    else:
        time_recommendation = "M"

    # é¢¨éšªå®¹å¿èª¿æ•´
    risk_map = {"low": ["Test", "Nano"], "medium": ["XS", "S"], "high": ["M", "L"]}

    risk_recommendations = risk_map.get(risk_tolerance, ["XS", "S"])

    # ç¶œåˆæ±ºç­–
    recommendations = [base_recommendation, time_recommendation] + risk_recommendations
    final_recommendation = max(set(recommendations), key=recommendations.count)

    print(f"   ğŸ¯ åŸºæ–¼æ•¸æ“šé‡: {base_recommendation}")
    print(f"   â±ï¸ åŸºæ–¼æ™‚é–“ç´„æŸ: {time_recommendation}")
    print(f"   ğŸ›¡ï¸ åŸºæ–¼é¢¨éšªå®¹å¿: {', '.join(risk_recommendations)}")
    print(f"   âœ… æœ€çµ‚æ¨è–¦: {final_recommendation}")

    return final_recommendation


def main():
    """ä¸»å‡½æ•¸ - è¶…å¿«é€Ÿç‰ˆæœ¬"""
    print("ğŸš€ è¶…ç¸®å°ç‰ˆ gMLP åœ–åƒåˆ†é¡è¨“ç·´é–‹å§‹")
    print("=" * 60)

    # æ¶æ§‹æ¯”è¼ƒ
    compare_ultra_architectures()

    # æ•¸æ“šåŠ è¼‰
    trainloader, testloader, classes = load_cifar10_data_ultrafast(quick_test=True)

    # æ™ºèƒ½æ¨¡å‹é¸æ“‡ï¼šåŸºæ–¼æ•¸æ“šé›†è¦æ¨¡è‡ªå‹•é¸æ“‡æœ€ä½³æ¶æ§‹
    train_samples = len(trainloader.dataset)

    # ä½¿ç”¨æ™ºèƒ½æ¨è–¦ç³»çµ±
    model_size = get_model_recommendation(
        train_samples=train_samples,
        target_time_min=5,  # ç›®æ¨™è¨“ç·´æ™‚é–“ï¼ˆåˆ†é˜ï¼‰
        risk_tolerance="medium",  # éæ“¬åˆé¢¨éšªå®¹å¿åº¦: "low", "medium", "high"
    )

    # å¯æ‰‹å‹•è¦†è“‹è‡ªå‹•é¸æ“‡
    # model_size = "XS"  # å–æ¶ˆè¨»è§£ä»¥æ‰‹å‹•é¸æ“‡: "Test", "Nano", "XS", "S", "M", "L"
    model, device = create_ultra_small_gmlp_model(model_size=model_size)

    # å¿«é€Ÿè¨“ç·´
    train_losses, train_accs, val_accs, val_losses, epoch_times, total_time = (
        train_ultra_fast(
            model, trainloader, testloader, device, epochs=100  # å¯èª¿æ•´epochs
        )
    )

    # çµæœå¯è¦–åŒ–ï¼ˆåŒ…å«è¨“ç·´å’Œé©—è­‰æå¤±æ¯”è¼ƒï¼‰
    plot_ultra_training_history(
        train_losses, train_accs, val_accs, val_losses, epoch_times
    )

    # æ¨¡å‹è©•ä¼°
    final_acc = evaluate_ultra_model(model, testloader, device, classes)

    print(f"\nğŸ‰ è¶…å¿«é€Ÿè¨“ç·´å®Œæˆ!")
    print(f"   â€¢ æ¨¡å‹å¤§å°: {model_size}")
    print(f"   â€¢ æœ€çµ‚æº–ç¢ºç‡: {final_acc:.2f}%")
    print(f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
    print(f"   â€¢ å¹³å‡æ¯epoch: {np.mean(epoch_times):.1f} ç§’")


if __name__ == "__main__":
    main()

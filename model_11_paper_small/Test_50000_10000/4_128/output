📋 超縮小版 gMLP 模型架構比較:
=====================================================================================
模型       深度     維度       FFN倍數    參數(M)      過擬合風險
-------------------------------------------------------------------------------------
Test     4      32       2        0.1        極低
Nano     6      64       2        0.3        很低
XS       8      80       3        0.8        低
S        12     128      3        2.0        中等
M        16     160      4        4.5        較高
L        30     128      6        5.9        很高
-------------------------------------------------------------------------------------
🎯 建議（針對50K訓練樣本）:
   • Test: 超極速測試，最低參數量 (<30秒訓練) 🚀新增
   • Nano: 極速原型開發，最低過擬合風險 (<1分鐘訓練)
   • XS: 快速實驗，低過擬合風險 (~2分鐘訓練) ⭐推薦
   • S: 平衡性能與速度，中等過擬合風險 (~5分鐘訓練)
   • M: 較好性能但過擬合風險較高 (~10分鐘訓練)
   • L: 最大模型，很高過擬合風險，不建議用於小數據集
📦 加載超快速 CIFAR-10 數據集...
   🚀 超快速模式：小規模數據集訓練
   ✓ 訓練樣本: 50000
   ✓ 測試樣本: 10000
   ✓ Batch大小: 64

🏗️ 創建超縮小版 gMLP-Test 模型...
   ⚡ CPU模式：已設置4個線程
   ✓ 超縮小版 gMLP-Test 模型創建完成
   ✓ 設備: cpu
   ✓ 參數數量: 224,650 (0.22M)
   ✓ 目標參數: 0.1M
   ✓ 模型大小: 0.9 MB
   ✓ 架構: depth=4, dim=128, ff_mult=2

🏋️ 開始超快速訓練 (100 個 epochs)...
   🛡️  啟用過擬合早停保護

Epoch 1/100, LR: 0.003000
   批次  50/782: 損失=2.0426, 準確率=25.00%
   批次 100/782: 損失=1.9468, 準確率=29.22%
   批次 150/782: 損失=1.8728, 準確率=32.52%
   批次 200/782: 損失=1.8266, 準確率=34.66%
   批次 250/782: 損失=1.7937, 準確率=35.99%
   批次 300/782: 損失=1.7652, 準確率=37.24%
   批次 350/782: 損失=1.7410, 準確率=38.21%
   批次 400/782: 損失=1.7205, 準確率=39.05%
   批次 450/782: 損失=1.7036, 準確率=39.86%
   批次 500/782: 損失=1.6855, 準確率=40.74%
   批次 550/782: 損失=1.6669, 準確率=41.59%
   批次 600/782: 損失=1.6502, 準確率=42.33%
   批次 650/782: 損失=1.6352, 準確率=43.05%
   批次 700/782: 損失=1.6248, 準確率=43.56%
   批次 750/782: 損失=1.6109, 準確率=44.24%
   批次 782/782: 損失=1.6043, 準確率=44.53%
Epoch 1 完成: 訓練=44.53%, 驗證=52.75%, 時間=136.0s
   💾 新最佳模型已保存: 驗證準確率 52.75%

Epoch 2/100, LR: 0.002999
   批次  50/782: 損失=1.3981, 準確率=53.34%
   批次 100/782: 損失=1.3853, 準確率=54.17%
   批次 150/782: 損失=1.3855, 準確率=54.15%
   批次 200/782: 損失=1.3824, 準確率=54.45%
   批次 250/782: 損失=1.3748, 準確率=55.12%
   批次 300/782: 損失=1.3739, 準確率=55.12%
   批次 350/782: 損失=1.3732, 準確率=55.14%
   批次 400/782: 損失=1.3673, 準確率=55.31%
   批次 450/782: 損失=1.3619, 準確率=55.47%
   批次 500/782: 損失=1.3539, 準確率=55.79%
   批次 550/782: 損失=1.3503, 準確率=55.96%
   批次 600/782: 損失=1.3469, 準確率=56.29%
   批次 650/782: 損失=1.3433, 準確率=56.53%
   批次 700/782: 損失=1.3412, 準確率=56.67%
   批次 750/782: 損失=1.3365, 準確率=56.83%
   批次 782/782: 損失=1.3333, 準確率=57.03%
Epoch 2 完成: 訓練=57.03%, 驗證=61.33%, 時間=144.8s
   💾 新最佳模型已保存: 驗證準確率 61.33%

Epoch 3/100, LR: 0.002997
   批次  50/782: 損失=1.2686, 準確率=59.38%
   批次 100/782: 損失=1.2703, 準確率=59.36%
   批次 150/782: 損失=1.2735, 準確率=59.22%
   批次 200/782: 損失=1.2679, 準確率=59.51%
   批次 250/782: 損失=1.2651, 準確率=59.89%
   批次 300/782: 損失=1.2632, 準確率=59.91%
   批次 350/782: 損失=1.2577, 準確率=60.20%
   批次 400/782: 損失=1.2537, 準確率=60.43%
   批次 450/782: 損失=1.2495, 準確率=60.70%
   批次 500/782: 損失=1.2449, 準確率=60.85%
   批次 550/782: 損失=1.2443, 準確率=60.94%
   批次 600/782: 損失=1.2436, 準確率=61.00%
   批次 650/782: 損失=1.2422, 準確率=60.97%
   批次 700/782: 損失=1.2380, 準確率=61.20%
   批次 750/782: 損失=1.2360, 準確率=61.36%
   批次 782/782: 損失=1.2338, 準確率=61.43%
Epoch 3 完成: 訓練=61.43%, 驗證=62.18%, 時間=117.2s
   💾 新最佳模型已保存: 驗證準確率 62.18%

Epoch 4/100, LR: 0.002993
   批次  50/782: 損失=1.2030, 準確率=62.75%
   批次 100/782: 損失=1.2067, 準確率=62.42%
   批次 150/782: 損失=1.1974, 準確率=62.59%
   批次 200/782: 損失=1.1895, 準確率=62.96%
   批次 250/782: 損失=1.1863, 準確率=63.20%
   批次 300/782: 損失=1.1874, 準確率=63.29%
   批次 350/782: 損失=1.1832, 準確率=63.50%
   批次 400/782: 損失=1.1795, 準確率=63.64%
   批次 450/782: 損失=1.1774, 準確率=63.69%
   批次 500/782: 損失=1.1782, 準確率=63.66%
   批次 550/782: 損失=1.1765, 準確率=63.70%
   批次 600/782: 損失=1.1732, 準確率=63.89%
   批次 650/782: 損失=1.1731, 準確率=63.88%
   批次 700/782: 損失=1.1713, 準確率=63.94%
   批次 750/782: 損失=1.1696, 準確率=63.99%
   批次 782/782: 損失=1.1685, 準確率=63.99%
Epoch 4 完成: 訓練=63.99%, 驗證=65.15%, 時間=118.1s
   💾 新最佳模型已保存: 驗證準確率 65.15%

Epoch 5/100, LR: 0.002988
   批次  50/782: 損失=1.1494, 準確率=65.34%
   批次 100/782: 損失=1.1490, 準確率=65.73%
   批次 150/782: 損失=1.1383, 準確率=65.94%
   批次 200/782: 損失=1.1407, 準確率=65.54%
   批次 250/782: 損失=1.1363, 準確率=65.58%
   批次 300/782: 損失=1.1361, 準確率=65.65%
   批次 350/782: 損失=1.1342, 準確率=65.69%
   批次 400/782: 損失=1.1340, 準確率=65.71%
   批次 450/782: 損失=1.1300, 準確率=65.93%
   批次 500/782: 損失=1.1300, 準確率=65.92%
   批次 550/782: 損失=1.1290, 準確率=65.92%
   批次 600/782: 損失=1.1248, 準確率=66.03%
   批次 650/782: 損失=1.1225, 準確率=66.11%
   批次 700/782: 損失=1.1216, 準確率=66.21%
   批次 750/782: 損失=1.1223, 準確率=66.27%
   批次 782/782: 損失=1.1197, 準確率=66.36%
Epoch 5 完成: 訓練=66.36%, 驗證=67.45%, 時間=121.7s
   💾 新最佳模型已保存: 驗證準確率 67.45%

Epoch 6/100, LR: 0.002982
   批次  50/782: 損失=1.0969, 準確率=67.75%
   批次 100/782: 損失=1.0837, 準確率=68.06%
   批次 150/782: 損失=1.0927, 準確率=67.97%
   批次 200/782: 損失=1.0941, 準確率=67.75%
   批次 250/782: 損失=1.0942, 準確率=67.54%
   批次 300/782: 損失=1.0956, 準確率=67.44%
   批次 350/782: 損失=1.0950, 準確率=67.44%
   批次 400/782: 損失=1.0935, 準確率=67.45%
   批次 450/782: 損失=1.0872, 準確率=67.77%
   批次 500/782: 損失=1.0850, 準確率=67.88%
   批次 550/782: 損失=1.0843, 準確率=67.92%
   批次 600/782: 損失=1.0818, 準確率=68.09%
   批次 650/782: 損失=1.0832, 準確率=68.04%
   批次 700/782: 損失=1.0848, 準確率=68.00%
   批次 750/782: 損失=1.0831, 準確率=68.10%
   批次 782/782: 損失=1.0823, 準確率=68.11%
Epoch 6 完成: 訓練=68.11%, 驗證=68.39%, 時間=120.1s
   💾 新最佳模型已保存: 驗證準確率 68.39%

Epoch 7/100, LR: 0.002974
   批次  50/782: 損失=1.0331, 準確率=70.16%
   批次 100/782: 損失=1.0344, 準確率=70.11%
   批次 150/782: 損失=1.0387, 準確率=70.03%
   批次 200/782: 損失=1.0378, 準確率=70.05%
   批次 250/782: 損失=1.0329, 準確率=70.09%
   批次 300/782: 損失=1.0323, 準確率=70.01%
   批次 350/782: 損失=1.0326, 準確率=69.98%
   批次 400/782: 損失=1.0315, 準確率=70.06%
   批次 450/782: 損失=1.0358, 準確率=69.95%
   批次 500/782: 損失=1.0382, 準確率=69.91%
   批次 550/782: 損失=1.0374, 準確率=69.96%
   批次 600/782: 損失=1.0386, 準確率=69.84%
   批次 650/782: 損失=1.0377, 準確率=69.90%
   批次 700/782: 損失=1.0366, 準確率=69.93%
   批次 750/782: 損失=1.0362, 準確率=69.94%
   批次 782/782: 損失=1.0359, 準確率=69.96%
Epoch 7 完成: 訓練=69.96%, 驗證=70.13%, 時間=122.9s
   💾 新最佳模型已保存: 驗證準確率 70.13%

Epoch 8/100, LR: 0.002964
   批次  50/782: 損失=1.0006, 準確率=72.19%
   批次 100/782: 損失=0.9816, 準確率=72.20%
   批次 150/782: 損失=0.9941, 準確率=71.75%
   批次 200/782: 損失=1.0000, 準確率=71.48%
   批次 250/782: 損失=1.0004, 準確率=71.53%
   批次 300/782: 損失=0.9997, 準確率=71.52%
   批次 350/782: 損失=1.0036, 準確率=71.27%
   批次 400/782: 損失=1.0024, 準確率=71.39%
   批次 450/782: 損失=1.0019, 準確率=71.39%
   批次 500/782: 損失=1.0034, 準確率=71.28%
   批次 550/782: 損失=1.0031, 準確率=71.32%
   批次 600/782: 損失=1.0037, 準確率=71.35%
   批次 650/782: 損失=1.0047, 準確率=71.27%
   批次 700/782: 損失=1.0029, 準確率=71.34%
   批次 750/782: 損失=1.0025, 準確率=71.33%
   批次 782/782: 損失=1.0018, 準確率=71.39%
Epoch 8 完成: 訓練=71.39%, 驗證=71.72%, 時間=128.8s
   💾 新最佳模型已保存: 驗證準確率 71.72%

Epoch 9/100, LR: 0.002953
   批次  50/782: 損失=0.9638, 準確率=73.12%
   批次 100/782: 損失=0.9766, 準確率=72.59%
   批次 150/782: 損失=0.9686, 準確率=72.94%
   批次 200/782: 損失=0.9744, 準確率=72.63%
   批次 250/782: 損失=0.9715, 準確率=72.83%
   批次 300/782: 損失=0.9720, 準確率=72.59%
   批次 350/782: 損失=0.9740, 準確率=72.53%
   批次 400/782: 損失=0.9722, 準確率=72.77%
   批次 450/782: 損失=0.9724, 準確率=72.77%
   批次 500/782: 損失=0.9727, 準確率=72.67%
   批次 550/782: 損失=0.9749, 準確率=72.58%
   批次 600/782: 損失=0.9758, 準確率=72.59%
   批次 650/782: 損失=0.9769, 準確率=72.46%
   批次 700/782: 損失=0.9763, 準確率=72.47%
   批次 750/782: 損失=0.9746, 準確率=72.54%
   批次 782/782: 損失=0.9746, 準確率=72.54%
Epoch 9 完成: 訓練=72.54%, 驗證=72.34%, 時間=119.0s
   💾 新最佳模型已保存: 驗證準確率 72.34%

Epoch 10/100, LR: 0.002941
   批次  50/782: 損失=0.9415, 準確率=74.19%
   批次 100/782: 損失=0.9235, 準確率=74.75%
   批次 150/782: 損失=0.9382, 準確率=73.91%
   批次 200/782: 損失=0.9500, 準確率=73.49%
   批次 250/782: 損失=0.9540, 準確率=73.26%
   批次 300/782: 損失=0.9547, 準確率=73.31%
   批次 350/782: 損失=0.9575, 準確率=73.07%
   批次 400/782: 損失=0.9556, 準確率=73.16%
   批次 450/782: 損失=0.9542, 準確率=73.28%
   批次 500/782: 損失=0.9563, 準確率=73.12%
   批次 550/782: 損失=0.9552, 準確率=73.14%
   批次 600/782: 損失=0.9535, 準確率=73.24%
   批次 650/782: 損失=0.9535, 準確率=73.20%
   批次 700/782: 損失=0.9550, 準確率=73.11%
   批次 750/782: 損失=0.9543, 準確率=73.15%
   批次 782/782: 損失=0.9552, 準確率=73.20%
Epoch 10 完成: 訓練=73.20%, 驗證=72.42%, 時間=119.8s
   💾 新最佳模型已保存: 驗證準確率 72.42%

Epoch 11/100, LR: 0.002927
   批次  50/782: 損失=0.9293, 準確率=74.12%
   批次 100/782: 損失=0.9234, 準確率=74.64%
   批次 150/782: 損失=0.9231, 準確率=74.72%
   批次 200/782: 損失=0.9236, 準確率=74.52%
   批次 250/782: 損失=0.9272, 準確率=74.50%
   批次 300/782: 損失=0.9276, 準確率=74.49%
   批次 350/782: 損失=0.9286, 準確率=74.53%
   批次 400/782: 損失=0.9308, 準確率=74.40%
   批次 450/782: 損失=0.9328, 準確率=74.27%
   批次 500/782: 損失=0.9298, 準確率=74.40%
   批次 550/782: 損失=0.9300, 準確率=74.38%
   批次 600/782: 損失=0.9312, 準確率=74.29%
   批次 650/782: 損失=0.9297, 準確率=74.33%
   批次 700/782: 損失=0.9294, 準確率=74.38%
   批次 750/782: 損失=0.9299, 準確率=74.35%
   批次 782/782: 損失=0.9312, 準確率=74.33%
Epoch 11 完成: 訓練=74.33%, 驗證=72.01%, 時間=128.1s

Epoch 12/100, LR: 0.002912
   批次  50/782: 損失=0.8958, 準確率=76.03%
   批次 100/782: 損失=0.8938, 準確率=75.95%
   批次 150/782: 損失=0.8996, 準確率=75.46%
   批次 200/782: 損失=0.8920, 準確率=75.71%
   批次 250/782: 損失=0.8936, 準確率=75.66%
   批次 300/782: 損失=0.8989, 準確率=75.55%
   批次 350/782: 損失=0.9027, 準確率=75.38%
   批次 400/782: 損失=0.9086, 準確率=75.20%
   批次 450/782: 損失=0.9124, 準確率=75.05%
   批次 500/782: 損失=0.9145, 準確率=74.95%
   批次 550/782: 損失=0.9153, 準確率=74.96%
   批次 600/782: 損失=0.9134, 準確率=75.03%
   批次 650/782: 損失=0.9143, 準確率=75.01%
   批次 700/782: 損失=0.9161, 準確率=74.94%
   批次 750/782: 損失=0.9169, 準確率=74.87%
   批次 782/782: 損失=0.9189, 準確率=74.83%
Epoch 12 完成: 訓練=74.83%, 驗證=73.50%, 時間=122.6s
   💾 新最佳模型已保存: 驗證準確率 73.50%

Epoch 13/100, LR: 0.002895
   批次  50/782: 損失=0.8906, 準確率=75.66%
   批次 100/782: 損失=0.8890, 準確率=75.80%
   批次 150/782: 損失=0.8842, 準確率=76.42%
   批次 200/782: 損失=0.8842, 準確率=76.49%
   批次 250/782: 損失=0.8864, 準確率=76.52%
   批次 300/782: 損失=0.8876, 準確率=76.48%
   批次 350/782: 損失=0.8887, 準確率=76.36%
   批次 400/782: 損失=0.8906, 準確率=76.30%
   批次 450/782: 損失=0.8918, 準確率=76.25%
   批次 500/782: 損失=0.8919, 準確率=76.18%
   批次 550/782: 損失=0.8922, 準確率=76.19%
   批次 600/782: 損失=0.8930, 準確率=76.10%
   批次 650/782: 損失=0.8950, 準確率=76.00%
   批次 700/782: 損失=0.8968, 準確率=75.94%
   批次 750/782: 損失=0.8978, 準確率=75.88%
   批次 782/782: 損失=0.8984, 準確率=75.88%
Epoch 13 完成: 訓練=75.88%, 驗證=74.13%, 時間=120.0s
   💾 新最佳模型已保存: 驗證準確率 74.13%

Epoch 14/100, LR: 0.002877
   批次  50/782: 損失=0.8651, 準確率=77.47%
   批次 100/782: 損失=0.8754, 準確率=76.83%
   批次 150/782: 損失=0.8754, 準確率=76.83%
   批次 200/782: 損失=0.8768, 準確率=76.64%
   批次 250/782: 損失=0.8794, 準確率=76.69%
   批次 300/782: 損失=0.8765, 準確率=76.94%
   批次 350/782: 損失=0.8790, 準確率=76.78%
   批次 400/782: 損失=0.8793, 準確率=76.68%
   批次 450/782: 損失=0.8786, 準確率=76.67%
   批次 500/782: 損失=0.8788, 準確率=76.62%
   批次 550/782: 損失=0.8792, 準確率=76.58%
   批次 600/782: 損失=0.8804, 準確率=76.52%
   批次 650/782: 損失=0.8815, 準確率=76.51%
   批次 700/782: 損失=0.8832, 準確率=76.39%
   批次 750/782: 損失=0.8831, 準確率=76.37%
   批次 782/782: 損失=0.8834, 準確率=76.34%
Epoch 14 完成: 訓練=76.34%, 驗證=74.24%, 時間=121.9s
   💾 新最佳模型已保存: 驗證準確率 74.24%

Epoch 15/100, LR: 0.002858
   批次  50/782: 損失=0.8626, 準確率=77.66%
   批次 100/782: 損失=0.8589, 準確率=77.50%
   批次 150/782: 損失=0.8694, 準確率=77.16%
   批次 200/782: 損失=0.8688, 準確率=77.21%
   批次 250/782: 損失=0.8650, 準確率=77.34%
   批次 300/782: 損失=0.8658, 準確率=77.37%
   批次 350/782: 損失=0.8667, 準確率=77.29%
   批次 400/782: 損失=0.8685, 準確率=77.23%
   批次 450/782: 損失=0.8696, 準確率=77.13%
   批次 500/782: 損失=0.8675, 準確率=77.24%
   批次 550/782: 損失=0.8664, 準確率=77.25%
   批次 600/782: 損失=0.8675, 準確率=77.22%
   批次 650/782: 損失=0.8691, 準確率=77.14%
   批次 700/782: 損失=0.8711, 準確率=77.09%
   批次 750/782: 損失=0.8737, 準確率=77.00%
   批次 782/782: 損失=0.8739, 準確率=77.01%
Epoch 15 完成: 訓練=77.01%, 驗證=74.97%, 時間=121.0s
   💾 新最佳模型已保存: 驗證準確率 74.97%

Epoch 16/100, LR: 0.002837
   批次  50/782: 損失=0.8247, 準確率=78.91%
   批次 100/782: 損失=0.8382, 準確率=78.50%
   批次 150/782: 損失=0.8460, 準確率=78.12%
   批次 200/782: 損失=0.8501, 準確率=77.89%
   批次 250/782: 損失=0.8477, 準確率=77.84%
   批次 300/782: 損失=0.8506, 準確率=77.80%
   批次 350/782: 損失=0.8511, 準確率=77.83%
   批次 400/782: 損失=0.8514, 準確率=77.77%
   批次 450/782: 損失=0.8562, 準確率=77.50%
   批次 500/782: 損失=0.8562, 準確率=77.55%
   批次 550/782: 損失=0.8573, 準確率=77.47%
   批次 600/782: 損失=0.8584, 準確率=77.40%
   批次 650/782: 損失=0.8591, 準確率=77.33%
   批次 700/782: 損失=0.8587, 準確率=77.31%
   批次 750/782: 損失=0.8589, 準確率=77.32%
   批次 782/782: 損失=0.8579, 準確率=77.37%
Epoch 16 完成: 訓練=77.37%, 驗證=75.29%, 時間=124.2s
   💾 新最佳模型已保存: 驗證準確率 75.29%

Epoch 17/100, LR: 0.002815
   批次  50/782: 損失=0.8174, 準確率=79.06%
   批次 100/782: 損失=0.8319, 準確率=78.48%
   批次 150/782: 損失=0.8311, 準確率=78.55%
   批次 200/782: 損失=0.8317, 準確率=78.38%
   批次 250/782: 損失=0.8373, 準確率=78.12%
   批次 300/782: 損失=0.8340, 準確率=78.25%
   批次 350/782: 損失=0.8382, 準確率=78.08%
   批次 400/782: 損失=0.8404, 準確率=77.97%
   批次 450/782: 損失=0.8419, 準確率=77.92%
   批次 500/782: 損失=0.8425, 準確率=77.89%
   批次 550/782: 損失=0.8426, 準確率=77.89%
   批次 600/782: 損失=0.8425, 準確率=77.93%
   批次 650/782: 損失=0.8439, 準確率=77.88%
   批次 700/782: 損失=0.8441, 準確率=77.83%
   批次 750/782: 損失=0.8446, 準確率=77.83%
   批次 782/782: 損失=0.8442, 準確率=77.88%
Epoch 17 完成: 訓練=77.88%, 驗證=74.19%, 時間=121.3s

Epoch 18/100, LR: 0.002792
   批次  50/782: 損失=0.8177, 準確率=79.22%
   批次 100/782: 損失=0.8167, 準確率=78.80%
   批次 150/782: 損失=0.8268, 準確率=78.59%
   批次 200/782: 損失=0.8267, 準確率=78.63%
   批次 250/782: 損失=0.8304, 準確率=78.54%
   批次 300/782: 損失=0.8293, 準確率=78.67%
   批次 350/782: 損失=0.8277, 準確率=78.72%
   批次 400/782: 損失=0.8292, 準確率=78.75%
   批次 450/782: 損失=0.8294, 準確率=78.71%
   批次 500/782: 損失=0.8308, 準確率=78.66%
   批次 550/782: 損失=0.8315, 準確率=78.62%
   批次 600/782: 損失=0.8318, 準確率=78.59%
   批次 650/782: 損失=0.8332, 準確率=78.50%
   批次 700/782: 損失=0.8338, 準確率=78.50%
   批次 750/782: 損失=0.8352, 準確率=78.42%
   批次 782/782: 損失=0.8364, 準確率=78.38%
Epoch 18 完成: 訓練=78.38%, 驗證=74.52%, 時間=119.6s

Epoch 19/100, LR: 0.002767
   批次  50/782: 損失=0.8023, 準確率=79.72%
   批次 100/782: 損失=0.8207, 準確率=79.19%
   批次 150/782: 損失=0.8098, 準確率=79.64%
   批次 200/782: 損失=0.8100, 準確率=79.47%
   批次 250/782: 損失=0.8111, 準確率=79.34%
   批次 300/782: 損失=0.8101, 準確率=79.47%
   批次 350/782: 損失=0.8118, 準確率=79.35%
   批次 400/782: 損失=0.8154, 準確率=79.19%
   批次 450/782: 損失=0.8181, 準確率=79.09%
   批次 500/782: 損失=0.8207, 準確率=78.97%
   批次 550/782: 損失=0.8225, 準確率=78.92%
   批次 600/782: 損失=0.8217, 準確率=78.96%
   批次 650/782: 損失=0.8229, 準確率=78.94%
   批次 700/782: 損失=0.8229, 準確率=78.99%
   批次 750/782: 損失=0.8230, 準確率=78.99%
   批次 782/782: 損失=0.8246, 準確率=78.94%
Epoch 19 完成: 訓練=78.94%, 驗證=75.01%, 時間=120.5s

Epoch 20/100, LR: 0.002741
   批次  50/782: 損失=0.7888, 準確率=80.38%
   批次 100/782: 損失=0.7925, 準確率=80.03%
   批次 150/782: 損失=0.8036, 準確率=79.68%
   批次 200/782: 損失=0.8032, 準確率=79.69%
   批次 250/782: 損失=0.8055, 準確率=79.62%
   批次 300/782: 損失=0.8064, 準確率=79.58%
   批次 350/782: 損失=0.8067, 準確率=79.56%
   批次 400/782: 損失=0.8056, 準確率=79.59%
   批次 450/782: 損失=0.8070, 準確率=79.52%
   批次 500/782: 損失=0.8077, 準確率=79.46%
   批次 550/782: 損失=0.8098, 準確率=79.39%
   批次 600/782: 損失=0.8122, 準確率=79.34%
   批次 650/782: 損失=0.8146, 準確率=79.22%
   批次 700/782: 損失=0.8152, 準確率=79.21%
   批次 750/782: 損失=0.8150, 準確率=79.17%
   批次 782/782: 損失=0.8152, 準確率=79.10%
Epoch 20 完成: 訓練=79.10%, 驗證=75.04%, 時間=121.5s

Epoch 21/100, LR: 0.002714
   批次  50/782: 損失=0.7995, 準確率=80.06%
   批次 100/782: 損失=0.7907, 準確率=80.20%
   批次 150/782: 損失=0.7877, 準確率=80.43%
   批次 200/782: 損失=0.7898, 準確率=80.31%
   批次 250/782: 損失=0.7887, 準確率=80.53%
   批次 300/782: 損失=0.7928, 準確率=80.23%
   批次 350/782: 損失=0.7951, 準確率=80.18%
   批次 400/782: 損失=0.7959, 準確率=80.17%
   批次 450/782: 損失=0.7971, 準確率=80.15%
   批次 500/782: 損失=0.7988, 準確率=80.03%
   批次 550/782: 損失=0.7979, 準確率=80.10%
   批次 600/782: 損失=0.8001, 準確率=80.02%
   批次 650/782: 損失=0.8010, 準確率=79.94%
   批次 700/782: 損失=0.8029, 準確率=79.84%
   批次 750/782: 損失=0.8025, 準確率=79.89%
   批次 782/782: 損失=0.8030, 準確率=79.89%
Epoch 21 完成: 訓練=79.89%, 驗證=75.64%, 時間=119.7s
   💾 新最佳模型已保存: 驗證準確率 75.64%

Epoch 22/100, LR: 0.002686
   批次  50/782: 損失=0.7767, 準確率=80.03%
   批次 100/782: 損失=0.7665, 準確率=80.94%
   批次 150/782: 損失=0.7722, 準確率=80.84%
   批次 200/782: 損失=0.7769, 準確率=80.78%
   批次 250/782: 損失=0.7774, 準確率=80.76%
   批次 300/782: 損失=0.7816, 準確率=80.53%
   批次 350/782: 損失=0.7870, 準確率=80.31%
   批次 400/782: 損失=0.7843, 準確率=80.53%
   批次 450/782: 損失=0.7874, 準確率=80.50%
   批次 500/782: 損失=0.7872, 準確率=80.55%
   批次 550/782: 損失=0.7891, 準確率=80.47%
   批次 600/782: 損失=0.7900, 準確率=80.41%
   批次 650/782: 損失=0.7919, 準確率=80.29%
   批次 700/782: 損失=0.7927, 準確率=80.23%
   批次 750/782: 損失=0.7923, 準確率=80.28%
   批次 782/782: 損失=0.7924, 準確率=80.25%
Epoch 22 完成: 訓練=80.25%, 驗證=74.95%, 時間=125.4s
   📊 訓練-驗證差異: 5.30%

Epoch 23/100, LR: 0.002657
   批次  50/782: 損失=0.7544, 準確率=81.69%
   批次 100/782: 損失=0.7631, 準確率=81.34%
   批次 150/782: 損失=0.7649, 準確率=81.22%
   批次 200/782: 損失=0.7712, 準確率=80.96%
   批次 250/782: 損失=0.7713, 準確率=80.88%
   批次 300/782: 損失=0.7732, 準確率=80.76%
   批次 350/782: 損失=0.7791, 準確率=80.54%
   批次 400/782: 損失=0.7825, 準確率=80.48%
   批次 450/782: 損失=0.7838, 準確率=80.39%
   批次 500/782: 損失=0.7836, 準確率=80.47%
   批次 550/782: 損失=0.7829, 準確率=80.52%
   批次 600/782: 損失=0.7855, 準確率=80.43%
   批次 650/782: 損失=0.7854, 準確率=80.42%
   批次 700/782: 損失=0.7846, 準確率=80.47%
   批次 750/782: 損失=0.7851, 準確率=80.42%
   批次 782/782: 損失=0.7838, 準確率=80.51%
Epoch 23 完成: 訓練=80.51%, 驗證=75.86%, 時間=123.8s
   💾 新最佳模型已保存: 驗證準確率 75.86%

Epoch 24/100, LR: 0.002626
   批次  50/782: 損失=0.7655, 準確率=81.28%
   批次 100/782: 損失=0.7782, 準確率=80.67%
   批次 150/782: 損失=0.7694, 準確率=81.07%
   批次 200/782: 損失=0.7657, 準確率=81.28%
   批次 250/782: 損失=0.7677, 準確率=81.17%
   批次 300/782: 損失=0.7707, 準確率=81.06%
   批次 350/782: 損失=0.7708, 準確率=80.92%
   批次 400/782: 損失=0.7699, 準確率=81.03%
   批次 450/782: 損失=0.7691, 準確率=81.08%
   批次 500/782: 損失=0.7690, 準確率=81.12%
   批次 550/782: 損失=0.7711, 準確率=81.03%
   批次 600/782: 損失=0.7737, 準確率=80.94%
   批次 650/782: 損失=0.7761, 準確率=80.84%
   批次 700/782: 損失=0.7769, 準確率=80.77%
   批次 750/782: 損失=0.7768, 準確率=80.80%
   批次 782/782: 損失=0.7778, 準確率=80.76%
Epoch 24 完成: 訓練=80.76%, 驗證=76.06%, 時間=125.9s
   💾 新最佳模型已保存: 驗證準確率 76.06%

Epoch 25/100, LR: 0.002595
   批次  50/782: 損失=0.7614, 準確率=82.06%
   批次 100/782: 損失=0.7647, 準確率=81.92%
   批次 150/782: 損失=0.7604, 準確率=81.79%
   批次 200/782: 損失=0.7622, 準確率=81.60%
   批次 250/782: 損失=0.7631, 準確率=81.46%
   批次 300/782: 損失=0.7626, 準確率=81.46%
   批次 350/782: 損失=0.7629, 準確率=81.41%
   批次 400/782: 損失=0.7596, 準確率=81.45%
   批次 450/782: 損失=0.7618, 準確率=81.40%
   批次 500/782: 損失=0.7619, 準確率=81.47%
   批次 550/782: 損失=0.7631, 準確率=81.42%
   批次 600/782: 損失=0.7635, 準確率=81.40%
   批次 650/782: 損失=0.7630, 準確率=81.41%
   批次 700/782: 損失=0.7627, 準確率=81.41%
   批次 750/782: 損失=0.7632, 準確率=81.38%
   批次 782/782: 損失=0.7638, 準確率=81.33%
Epoch 25 完成: 訓練=81.33%, 驗證=76.39%, 時間=120.3s
   💾 新最佳模型已保存: 驗證準確率 76.39%

Epoch 26/100, LR: 0.002562
   批次  50/782: 損失=0.7500, 準確率=82.59%
   批次 100/782: 損失=0.7531, 準確率=81.72%
   批次 150/782: 損失=0.7540, 準確率=81.64%
   批次 200/782: 損失=0.7480, 準確率=81.95%
   批次 250/782: 損失=0.7496, 準確率=81.89%
   批次 300/782: 損失=0.7510, 準確率=81.86%
   批次 350/782: 損失=0.7504, 準確率=81.84%
   批次 400/782: 損失=0.7524, 準確率=81.79%
   批次 450/782: 損失=0.7531, 準確率=81.72%
   批次 500/782: 損失=0.7549, 準確率=81.68%
   批次 550/782: 損失=0.7568, 準確率=81.62%
   批次 600/782: 損失=0.7577, 準確率=81.58%
   批次 650/782: 損失=0.7597, 準確率=81.54%
   批次 700/782: 損失=0.7584, 準確率=81.61%
   批次 750/782: 損失=0.7585, 準確率=81.56%
   批次 782/782: 損失=0.7599, 準確率=81.53%
Epoch 26 完成: 訓練=81.53%, 驗證=76.14%, 時間=122.7s
   📊 訓練-驗證差異: 5.39%

Epoch 27/100, LR: 0.002528
   批次  50/782: 損失=0.7231, 準確率=83.81%
   批次 100/782: 損失=0.7325, 準確率=83.23%
   批次 150/782: 損失=0.7341, 準確率=83.08%
   批次 200/782: 損失=0.7386, 準確率=82.79%
   批次 250/782: 損失=0.7331, 準確率=83.08%
   批次 300/782: 損失=0.7377, 準確率=82.76%
   批次 350/782: 損失=0.7374, 準確率=82.82%
   批次 400/782: 損失=0.7404, 準確率=82.61%
   批次 450/782: 損失=0.7433, 準確率=82.41%
   批次 500/782: 損失=0.7441, 準確率=82.36%
   批次 550/782: 損失=0.7446, 準確率=82.32%
   批次 600/782: 損失=0.7458, 準確率=82.23%
   批次 650/782: 損失=0.7467, 準確率=82.18%
   批次 700/782: 損失=0.7481, 準確率=82.08%
   批次 750/782: 損失=0.7488, 準確率=82.10%
   批次 782/782: 損失=0.7500, 準確率=82.08%
Epoch 27 完成: 訓練=82.08%, 驗證=76.09%, 時間=122.5s
   📊 訓練-驗證差異: 5.99%

Epoch 28/100, LR: 0.002494
   批次  50/782: 損失=0.7318, 準確率=82.91%
   批次 100/782: 損失=0.7193, 準確率=83.22%
   批次 150/782: 損失=0.7292, 準確率=82.77%
   批次 200/782: 損失=0.7231, 準確率=83.04%
   批次 250/782: 損失=0.7244, 準確率=83.07%
   批次 300/782: 損失=0.7255, 準確率=83.05%
   批次 350/782: 損失=0.7269, 準確率=83.02%
   批次 400/782: 損失=0.7286, 準確率=82.91%
   批次 450/782: 損失=0.7303, 準確率=82.81%
   批次 500/782: 損失=0.7331, 準確率=82.72%
   批次 550/782: 損失=0.7347, 準確率=82.62%
   批次 600/782: 損失=0.7363, 準確率=82.55%
   批次 650/782: 損失=0.7378, 準確率=82.47%
   批次 700/782: 損失=0.7385, 準確率=82.43%
   批次 750/782: 損失=0.7400, 準確率=82.37%
   批次 782/782: 損失=0.7404, 準確率=82.35%
Epoch 28 完成: 訓練=82.35%, 驗證=77.09%, 時間=120.2s
   📊 訓練-驗證差異: 5.26%
   💾 新最佳模型已保存: 驗證準確率 77.09%

Epoch 29/100, LR: 0.002458
   批次  50/782: 損失=0.6789, 準確率=85.38%
   批次 100/782: 損失=0.6911, 準確率=84.92%
   批次 150/782: 損失=0.7049, 準確率=84.16%
   批次 200/782: 損失=0.7053, 準確率=84.12%
   批次 250/782: 損失=0.7057, 準確率=84.04%
   批次 300/782: 損失=0.7116, 準確率=83.78%
   批次 350/782: 損失=0.7189, 準確率=83.49%
   批次 400/782: 損失=0.7237, 準確率=83.30%
   批次 450/782: 損失=0.7237, 準確率=83.36%
   批次 500/782: 損失=0.7257, 準確率=83.23%
   批次 550/782: 損失=0.7278, 準確率=83.16%
   批次 600/782: 損失=0.7315, 準確率=82.97%
   批次 650/782: 損失=0.7342, 準確率=82.79%
   批次 700/782: 損失=0.7346, 準確率=82.81%
   批次 750/782: 損失=0.7349, 準確率=82.79%
   批次 782/782: 損失=0.7352, 準確率=82.75%
Epoch 29 完成: 訓練=82.75%, 驗證=76.51%, 時間=119.6s
   📊 訓練-驗證差異: 6.24%

Epoch 30/100, LR: 0.002421
   批次  50/782: 損失=0.6934, 準確率=84.41%
   批次 100/782: 損失=0.6995, 準確率=84.14%
   批次 150/782: 損失=0.7116, 準確率=83.58%
   批次 200/782: 損失=0.7145, 準確率=83.63%
   批次 250/782: 損失=0.7188, 準確率=83.40%
   批次 300/782: 損失=0.7209, 準確率=83.31%
   批次 350/782: 損失=0.7241, 準確率=83.11%
   批次 400/782: 損失=0.7236, 準確率=83.08%
   批次 450/782: 損失=0.7217, 準確率=83.19%
   批次 500/782: 損失=0.7255, 準確率=83.02%
   批次 550/782: 損失=0.7247, 準確率=83.09%
   批次 600/782: 損失=0.7245, 準確率=83.16%
   批次 650/782: 損失=0.7244, 準確率=83.11%
   批次 700/782: 損失=0.7254, 準確率=83.04%
   批次 750/782: 損失=0.7265, 準確率=83.02%
   批次 782/782: 損失=0.7267, 準確率=83.02%
Epoch 30 完成: 訓練=83.02%, 驗證=76.65%, 時間=121.7s
   📊 訓練-驗證差異: 6.37%

Epoch 31/100, LR: 0.002384
   批次  50/782: 損失=0.7028, 準確率=83.78%
   批次 100/782: 損失=0.7021, 準確率=84.03%
   批次 150/782: 損失=0.7032, 準確率=84.04%
   批次 200/782: 損失=0.7056, 準確率=83.85%
   批次 250/782: 損失=0.7128, 準確率=83.42%
   批次 300/782: 損失=0.7130, 準確率=83.51%
   批次 350/782: 損失=0.7129, 準確率=83.50%
   批次 400/782: 損失=0.7098, 準確率=83.75%
   批次 450/782: 損失=0.7123, 準確率=83.56%
   批次 500/782: 損失=0.7121, 準確率=83.62%
   批次 550/782: 損失=0.7152, 準確率=83.48%
   批次 600/782: 損失=0.7142, 準確率=83.59%
   批次 650/782: 損失=0.7159, 準確率=83.51%
   批次 700/782: 損失=0.7168, 準確率=83.46%
   批次 750/782: 損失=0.7206, 準確率=83.33%
   批次 782/782: 損失=0.7230, 準確率=83.23%
Epoch 31 完成: 訓練=83.23%, 驗證=77.07%, 時間=120.3s
   📊 訓練-驗證差異: 6.16%

Epoch 32/100, LR: 0.002345
   批次  50/782: 損失=0.6741, 準確率=85.16%
   批次 100/782: 損失=0.6861, 準確率=84.70%
   批次 150/782: 損失=0.6950, 準確率=84.25%
   批次 200/782: 損失=0.7001, 準確率=84.05%
   批次 250/782: 損失=0.7027, 準確率=84.02%
   批次 300/782: 損失=0.7046, 準確率=83.95%
   批次 350/782: 損失=0.7061, 準確率=83.88%
   批次 400/782: 損失=0.7045, 準確率=83.98%
   批次 450/782: 損失=0.7039, 準確率=84.04%
   批次 500/782: 損失=0.7070, 準確率=83.90%
   批次 550/782: 損失=0.7081, 準確率=83.87%
   批次 600/782: 損失=0.7093, 準確率=83.81%
   批次 650/782: 損失=0.7106, 準確率=83.76%
   批次 700/782: 損失=0.7111, 準確率=83.74%
   批次 750/782: 損失=0.7109, 準確率=83.75%
   批次 782/782: 損失=0.7114, 準確率=83.70%
Epoch 32 完成: 訓練=83.70%, 驗證=77.18%, 時間=119.4s
   📊 訓練-驗證差異: 6.52%
   💾 新最佳模型已保存: 驗證準確率 77.18%

Epoch 33/100, LR: 0.002306
   批次  50/782: 損失=0.6851, 準確率=84.59%
   批次 100/782: 損失=0.6916, 準確率=84.27%
   批次 150/782: 損失=0.6938, 準確率=84.04%
   批次 200/782: 損失=0.6925, 準確率=84.23%
   批次 250/782: 損失=0.6936, 準確率=84.24%
   批次 300/782: 損失=0.6915, 準確率=84.42%
   批次 350/782: 損失=0.6966, 準確率=84.23%
   批次 400/782: 損失=0.6971, 準確率=84.20%
   批次 450/782: 損失=0.6983, 準確率=84.12%
   批次 500/782: 損失=0.6984, 準確率=84.14%
   批次 550/782: 損失=0.7011, 準確率=83.99%
   批次 600/782: 損失=0.7031, 準確率=83.93%
   批次 650/782: 損失=0.7027, 準確率=83.95%
   批次 700/782: 損失=0.7040, 準確率=83.86%
   批次 750/782: 損失=0.7053, 準確率=83.84%
   批次 782/782: 損失=0.7054, 準確率=83.84%
Epoch 33 完成: 訓練=83.84%, 驗證=77.52%, 時間=120.7s
   📊 訓練-驗證差異: 6.32%
   💾 新最佳模型已保存: 驗證準確率 77.52%

Epoch 34/100, LR: 0.002266
   批次  50/782: 損失=0.6712, 準確率=85.09%
   批次 100/782: 損失=0.6751, 準確率=85.36%
   批次 150/782: 損失=0.6758, 準確率=85.05%
   批次 200/782: 損失=0.6732, 準確率=85.16%
   批次 250/782: 損失=0.6753, 準確率=85.08%
   批次 300/782: 損失=0.6780, 準確率=85.05%
   批次 350/782: 損失=0.6834, 準確率=84.78%
   批次 400/782: 損失=0.6844, 準確率=84.75%
   批次 450/782: 損失=0.6873, 準確率=84.61%
   批次 500/782: 損失=0.6886, 準確率=84.60%
   批次 550/782: 損失=0.6899, 準確率=84.54%
   批次 600/782: 損失=0.6906, 準確率=84.49%
   批次 650/782: 損失=0.6912, 準確率=84.47%
   批次 700/782: 損失=0.6915, 準確率=84.48%
   批次 750/782: 損失=0.6936, 準確率=84.37%
   批次 782/782: 損失=0.6951, 準確率=84.32%
Epoch 34 完成: 訓練=84.32%, 驗證=76.32%, 時間=117.9s
   📊 訓練-驗證差異: 8.00%

Epoch 35/100, LR: 0.002225
   批次  50/782: 損失=0.6575, 準確率=86.69%
   批次 100/782: 損失=0.6610, 準確率=86.19%
   批次 150/782: 損失=0.6700, 準確率=85.59%
   批次 200/782: 損失=0.6744, 準確率=85.40%
   批次 250/782: 損失=0.6760, 準確率=85.22%
   批次 300/782: 損失=0.6784, 準確率=85.09%
   批次 350/782: 損失=0.6800, 準確率=85.02%
   批次 400/782: 損失=0.6816, 準確率=85.02%
   批次 450/782: 損失=0.6823, 準確率=84.93%
   批次 500/782: 損失=0.6834, 準確率=84.83%
   批次 550/782: 損失=0.6847, 準確率=84.73%
   批次 600/782: 損失=0.6856, 準確率=84.64%
   批次 650/782: 損失=0.6869, 準確率=84.60%
   批次 700/782: 損失=0.6892, 準確率=84.49%
   批次 750/782: 損失=0.6896, 準確率=84.49%
   批次 782/782: 損失=0.6905, 準確率=84.42%
Epoch 35 完成: 訓練=84.42%, 驗證=76.88%, 時間=120.0s
   📊 訓練-驗證差異: 7.54%

Epoch 36/100, LR: 0.002184
   批次  50/782: 損失=0.6558, 準確率=86.62%
   批次 100/782: 損失=0.6614, 準確率=86.05%
   批次 150/782: 損失=0.6667, 準確率=85.82%
   批次 200/782: 損失=0.6675, 準確率=85.75%
   批次 250/782: 損失=0.6648, 準確率=85.79%
   批次 300/782: 損失=0.6710, 準確率=85.54%
   批次 350/782: 損失=0.6758, 準確率=85.35%
   批次 400/782: 損失=0.6765, 準確率=85.26%
   批次 450/782: 損失=0.6771, 準確率=85.22%
   批次 500/782: 損失=0.6773, 準確率=85.20%
   批次 550/782: 損失=0.6807, 準確率=85.10%
   批次 600/782: 損失=0.6813, 準確率=85.04%
   批次 650/782: 損失=0.6826, 準確率=85.00%
   批次 700/782: 損失=0.6828, 準確率=85.01%
   批次 750/782: 損失=0.6846, 準確率=84.86%
   批次 782/782: 損失=0.6840, 準確率=84.87%
Epoch 36 完成: 訓練=84.87%, 驗證=76.61%, 時間=119.5s
   ⚠️  過擬合警告: 差異 8.26% > 閾值 8.0% (1/6)
   📊 訓練-驗證差異: 8.26%

Epoch 37/100, LR: 0.002142
   批次  50/782: 損失=0.6788, 準確率=85.09%
   批次 100/782: 損失=0.6639, 準確率=85.47%
   批次 150/782: 損失=0.6675, 準確率=85.33%
   批次 200/782: 損失=0.6694, 準確率=85.22%
   批次 250/782: 損失=0.6669, 準確率=85.28%
   批次 300/782: 損失=0.6693, 準確率=85.22%
   批次 350/782: 損失=0.6709, 準確率=85.20%
   批次 400/782: 損失=0.6716, 準確率=85.17%
   批次 450/782: 損失=0.6734, 準確率=85.18%
   批次 500/782: 損失=0.6746, 準確率=85.20%
   批次 550/782: 損失=0.6760, 準確率=85.16%
   批次 600/782: 損失=0.6760, 準確率=85.13%
   批次 650/782: 損失=0.6765, 準確率=85.14%
   批次 700/782: 損失=0.6774, 準確率=85.11%
   批次 750/782: 損失=0.6784, 準確率=85.08%
   批次 782/782: 損失=0.6778, 準確率=85.10%
Epoch 37 完成: 訓練=85.10%, 驗證=77.30%, 時間=127.4s
   📊 訓練-驗證差異: 7.80%

Epoch 38/100, LR: 0.002099
   批次  50/782: 損失=0.6506, 準確率=86.06%
   批次 100/782: 損失=0.6440, 準確率=86.53%
   批次 150/782: 損失=0.6519, 準確率=86.23%
   批次 200/782: 損失=0.6503, 準確率=86.40%
   批次 250/782: 損失=0.6556, 準確率=86.14%
   批次 300/782: 損失=0.6571, 準確率=85.96%
   批次 350/782: 損失=0.6581, 準確率=85.89%
   批次 400/782: 損失=0.6591, 準確率=85.81%
   批次 450/782: 損失=0.6608, 準確率=85.71%
   批次 500/782: 損失=0.6635, 準確率=85.59%
   批次 550/782: 損失=0.6655, 準確率=85.48%
   批次 600/782: 損失=0.6669, 準確率=85.46%
   批次 650/782: 損失=0.6688, 準確率=85.42%
   批次 700/782: 損失=0.6693, 準確率=85.37%
   批次 750/782: 損失=0.6694, 準確率=85.35%
   批次 782/782: 損失=0.6702, 準確率=85.36%
Epoch 38 完成: 訓練=85.36%, 驗證=77.31%, 時間=121.9s
   ⚠️  過擬合警告: 差異 8.05% > 閾值 8.0% (1/6)
   📊 訓練-驗證差異: 8.05%

Epoch 39/100, LR: 0.002055
   批次  50/782: 損失=0.6492, 準確率=86.56%
   批次 100/782: 損失=0.6449, 準確率=86.69%
   批次 150/782: 損失=0.6443, 準確率=86.85%
   批次 200/782: 損失=0.6450, 準確率=86.77%
   批次 250/782: 損失=0.6436, 準確率=86.78%
   批次 300/782: 損失=0.6453, 準確率=86.65%
   批次 350/782: 損失=0.6487, 準確率=86.42%
   批次 400/782: 損失=0.6527, 準確率=86.25%
   批次 450/782: 損失=0.6548, 準確率=86.14%
   批次 500/782: 損失=0.6570, 準確率=85.98%
   批次 550/782: 損失=0.6573, 準確率=85.99%
   批次 600/782: 損失=0.6572, 準確率=85.99%
   批次 650/782: 損失=0.6584, 準確率=85.93%
   批次 700/782: 損失=0.6595, 準確率=85.93%
   批次 750/782: 損失=0.6603, 準確率=85.91%
   批次 782/782: 損失=0.6612, 準確率=85.85%
Epoch 39 完成: 訓練=85.85%, 驗證=77.41%, 時間=122.4s
   ⚠️  過擬合警告: 差異 8.44% > 閾值 8.0% (2/6)
   📊 訓練-驗證差異: 8.44%

Epoch 40/100, LR: 0.002011
   批次  50/782: 損失=0.6197, 準確率=87.78%
   批次 100/782: 損失=0.6270, 準確率=87.75%
   批次 150/782: 損失=0.6293, 準確率=87.74%
   批次 200/782: 損失=0.6306, 準確率=87.59%
   批次 250/782: 損失=0.6318, 準確率=87.34%
   批次 300/782: 損失=0.6340, 準確率=87.28%
   批次 350/782: 損失=0.6370, 準確率=87.06%
   批次 400/782: 損失=0.6394, 準確率=86.97%
   批次 450/782: 損失=0.6409, 準確率=86.91%
   批次 500/782: 損失=0.6417, 準確率=86.88%
   批次 550/782: 損失=0.6432, 準確率=86.74%
   批次 600/782: 損失=0.6459, 準確率=86.60%
   批次 650/782: 損失=0.6474, 準確率=86.51%
   批次 700/782: 損失=0.6489, 準確率=86.42%
   批次 750/782: 損失=0.6505, 準確率=86.35%
   批次 782/782: 損失=0.6514, 準確率=86.34%
Epoch 40 完成: 訓練=86.34%, 驗證=77.56%, 時間=116.5s
   ⚠️  過擬合警告: 差異 8.78% > 閾值 8.0% (3/6)
   📊 訓練-驗證差異: 8.78%
   💾 新最佳模型已保存: 驗證準確率 77.56%

Epoch 41/100, LR: 0.001967
   批次  50/782: 損失=0.6285, 準確率=87.56%
   批次 100/782: 損失=0.6162, 準確率=87.80%
   批次 150/782: 損失=0.6239, 準確率=87.33%
   批次 200/782: 損失=0.6282, 準確率=87.07%
   批次 250/782: 損失=0.6333, 準確率=86.94%
   批次 300/782: 損失=0.6341, 準確率=86.95%
   批次 350/782: 損失=0.6348, 準確率=86.90%
   批次 400/782: 損失=0.6373, 準確率=86.77%
   批次 450/782: 損失=0.6383, 準確率=86.75%
   批次 500/782: 損失=0.6389, 準確率=86.68%
   批次 550/782: 損失=0.6389, 準確率=86.68%
   批次 600/782: 損失=0.6423, 準確率=86.51%
   批次 650/782: 損失=0.6443, 準確率=86.42%
   批次 700/782: 損失=0.6451, 準確率=86.38%
   批次 750/782: 損失=0.6460, 準確率=86.36%
   批次 782/782: 損失=0.6472, 準確率=86.32%
Epoch 41 完成: 訓練=86.32%, 驗證=76.98%, 時間=120.8s
   ⚠️  過擬合警告: 差異 9.34% > 閾值 8.0% (4/6)
   📊 訓練-驗證差異: 9.34%

Epoch 42/100, LR: 0.001922
   批次  50/782: 損失=0.6473, 準確率=86.62%
   批次 100/782: 損失=0.6362, 準確率=86.92%
   批次 150/782: 損失=0.6363, 準確率=86.78%
   批次 200/782: 損失=0.6314, 準確率=87.13%
   批次 250/782: 損失=0.6307, 準確率=87.16%
   批次 300/782: 損失=0.6313, 準確率=87.09%
   批次 350/782: 損失=0.6343, 準確率=87.04%
   批次 400/782: 損失=0.6349, 準確率=87.01%
   批次 450/782: 損失=0.6368, 準確率=86.92%
   批次 500/782: 損失=0.6359, 準確率=86.93%
   批次 550/782: 損失=0.6387, 準確率=86.79%
   批次 600/782: 損失=0.6399, 準確率=86.72%
   批次 650/782: 損失=0.6400, 準確率=86.71%
   批次 700/782: 損失=0.6405, 準確率=86.69%
   批次 750/782: 損失=0.6401, 準確率=86.68%
   批次 782/782: 損失=0.6419, 準確率=86.61%
Epoch 42 完成: 訓練=86.61%, 驗證=77.21%, 時間=119.0s
   ⚠️  過擬合警告: 差異 9.40% > 閾值 8.0% (5/6)
   📊 訓練-驗證差異: 9.40%

Epoch 43/100, LR: 0.001877
   批次  50/782: 損失=0.6128, 準確率=87.56%
   批次 100/782: 損失=0.6224, 準確率=87.36%
   批次 150/782: 損失=0.6238, 準確率=87.27%
   批次 200/782: 損失=0.6326, 準確率=86.98%
   批次 250/782: 損失=0.6313, 準確率=87.09%
   批次 300/782: 損失=0.6294, 準確率=87.22%
   批次 350/782: 損失=0.6288, 準確率=87.25%
   批次 400/782: 損失=0.6288, 準確率=87.34%
   批次 450/782: 損失=0.6313, 準確率=87.24%
   批次 500/782: 損失=0.6309, 準確率=87.26%
   批次 550/782: 損失=0.6314, 準確率=87.18%
   批次 600/782: 損失=0.6316, 準確率=87.20%
   批次 650/782: 損失=0.6329, 準確率=87.11%
   批次 700/782: 損失=0.6323, 準確率=87.10%
   批次 750/782: 損失=0.6330, 準確率=87.08%
   批次 782/782: 損失=0.6334, 準確率=87.08%
Epoch 43 完成: 訓練=87.08%, 驗證=77.65%, 時間=119.7s
   ⚠️  過擬合警告: 差異 9.43% > 閾值 8.0% (6/6)
   🛑 過擬合早停: 連續 6 epochs 訓練-驗證差異超過 8.0%

⏱️ 超快速訓練時間統計:
   • 總訓練時間: 5256.8s (87.6min)
   • 實際訓練epochs: 43 / 100
   • 平均每epoch: 122.2s
   • 最佳驗證準確率: 77.56%
   • 早停原因: 過擬合檢測 (差異: 9.43%)
   • 已載入最佳模型權重

📈 繪製超快速訓練歷史...

📊 評估超縮小版模型...
   ✓ 整體準確率: 77.56%

🎉 超快速訓練完成!
   • 模型大小: Test
   • 最終準確率: 77.56%
   • 總訓練時間: 87.6 分鐘
   • 平均每epoch: 122.2 秒
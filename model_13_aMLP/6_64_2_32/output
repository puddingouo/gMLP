  超縮小版 aMLP 圖像分類訓練 - 注意力增強版本
🔥 附加注意力機制，預期準確率提升 2-5%
===========================================================================
🔥 附加注意力機制，預期準確率提升 2-5%
===========================================================================
===========================================================================

==========================================================================================
🏗️  可用的 aMLP 模型架構 (附加注意力機制)
==========================================================================================
編號   名稱     深度     維度     FFN   注意力      參數       時間         風險       描述

==========================================================================================
🏗️  可用的 aMLP 模型架構 (附加注意力機制)
==========================================================================================
編號   名稱     深度     維度     FFN   注意力      參數       時間         風險       描述
🏗️  可用的 aMLP 模型架構 (附加注意力機制)
==========================================================================================
編號   名稱     深度     維度     FFN   注意力      參數       時間         風險       描述
------------------------------------------------------------------------------------------
1    Test   4      64     2     32       0.18M    <40秒       極低       超極速測試模型 + 注意力
2    Nano   6      64     2     32       0.25M    ~1.5分鐘     很低       極小快速模型 + 注意力
編號   名稱     深度     維度     FFN   注意力      參數       時間         風險       描述
------------------------------------------------------------------------------------------
1    Test   4      64     2     32       0.18M    <40秒       極低       超極速測試模型 + 注意力
2    Nano   6      64     2     32       0.25M    ~1.5分鐘     很低       極小快速模型 + 注意力
------------------------------------------------------------------------------------------
1    Test   4      64     2     32       0.18M    <40秒       極低       超極速測試模型 + 注意力
2    Nano   6      64     2     32       0.25M    ~1.5分鐘     很低       極小快速模型 + 注意力
2    Nano   6      64     2     32       0.25M    ~1.5分鐘     很低       極小快速模型 + 注意力
3    XS     8      80     3     40       0.35M    ~2.5分鐘     低        超小平衡模型 + 注意力
4    S      12     128    3     64       0.85M    ~6分鐘       中等       小型性能模型 + 注意力
5    M      16     160    4     80       1.65M    ~12分鐘      較高       中型高性能模型 + 注意力
6    L      30     128    6     64       2.15M    ~18分鐘      很高       大型頂級模型 + 注意力
------------------------------------------------------------------------------------------
💡 推薦選擇 (aMLP 版本):
   🚀 快速測試: Test (1) 或 Nano (2) - 注意力增強
   ⚖️  平衡性能: XS (3) 或 S (4) - 最推薦，效能提升明顯
   🎯 高性能: M (5) 或 L (6) - 頂級注意力效能
   🔥 注意力機制將顯著提升準確率，建議從 XS 開始嘗試
==========================================================================================

🤖 請選擇要使用的 aMLP 模型:
   輸入編號 (1-6) 或模型名稱 (Test/Nano/XS/S/M/L): 2

✅ 您選擇了: aMLP-Nano 模型
   📋 模型詳情:
      • 深度: 6 層
      • 維度: 64
      • FFN倍數: 2
      • 注意力維度: 32 (🔥 性能提升關鍵)
      • 預估參數: 0.25M
      • 預估時間: ~1.5分鐘
      • 過擬合風險: 很低
      • 描述: 極小快速模型 + 注意力

   確認使用 aMLP-Nano 模型嗎? (y/n, 預設=y): y

============================================================
⚙️  aMLP 訓練參數設置
============================================================

📦 數據集模式選擇:
   1. 快速模式 (50K訓練 + 10K測試) - 推薦
   2. 完整模式 (50K訓練 + 10K測試)
   選擇模式 (1/2, 預設=1): 2

🏋️  訓練輪數 (預設=50): 80

🛡️  過擬合保護:
   1. 啟用早停機制 (推薦)
   2. 關閉早停機制
   選擇 (1/2, 預設=1): 1

✅ aMLP 訓練參數確認:
   📦 數據模式: 完整模式
   🏋️  訓練輪數: 80
   🛡️  早停機制: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
📦 加載超快速 CIFAR-10 數據集...
   ✓ 訓練樣本: 50000
   ✓ 測試樣本: 10000
   ✓ Batch大小: 128

🏗️ 創建超縮小版 aMLP-Nano 模型...
   ⚡ CPU模式：已設置4個線程

✅ 超縮小版 aMLP-Nano 模型創建完成
   ✓ 設備: cpu
   ✓ 實際參數數量: 154,826 (0.15M)
   ✓ 目標參數預期: 0.25M
   ✓ 注意力維度: 32 (🔥 aMLP 增強)
   ✓ 架構配置: depth=6, dim=64, ff_mult=2
   🔥 注意力機制已啟用，預期準確率提升 2-5%

🎬 開始訓練 aMLP-Nano 模型...
🔥 注意力機制已啟用，預期更佳訓練效果...

🏋️ 開始超快速 aMLP 訓練 (80 個 epochs)...
   🛡️  啟用過擬合早停保護
   🔥 注意力機制已啟用 - 預期更佳收斂效果

Epoch 1/80, LR: 0.002800
   批次  50/391: 損失=2.0118, 準確率=27.70%
   批次 100/391: 損失=1.8917, 準確率=33.62%
   批次 150/391: 損失=1.8169, 準確率=37.49%
   批次 200/391: 損失=1.7699, 準確率=39.45%
   批次 250/391: 損失=1.7251, 準確率=41.58%
   批次 300/391: 損失=1.6930, 準確率=43.06%
   批次 350/391: 損失=1.6672, 準確率=44.17%
   批次 391/391: 損失=1.6485, 準確率=45.03%
Epoch 1 完成: 訓練=45.03%, 驗證=52.84%, 時間=121.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 52.84%

Epoch 2/80, LR: 0.002799
   批次  50/391: 損失=1.4412, 準確率=54.34%
   批次 100/391: 損失=1.4449, 準確率=54.29%
   批次 150/391: 損失=1.4383, 準確率=54.68%
   批次 200/391: 損失=1.4289, 準確率=55.08%
   批次 250/391: 損失=1.4211, 準確率=55.54%
   批次 300/391: 損失=1.4124, 準確率=56.04%
   批次 350/391: 損失=1.3998, 準確率=56.68%
   批次 391/391: 損失=1.3944, 準確率=56.96%
Epoch 2 完成: 訓練=56.96%, 驗證=60.02%, 時間=113.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 60.02%

Epoch 3/80, LR: 0.002796
   批次  50/391: 損失=1.3198, 準確率=60.52%
   批次 100/391: 損失=1.3109, 準確率=60.77%
   批次 150/391: 損失=1.3071, 準確率=60.89%
   批次 200/391: 損失=1.3012, 準確率=61.34%
   批次 250/391: 損失=1.2958, 準確率=61.53%
   批次 300/391: 損失=1.2930, 準確率=61.67%
   批次 350/391: 損失=1.2902, 準確率=61.79%
   批次 391/391: 損失=1.2864, 準確率=62.04%
Epoch 3 完成: 訓練=62.04%, 驗證=63.95%, 時間=118.7s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 63.95%

Epoch 4/80, LR: 0.002790
   批次  50/391: 損失=1.2252, 準確率=65.27%
   批次 100/391: 損失=1.2263, 準確率=65.04%
   批次 150/391: 損失=1.2297, 準確率=64.91%
   批次 200/391: 損失=1.2299, 準確率=64.91%
   批次 250/391: 損失=1.2305, 準確率=64.91%
   批次 300/391: 損失=1.2254, 準確率=65.10%
   批次 350/391: 損失=1.2190, 準確率=65.40%
   批次 391/391: 損失=1.2181, 準確率=65.47%
Epoch 4 完成: 訓練=65.47%, 驗證=66.12%, 時間=114.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 66.12%

Epoch 5/80, LR: 0.002783
   批次  50/391: 損失=1.1569, 準確率=67.77%
   批次 100/391: 損失=1.1672, 準確率=67.18%
   批次 150/391: 損失=1.1744, 準確率=67.09%
   批次 200/391: 損失=1.1762, 準確率=67.09%
   批次 250/391: 損失=1.1743, 準確率=67.28%
   批次 300/391: 損失=1.1722, 準確率=67.42%
   批次 350/391: 損失=1.1676, 準確率=67.59%
   批次 391/391: 損失=1.1687, 準確率=67.53%
Epoch 5 完成: 訓練=67.53%, 驗證=68.38%, 時間=113.4s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 68.38%

Epoch 6/80, LR: 0.002773
   批次  50/391: 損失=1.1361, 準確率=68.83%
   批次 100/391: 損失=1.1355, 準確率=68.91%
   批次 150/391: 損失=1.1404, 準確率=68.61%
   批次 200/391: 損失=1.1360, 準確率=68.89%
   批次 250/391: 損失=1.1330, 準確率=69.05%
   批次 300/391: 損失=1.1309, 準確率=69.02%
   批次 350/391: 損失=1.1280, 準確率=69.21%
   批次 391/391: 損失=1.1275, 準確率=69.19%
Epoch 6 完成: 訓練=69.19%, 驗證=68.50%, 時間=113.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 68.50%

Epoch 7/80, LR: 0.002761
   批次  50/391: 損失=1.0875, 準確率=71.52%
   批次 100/391: 損失=1.0881, 準確率=71.44%
   批次 150/391: 損失=1.0840, 準確率=71.67%
   批次 200/391: 損失=1.0915, 準確率=71.20%
   批次 250/391: 損失=1.0917, 準確率=71.08%
   批次 300/391: 損失=1.0929, 準確率=71.08%
   批次 350/391: 損失=1.0901, 準確率=71.16%
   批次 391/391: 損失=1.0902, 準確率=71.18%
Epoch 7 完成: 訓練=71.18%, 驗證=71.18%, 時間=113.4s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 71.18%

Epoch 8/80, LR: 0.002748
   批次  50/391: 損失=1.0678, 準確率=72.50%
   批次 100/391: 損失=1.0511, 準確率=73.00%
   批次 150/391: 損失=1.0480, 準確率=73.03%
   批次 200/391: 損失=1.0554, 準確率=72.66%
   批次 250/391: 損失=1.0545, 準確率=72.71%
   批次 300/391: 損失=1.0514, 準確率=72.89%
   批次 350/391: 損失=1.0508, 準確率=72.85%
   批次 391/391: 損失=1.0509, 準確率=72.85%
Epoch 8 完成: 訓練=72.85%, 驗證=71.70%, 時間=114.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 71.70%

Epoch 9/80, LR: 0.002732
   批次  50/391: 損失=1.0135, 準確率=74.36%
   批次 100/391: 損失=1.0196, 準確率=74.34%
   批次 150/391: 損失=1.0230, 準確率=74.07%
   批次 200/391: 損失=1.0270, 準確率=73.92%
   批次 250/391: 損失=1.0245, 準確率=74.15%
   批次 300/391: 損失=1.0244, 準確率=74.15%
   批次 350/391: 損失=1.0245, 準確率=74.13%
   批次 391/391: 損失=1.0259, 準確率=74.12%
Epoch 9 完成: 訓練=74.12%, 驗證=73.55%, 時間=113.6s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 73.55%

Epoch 10/80, LR: 0.002714
   批次  50/391: 損失=0.9858, 準確率=75.42%
   批次 100/391: 損失=0.9893, 準確率=75.59%
   批次 150/391: 損失=0.9987, 準確率=75.19%
   批次 200/391: 損失=1.0011, 準確率=74.93%
   批次 250/391: 損失=1.0043, 準確率=74.78%
   批次 300/391: 損失=1.0020, 準確率=74.91%
   批次 350/391: 損失=1.0046, 準確率=74.86%
   批次 391/391: 損失=1.0035, 準確率=74.87%
Epoch 10 完成: 訓練=74.87%, 驗證=73.50%, 時間=114.4s

Epoch 11/80, LR: 0.002694
   批次  50/391: 損失=1.0056, 準確率=74.95%
   批次 100/391: 損失=0.9954, 準確率=75.38%
   批次 150/391: 損失=0.9935, 準確率=75.48%
   批次 200/391: 損失=0.9910, 準確率=75.58%
   批次 250/391: 損失=0.9891, 準確率=75.62%
   批次 300/391: 損失=0.9892, 準確率=75.60%
   批次 350/391: 損失=0.9860, 準確率=75.77%
   批次 391/391: 損失=0.9866, 準確率=75.73%
Epoch 11 完成: 訓練=75.73%, 驗證=74.28%, 時間=115.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 74.28%

Epoch 12/80, LR: 0.002672
   批次  50/391: 損失=0.9533, 準確率=76.59%
   批次 100/391: 損失=0.9501, 準確率=77.35%
   批次 150/391: 損失=0.9556, 準確率=77.10%
   批次 200/391: 損失=0.9603, 準確率=76.86%
   批次 250/391: 損失=0.9619, 準確率=76.82%
   批次 300/391: 損失=0.9611, 準確率=76.82%
   批次 350/391: 損失=0.9639, 準確率=76.74%
   批次 391/391: 損失=0.9643, 準確率=76.71%
Epoch 12 完成: 訓練=76.71%, 驗證=74.15%, 時間=113.7s

Epoch 13/80, LR: 0.002648
   批次  50/391: 損失=0.9391, 準確率=77.98%
   批次 100/391: 損失=0.9392, 準確率=77.77%
   批次 150/391: 損失=0.9399, 準確率=77.75%
   批次 200/391: 損失=0.9424, 準確率=77.54%
   批次 250/391: 損失=0.9456, 準確率=77.38%
   批次 300/391: 損失=0.9455, 準確率=77.39%
   批次 350/391: 損失=0.9453, 準確率=77.39%
   批次 391/391: 損失=0.9485, 準確率=77.21%
Epoch 13 完成: 訓練=77.21%, 驗證=74.74%, 時間=112.7s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 74.74%

Epoch 14/80, LR: 0.002622
   批次  50/391: 損失=0.9192, 準確率=79.39%
   批次 100/391: 損失=0.9326, 準確率=78.46%
   批次 150/391: 損失=0.9372, 準確率=78.12%
   批次 200/391: 損失=0.9384, 準確率=78.04%
   批次 250/391: 損失=0.9370, 準確率=78.12%
   批次 300/391: 損失=0.9353, 準確率=78.06%
   批次 350/391: 損失=0.9363, 準確率=77.98%
   批次 391/391: 損失=0.9363, 準確率=77.89%
Epoch 14 完成: 訓練=77.89%, 驗證=75.32%, 時間=114.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.32%

Epoch 15/80, LR: 0.002594
   批次  50/391: 損失=0.9145, 準確率=78.22%
   批次 100/391: 損失=0.9195, 準確率=78.33%
   批次 150/391: 損失=0.9179, 準確率=78.52%
   批次 200/391: 損失=0.9181, 準確率=78.50%
   批次 250/391: 損失=0.9194, 準確率=78.54%
   批次 300/391: 損失=0.9203, 準確率=78.60%
   批次 350/391: 損失=0.9204, 準確率=78.59%
   批次 391/391: 損失=0.9205, 準確率=78.60%
Epoch 15 完成: 訓練=78.60%, 驗證=75.77%, 時間=113.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.77%

Epoch 16/80, LR: 0.002565
   批次  50/391: 損失=0.8856, 準確率=79.86%
   批次 100/391: 損失=0.8842, 準確率=79.82%
   批次 150/391: 損失=0.8926, 準確率=79.62%
   批次 200/391: 損失=0.8963, 準確率=79.45%
   批次 250/391: 損失=0.9000, 準確率=79.39%
   批次 300/391: 損失=0.9015, 準確率=79.34%
   批次 350/391: 損失=0.9031, 準確率=79.36%
   批次 391/391: 損失=0.9061, 準確率=79.26%
Epoch 16 完成: 訓練=79.26%, 驗證=76.26%, 時間=114.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.26%

Epoch 17/80, LR: 0.002533
   批次  50/391: 損失=0.8837, 準確率=80.27%
   批次 100/391: 損失=0.8875, 準確率=79.90%
   批次 150/391: 損失=0.8922, 準確率=79.83%
   批次 200/391: 損失=0.8946, 準確率=79.71%
   批次 250/391: 損失=0.8960, 準確率=79.67%
   批次 300/391: 損失=0.8965, 準確率=79.70%
   批次 350/391: 損失=0.8977, 準確率=79.64%
   批次 391/391: 損失=0.9005, 準確率=79.48%
Epoch 17 完成: 訓練=79.48%, 驗證=75.73%, 時間=113.6s

Epoch 18/80, LR: 0.002500
   批次  50/391: 損失=0.8818, 準確率=80.72%
   批次 100/391: 損失=0.8783, 準確率=80.77%
   批次 150/391: 損失=0.8751, 準確率=80.80%
   批次 200/391: 損失=0.8756, 準確率=80.69%
   批次 250/391: 損失=0.8822, 準確率=80.32%
   批次 300/391: 損失=0.8830, 準確率=80.29%
   批次 350/391: 損失=0.8826, 準確率=80.30%
   批次 391/391: 損失=0.8837, 準確率=80.28%
Epoch 18 完成: 訓練=80.28%, 驗證=76.01%, 時間=112.8s

Epoch 19/80, LR: 0.002466
   批次  50/391: 損失=0.8586, 準確率=81.83%
   批次 100/391: 損失=0.8559, 準確率=81.61%
   批次 150/391: 損失=0.8635, 準確率=81.20%
   批次 200/391: 損失=0.8626, 準確率=81.22%
   批次 250/391: 損失=0.8616, 準確率=81.12%
   批次 300/391: 損失=0.8664, 準確率=80.95%
   批次 350/391: 損失=0.8698, 準確率=80.79%
   批次 391/391: 損失=0.8720, 準確率=80.75%
Epoch 19 完成: 訓練=80.75%, 驗證=77.54%, 時間=114.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.54%

Epoch 20/80, LR: 0.002429
   批次  50/391: 損失=0.8587, 準確率=81.34%
   批次 100/391: 損失=0.8527, 準確率=81.68%
   批次 150/391: 損失=0.8566, 準確率=81.39%
   批次 200/391: 損失=0.8564, 準確率=81.39%
   批次 250/391: 損失=0.8582, 準確率=81.26%
   批次 300/391: 損失=0.8597, 準確率=81.25%
   批次 350/391: 損失=0.8610, 準確率=81.15%
   批次 391/391: 損失=0.8625, 準確率=81.04%
Epoch 20 完成: 訓練=81.04%, 驗證=77.41%, 時間=112.0s

Epoch 21/80, LR: 0.002391
   批次  50/391: 損失=0.8361, 準確率=82.25%
   批次 100/391: 損失=0.8397, 準確率=82.31%
   批次 150/391: 損失=0.8423, 準確率=82.16%
   批次 200/391: 損失=0.8424, 準確率=82.14%
   批次 250/391: 損失=0.8453, 準確率=81.88%
   批次 300/391: 損失=0.8477, 準確率=81.72%
   批次 350/391: 損失=0.8484, 準確率=81.71%
   批次 391/391: 損失=0.8502, 準確率=81.61%
Epoch 21 完成: 訓練=81.61%, 驗證=77.13%, 時間=112.7s

Epoch 22/80, LR: 0.002352
   批次  50/391: 損失=0.8229, 準確率=82.67%
   批次 100/391: 損失=0.8278, 準確率=82.41%
   批次 150/391: 損失=0.8321, 準確率=82.36%
   批次 200/391: 損失=0.8356, 準確率=82.14%
   批次 250/391: 損失=0.8373, 準確率=82.17%
   批次 300/391: 損失=0.8407, 準確率=81.98%
   批次 350/391: 損失=0.8414, 準確率=81.93%
   批次 391/391: 損失=0.8425, 準確率=81.83%
Epoch 22 完成: 訓練=81.83%, 驗證=77.48%, 時間=112.5s

Epoch 23/80, LR: 0.002311
   批次  50/391: 損失=0.8079, 準確率=83.48%
   批次 100/391: 損失=0.8191, 準確率=83.23%
   批次 150/391: 損失=0.8222, 準確率=82.80%
   批次 200/391: 損失=0.8224, 準確率=82.86%
   批次 250/391: 損失=0.8246, 準確率=82.85%
   批次 300/391: 損失=0.8247, 準確率=82.76%
   批次 350/391: 損失=0.8286, 準確率=82.58%
   批次 391/391: 損失=0.8299, 準確率=82.53%
Epoch 23 完成: 訓練=82.53%, 驗證=78.21%, 時間=113.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.21%

Epoch 24/80, LR: 0.002268
   批次  50/391: 損失=0.8078, 準確率=83.33%
   批次 100/391: 損失=0.8154, 準確率=83.20%
   批次 150/391: 損失=0.8105, 準確率=83.49%
   批次 200/391: 損失=0.8199, 準確率=83.07%
   批次 250/391: 損失=0.8193, 準確率=83.07%
   批次 300/391: 損失=0.8191, 準確率=83.04%
   批次 350/391: 損失=0.8209, 準確率=82.93%
   批次 391/391: 損失=0.8215, 準確率=82.84%
Epoch 24 完成: 訓練=82.84%, 驗證=78.16%, 時間=112.8s

Epoch 25/80, LR: 0.002225
   批次  50/391: 損失=0.7947, 準確率=84.20%
   批次 100/391: 損失=0.7961, 準確率=84.21%
   批次 150/391: 損失=0.8003, 準確率=84.11%
   批次 200/391: 損失=0.8040, 準確率=83.82%
   批次 250/391: 損失=0.8062, 準確率=83.67%
   批次 300/391: 損失=0.8089, 準確率=83.56%
   批次 350/391: 損失=0.8132, 準確率=83.35%
   批次 391/391: 損失=0.8131, 準確率=83.32%
Epoch 25 完成: 訓練=83.32%, 驗證=78.22%, 時間=117.7s
   📊 訓練-驗證差異: 5.10%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.22%

Epoch 26/80, LR: 0.002180
   批次  50/391: 損失=0.7920, 準確率=84.69%
   批次 100/391: 損失=0.7958, 準確率=84.18%
   批次 150/391: 損失=0.8030, 準確率=83.82%
   批次 200/391: 損失=0.8080, 準確率=83.60%
   批次 250/391: 損失=0.8094, 準確率=83.54%
   批次 300/391: 損失=0.8086, 準確率=83.62%
   批次 350/391: 損失=0.8069, 準確率=83.63%
   批次 391/391: 損失=0.8072, 準確率=83.59%
Epoch 26 完成: 訓練=83.59%, 驗證=78.01%, 時間=113.3s
   📊 訓練-驗證差異: 5.58%

Epoch 27/80, LR: 0.002133
   批次  50/391: 損失=0.7829, 準確率=84.81%
   批次 100/391: 損失=0.7887, 準確率=84.44%
   批次 150/391: 損失=0.7892, 準確率=84.43%
   批次 200/391: 損失=0.7929, 準確率=84.21%
   批次 250/391: 損失=0.7915, 準確率=84.24%
   批次 300/391: 損失=0.7929, 準確率=84.14%
   批次 350/391: 損失=0.7941, 準確率=84.11%
   批次 391/391: 損失=0.7956, 準確率=84.08%
Epoch 27 完成: 訓練=84.08%, 驗證=78.44%, 時間=113.5s
   📊 訓練-驗證差異: 5.64%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.44%

Epoch 28/80, LR: 0.002086
   批次  50/391: 損失=0.7748, 準確率=85.22%
   批次 100/391: 損失=0.7766, 準確率=84.84%
   批次 150/391: 損失=0.7802, 準確率=84.69%
   批次 200/391: 損失=0.7817, 準確率=84.67%
   批次 250/391: 損失=0.7869, 準確率=84.52%
   批次 300/391: 損失=0.7862, 準確率=84.55%
   批次 350/391: 損失=0.7894, 準確率=84.43%
   批次 391/391: 損失=0.7891, 準確率=84.37%
Epoch 28 完成: 訓練=84.37%, 驗證=79.06%, 時間=112.4s
   📊 訓練-驗證差異: 5.31%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.06%

Epoch 29/80, LR: 0.002038
   批次  50/391: 損失=0.7702, 準確率=84.98%
   批次 100/391: 損失=0.7705, 準確率=85.30%
   批次 150/391: 損失=0.7762, 準確率=84.98%
   批次 200/391: 損失=0.7780, 準確率=84.91%
   批次 250/391: 損失=0.7784, 準確率=84.87%
   批次 300/391: 損失=0.7771, 準確率=84.89%
   批次 350/391: 損失=0.7786, 準確率=84.78%
   批次 391/391: 損失=0.7798, 準確率=84.69%
Epoch 29 完成: 訓練=84.69%, 驗證=78.85%, 時間=113.3s
   📊 訓練-驗證差異: 5.84%

Epoch 30/80, LR: 0.001988
   批次  50/391: 損失=0.7614, 準確率=85.42%
   批次 100/391: 損失=0.7648, 準確率=85.32%
   批次 150/391: 損失=0.7706, 準確率=85.15%
   批次 200/391: 損失=0.7686, 準確率=85.26%
   批次 250/391: 損失=0.7714, 準確率=85.21%
   批次 300/391: 損失=0.7735, 準確率=85.12%
   批次 350/391: 損失=0.7740, 準確率=85.10%
   批次 391/391: 損失=0.7751, 準確率=85.04%
Epoch 30 完成: 訓練=85.04%, 驗證=78.43%, 時間=112.4s
   📊 訓練-驗證差異: 6.61%

Epoch 31/80, LR: 0.001938
   批次  50/391: 損失=0.7562, 準確率=85.67%
   批次 100/391: 損失=0.7578, 準確率=85.82%
   批次 150/391: 損失=0.7561, 準確率=85.76%
   批次 200/391: 損失=0.7614, 準確率=85.58%
   批次 250/391: 損失=0.7628, 準確率=85.59%
   批次 300/391: 損失=0.7637, 準確率=85.57%
   批次 350/391: 損失=0.7643, 準確率=85.48%
   批次 391/391: 損失=0.7664, 準確率=85.41%
Epoch 31 完成: 訓練=85.41%, 驗證=79.28%, 時間=112.5s
   📊 訓練-驗證差異: 6.13%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.28%

Epoch 32/80, LR: 0.001887
   批次  50/391: 損失=0.7512, 準確率=86.06%
   批次 100/391: 損失=0.7499, 準確率=86.04%
   批次 150/391: 損失=0.7509, 準確率=86.14%
   批次 200/391: 損失=0.7533, 準確率=86.03%
   批次 250/391: 損失=0.7582, 準確率=85.83%
   批次 300/391: 損失=0.7576, 準確率=85.89%
   批次 350/391: 損失=0.7583, 準確率=85.84%
   批次 391/391: 損失=0.7577, 準確率=85.86%
Epoch 32 完成: 訓練=85.86%, 驗證=79.10%, 時間=112.7s
   📊 訓練-驗證差異: 6.76%

Epoch 33/80, LR: 0.001835
   批次  50/391: 損失=0.7306, 準確率=86.88%
   批次 100/391: 損失=0.7398, 準確率=86.31%
   批次 150/391: 損失=0.7466, 準確率=85.96%
   批次 200/391: 損失=0.7457, 準確率=86.05%
   批次 250/391: 損失=0.7475, 準確率=86.03%
   批次 300/391: 損失=0.7483, 準確率=85.96%
   批次 350/391: 損失=0.7512, 準確率=85.87%
   批次 391/391: 損失=0.7521, 準確率=85.86%
Epoch 33 完成: 訓練=85.86%, 驗證=78.87%, 時間=113.4s
   📊 訓練-驗證差異: 6.99%

Epoch 34/80, LR: 0.001783
   批次  50/391: 損失=0.7245, 準確率=87.20%
   批次 100/391: 損失=0.7342, 準確率=86.88%
   批次 150/391: 損失=0.7432, 準確率=86.41%
   批次 200/391: 損失=0.7423, 準確率=86.35%
   批次 250/391: 損失=0.7418, 準確率=86.50%
   批次 300/391: 損失=0.7450, 準確率=86.29%
   批次 350/391: 損失=0.7461, 準確率=86.20%
   批次 391/391: 損失=0.7465, 準確率=86.16%
Epoch 34 完成: 訓練=86.16%, 驗證=79.30%, 時間=112.7s
   📊 訓練-驗證差異: 6.86%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.30%

Epoch 35/80, LR: 0.001730
   批次  50/391: 損失=0.6987, 準確率=87.95%
   批次 100/391: 損失=0.7127, 準確率=87.40%
   批次 150/391: 損失=0.7171, 準確率=87.37%
   批次 200/391: 損失=0.7218, 準確率=87.14%
   批次 250/391: 損失=0.7257, 準確率=87.03%
   批次 300/391: 損失=0.7286, 準確率=86.96%
   批次 350/391: 損失=0.7328, 準確率=86.72%
   批次 391/391: 損失=0.7349, 準確率=86.65%
Epoch 35 完成: 訓練=86.65%, 驗證=78.62%, 時間=118.2s
   ⚠️  過擬合警告: 差異 8.03% > 閾值 7.5% (1/8)
   📊 訓練-驗證差異: 8.03%

Epoch 36/80, LR: 0.001676
   批次  50/391: 損失=0.7103, 準確率=88.00%
   批次 100/391: 損失=0.7175, 準確率=87.49%
   批次 150/391: 損失=0.7208, 準確率=87.44%
   批次 200/391: 損失=0.7215, 準確率=87.34%
   批次 250/391: 損失=0.7237, 準確率=87.22%
   批次 300/391: 損失=0.7260, 準確率=87.13%
   批次 350/391: 損失=0.7285, 準確率=87.04%
   批次 391/391: 損失=0.7297, 準確率=86.99%
Epoch 36 完成: 訓練=86.99%, 驗證=78.69%, 時間=115.4s
   ⚠️  過擬合警告: 差異 8.30% > 閾值 7.5% (2/8)
   📊 訓練-驗證差異: 8.30%

Epoch 37/80, LR: 0.001622
   批次  50/391: 損失=0.7060, 準確率=87.92%
   批次 100/391: 損失=0.7116, 準確率=87.90%
   批次 150/391: 損失=0.7163, 準確率=87.72%
   批次 200/391: 損失=0.7176, 準確率=87.56%
   批次 250/391: 損失=0.7192, 準確率=87.51%
   批次 300/391: 損失=0.7215, 準確率=87.38%
   批次 350/391: 損失=0.7222, 準確率=87.36%
   批次 391/391: 損失=0.7240, 準確率=87.24%
Epoch 37 完成: 訓練=87.24%, 驗證=79.04%, 時間=127.0s
   ⚠️  過擬合警告: 差異 8.20% > 閾值 7.5% (3/8)
   📊 訓練-驗證差異: 8.20%

Epoch 38/80, LR: 0.001568
   批次  50/391: 損失=0.6994, 準確率=88.20%
   批次 100/391: 損失=0.7063, 準確率=87.98%
   批次 150/391: 損失=0.7061, 準確率=87.95%
   批次 200/391: 損失=0.7074, 準確率=87.94%
   批次 250/391: 損失=0.7072, 準確率=88.00%
   批次 300/391: 損失=0.7100, 準確率=87.89%
   批次 350/391: 損失=0.7120, 準確率=87.82%
   批次 391/391: 損失=0.7143, 準確率=87.76%
Epoch 38 完成: 訓練=87.76%, 驗證=79.73%, 時間=115.2s
   ⚠️  過擬合警告: 差異 8.03% > 閾值 7.5% (4/8)
   📊 訓練-驗證差異: 8.03%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.73%

Epoch 39/80, LR: 0.001514
   批次  50/391: 損失=0.7042, 準確率=88.42%
   批次 100/391: 損失=0.6995, 準確率=88.59%
   批次 150/391: 損失=0.7006, 準確率=88.59%
   批次 200/391: 損失=0.7005, 準確率=88.66%
   批次 250/391: 損失=0.7030, 準確率=88.51%
   批次 300/391: 損失=0.7070, 準確率=88.23%
   批次 350/391: 損失=0.7065, 準確率=88.23%
   批次 391/391: 損失=0.7091, 準確率=88.10%
Epoch 39 完成: 訓練=88.10%, 驗證=79.05%, 時間=121.2s
   ⚠️  過擬合警告: 差異 9.05% > 閾值 7.5% (5/8)
   📊 訓練-驗證差異: 9.05%

Epoch 40/80, LR: 0.001459
   批次  50/391: 損失=0.6794, 準確率=89.17%
   批次 100/391: 損失=0.6920, 準確率=88.78%
   批次 150/391: 損失=0.6958, 準確率=88.56%
   批次 200/391: 損失=0.6966, 準確率=88.61%
   批次 250/391: 損失=0.6977, 準確率=88.61%
   批次 300/391: 損失=0.6987, 準確率=88.50%
   批次 350/391: 損失=0.7005, 準確率=88.39%
   批次 391/391: 損失=0.7017, 準確率=88.31%
Epoch 40 完成: 訓練=88.31%, 驗證=78.67%, 時間=115.6s
   ⚠️  過擬合警告: 差異 9.64% > 閾值 7.5% (6/8)
   📊 訓練-驗證差異: 9.64%

Epoch 41/80, LR: 0.001404
   批次  50/391: 損失=0.6772, 準確率=89.62%
   批次 100/391: 損失=0.6873, 準確率=88.98%
   批次 150/391: 損失=0.6866, 準確率=88.89%
   批次 200/391: 損失=0.6886, 準確率=88.83%
   批次 250/391: 損失=0.6911, 準確率=88.77%
   批次 300/391: 損失=0.6930, 準確率=88.68%
   批次 350/391: 損失=0.6943, 準確率=88.62%
   批次 391/391: 損失=0.6955, 準確率=88.59%
Epoch 41 完成: 訓練=88.59%, 驗證=79.30%, 時間=115.6s
   ⚠️  過擬合警告: 差異 9.29% > 閾值 7.5% (7/8)
   📊 訓練-驗證差異: 9.29%

Epoch 42/80, LR: 0.001349
   批次  50/391: 損失=0.6742, 準確率=89.59%
   批次 100/391: 損失=0.6736, 準確率=89.57%
   批次 150/391: 損失=0.6796, 準確率=89.21%
   批次 200/391: 損失=0.6807, 準確率=89.13%
   批次 250/391: 損失=0.6836, 準確率=89.10%
   批次 300/391: 損失=0.6865, 準確率=89.01%
   批次 350/391: 損失=0.6862, 準確率=89.06%
   批次 391/391: 損失=0.6873, 準確率=89.00%
Epoch 42 完成: 訓練=89.00%, 驗證=79.70%, 時間=115.1s
   ⚠️  過擬合警告: 差異 9.30% > 閾值 7.5% (8/8)
   🛑 過擬合早停: 連續 8 epochs 訓練-驗證差異超過 7.5%

⏱️ 超快速 aMLP 訓練時間統計:
   • 總訓練時間: 4816.5s (80.3min)
   • 實際訓練epochs: 42 / 80
   • 平均每epoch: 114.7s
   • 最佳 aMLP 驗證準確率: 79.73%
   🔥 注意力機制效果: 預期比原 gMLP 提升 2-5%
   • 已載入最佳 aMLP 模型權重

📈 繪製超快速 aMLP 訓練歷史...

📊 評估超縮小版 aMLP 模型...
   ✓ aMLP 整體準確率: 79.73%
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:697: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.tight_layout()
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:698: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.savefig("ultra_small_amlp_results.png", dpi=300, bbox_inches="tight")
C:\Users\n10825019\AppData\Local\Programs\Python\Python311\Lib\tkinter\__init__.py:861: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  func(*args)

🎉 aMLP 訓練完成總結:
   • 選擇模型: aMLP-Nano (注意力增強)
   • 最終準確率: 79.73%
   • 總訓練時間: 80.3 分鐘
   • 平均每epoch: 114.7 秒
   • 實際訓練輪數: 42/80
   • 早停狀態: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
   ✅ 良好性能！注意力機制有效

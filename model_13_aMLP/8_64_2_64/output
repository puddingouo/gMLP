  可用的 aMLP 模型架構 (附加注意力機制)
==========================================================================================
編號   名稱     深度     維度     FFN   注意力      參數       時間         風險       描述
------------------------------------------------------------------------------------------
1    Test   8      64     2     64       0.18M    <40秒       極低       超極速測試模型 + 注意力
2    Nano   6      64     2     64       0.25M    ~1.5分鐘     很低       極小快速模型 + 注意力
3    XS     8      80     3     40       0.35M    ~2.5分鐘     低        超小平衡模型 + 注意力
4    S      12     128    3     64       0.85M    ~6分鐘       中等       小型性能模型 + 注意力
5    M      16     160    4     80       1.65M    ~12分鐘      較高       中型高性能模型 + 注意力
6    L      30     128    6     64       2.15M    ~18分鐘      很高       大型頂級模型 + 注意力
------------------------------------------------------------------------------------------
💡 推薦選擇 (aMLP 版本):
   🚀 快速測試: Test (1) 或 Nano (2) - 注意力增強
   ⚖️  平衡性能: XS (3) 或 S (4) - 最推薦，效能提升明顯
   🎯 高性能: M (5) 或 L (6) - 頂級注意力效能
   🔥 注意力機制將顯著提升準確率，建議從 XS 開始嘗試
==========================================================================================

🤖 請選擇要使用的 aMLP 模型:
   輸入編號 (1-6) 或模型名稱 (Test/Nano/XS/S/M/L): 1

✅ 您選擇了: aMLP-Test 模型
   📋 模型詳情:
      • 深度: 8 層
      • 維度: 64
      • FFN倍數: 2
      • 注意力維度: 64 (🔥 性能提升關鍵)
      • 預估參數: 0.18M
      • 預估時間: <40秒
      • 過擬合風險: 極低
      • 描述: 超極速測試模型 + 注意力

   確認使用 aMLP-Test 模型嗎? (y/n, 預設=y): y

============================================================
⚙️  aMLP 訓練參數設置
============================================================

📦 數據集模式選擇:
   1. 快速模式 (50K訓練 + 10K測試) - 推薦
   2. 完整模式 (50K訓練 + 10K測試)
   選擇模式 (1/2, 預設=1): 2

🏋️  訓練輪數 (預設=50): 100

🛡️  過擬合保護:
   1. 啟用早停機制 (推薦)
   2. 關閉早停機制
   選擇 (1/2, 預設=1): 1

✅ aMLP 訓練參數確認:
   📦 數據模式: 完整模式
   🏋️  訓練輪數: 100
   🛡️  早停機制: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
📦 加載超快速 CIFAR-10 數據集...
   ✓ 訓練樣本: 50000
   ✓ 測試樣本: 10000
   ✓ Batch大小: 128

🏗️ 創建超縮小版 aMLP-Test 模型...
   ⚡ CPU模式：已設置4個線程

✅ 超縮小版 aMLP-Test 模型創建完成
   ✓ 設備: cpu
   ✓ 實際參數數量: 205,130 (0.21M)
   ✓ 目標參數預期: 0.18M
   ✓ 注意力維度: 32 (🔥 aMLP 增強)
   ✓ 架構配置: depth=8, dim=64, ff_mult=2
   🔥 注意力機制已啟用，預期準確率提升 2-5%

🎬 開始訓練 aMLP-Test 模型...
🔥 注意力機制已啟用，預期更佳訓練效果...

🏋️ 開始超快速 aMLP 訓練 (100 個 epochs)...
   🛡️  啟用過擬合早停保護
   🔥 注意力機制已啟用 - 預期更佳收斂效果

Epoch 1/100, LR: 0.002800
   批次 100/391: 損失=1.8948, 準確率=33.95%
   批次 200/391: 損失=1.7679, 準確率=39.78%
   批次 300/391: 損失=1.6960, 準確率=42.78%
   批次 391/391: 損失=1.6470, 準確率=45.06%
Epoch 1 完成: 訓練=45.06%, 驗證=55.18%, 時間=139.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 55.18%

Epoch 2/100, LR: 0.002799
   批次 100/391: 損失=1.4317, 準確率=55.24%
   批次 200/391: 損失=1.4214, 準確率=55.64%
   批次 300/391: 損失=1.4030, 準確率=56.42%
   批次 391/391: 損失=1.3895, 準確率=57.15%
Epoch 2 完成: 訓練=57.15%, 驗證=61.81%, 時間=136.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 61.81%

Epoch 3/100, LR: 0.002797
   批次 100/391: 損失=1.2985, 準確率=61.82%
   批次 200/391: 損失=1.2860, 準確率=62.07%
   批次 300/391: 損失=1.2734, 準確率=62.68%
   批次 391/391: 損失=1.2673, 準確率=63.10%
Epoch 3 完成: 訓練=63.10%, 驗證=64.36%, 時間=136.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 64.36%

Epoch 4/100, LR: 0.002794
   批次 100/391: 損失=1.2010, 準確率=65.91%
   批次 200/391: 損失=1.2013, 準確率=66.05%
   批次 300/391: 損失=1.2012, 準確率=66.16%
   批次 391/391: 損失=1.1978, 準確率=66.28%
Epoch 4 完成: 訓練=66.28%, 驗證=67.28%, 時間=134.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 67.28%

Epoch 5/100, LR: 0.002789
   批次 100/391: 損失=1.1451, 準確率=69.09%
   批次 200/391: 損失=1.1461, 準確率=68.79%
   批次 300/391: 損失=1.1457, 準確率=68.89%
   批次 391/391: 損失=1.1428, 準確率=68.96%
Epoch 5 完成: 訓練=68.96%, 驗證=69.66%, 時間=137.4s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 69.66%

Epoch 6/100, LR: 0.002783
   批次 100/391: 損失=1.1084, 準確率=69.93%
   批次 200/391: 損失=1.1113, 準確率=69.83%
   批次 300/391: 損失=1.1089, 準確率=70.11%
   批次 391/391: 損失=1.1053, 準確率=70.32%
Epoch 6 完成: 訓練=70.32%, 驗證=70.29%, 時間=141.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 70.29%

Epoch 7/100, LR: 0.002775
   批次 100/391: 損失=1.0647, 準確率=71.91%
   批次 200/391: 損失=1.0652, 準確率=71.99%
   批次 300/391: 損失=1.0665, 準確率=71.97%
   批次 391/391: 損失=1.0672, 準確率=72.08%
Epoch 7 完成: 訓練=72.08%, 驗證=71.36%, 時間=137.7s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 71.36%

Epoch 8/100, LR: 0.002766
   批次 100/391: 損失=1.0344, 準確率=73.58%
   批次 200/391: 損失=1.0382, 準確率=73.53%
   批次 300/391: 損失=1.0387, 準確率=73.61%
   批次 391/391: 損失=1.0398, 準確率=73.50%
Epoch 8 完成: 訓練=73.50%, 驗證=72.75%, 時間=136.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 72.75%

Epoch 9/100, LR: 0.002756
   批次 100/391: 損失=1.0055, 準確率=74.67%
   批次 200/391: 損失=1.0058, 準確率=74.82%
   批次 300/391: 損失=1.0083, 準確率=74.76%
   批次 391/391: 損失=1.0086, 準確率=74.77%
Epoch 9 完成: 訓練=74.77%, 驗證=73.92%, 時間=142.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 73.92%

Epoch 10/100, LR: 0.002745
   批次 100/391: 損失=0.9793, 準確率=75.88%
   批次 200/391: 損失=0.9824, 準確率=75.80%
   批次 300/391: 損失=0.9866, 準確率=75.56%
   批次 391/391: 損失=0.9866, 準確率=75.67%
Epoch 10 完成: 訓練=75.67%, 驗證=72.88%, 時間=139.3s

Epoch 11/100, LR: 0.002732
   批次 100/391: 損失=0.9546, 準確率=77.17%
   批次 200/391: 損失=0.9623, 準確率=76.84%
   批次 300/391: 損失=0.9663, 準確率=76.66%
   批次 391/391: 損失=0.9653, 準確率=76.71%
Epoch 11 完成: 訓練=76.71%, 驗證=73.82%, 時間=139.7s

Epoch 12/100, LR: 0.002717
   批次 100/391: 損失=0.9347, 準確率=77.86%
   批次 200/391: 損失=0.9395, 準確率=77.69%
   批次 300/391: 損失=0.9425, 準確率=77.69%
   批次 391/391: 損失=0.9457, 準確率=77.55%
Epoch 12 完成: 訓練=77.55%, 驗證=74.19%, 時間=138.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 74.19%

Epoch 13/100, LR: 0.002702
   批次 100/391: 損失=0.9251, 準確率=78.33%
   批次 200/391: 損失=0.9220, 準確率=78.30%
   批次 300/391: 損失=0.9255, 準確率=78.25%
   批次 391/391: 損失=0.9299, 準確率=78.08%
Epoch 13 完成: 訓練=78.08%, 驗證=75.01%, 時間=140.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.01%

Epoch 14/100, LR: 0.002685
   批次 100/391: 損失=0.9049, 準確率=79.26%
   批次 200/391: 損失=0.9051, 準確率=79.21%
   批次 300/391: 損失=0.9083, 準確率=79.07%
   批次 391/391: 損失=0.9100, 準確率=78.99%
Epoch 14 完成: 訓練=78.99%, 驗證=76.03%, 時間=138.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.03%

Epoch 15/100, LR: 0.002667
   批次 100/391: 損失=0.8897, 準確率=79.71%
   批次 200/391: 損失=0.8885, 準確率=79.79%
   批次 300/391: 損失=0.8885, 準確率=79.89%
   批次 391/391: 損失=0.8933, 準確率=79.71%
Epoch 15 完成: 訓練=79.71%, 驗證=75.92%, 時間=139.1s

Epoch 16/100, LR: 0.002648
   批次 100/391: 損失=0.8589, 準確率=81.21%
   批次 200/391: 損失=0.8713, 準確率=80.61%
   批次 300/391: 損失=0.8722, 準確率=80.70%
   批次 391/391: 損失=0.8764, 準確率=80.50%
Epoch 16 完成: 訓練=80.50%, 驗證=76.31%, 時間=140.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.31%

Epoch 17/100, LR: 0.002627
   批次 100/391: 損失=0.8414, 準確率=82.16%
   批次 200/391: 損失=0.8549, 準確率=81.59%
   批次 300/391: 損失=0.8618, 準確率=81.35%
   批次 391/391: 損失=0.8650, 準確率=81.13%
Epoch 17 完成: 訓練=81.13%, 驗證=76.85%, 時間=141.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.85%

Epoch 18/100, LR: 0.002606
   批次 100/391: 損失=0.8418, 準確率=82.19%
   批次 200/391: 損失=0.8453, 準確率=82.07%
   批次 300/391: 損失=0.8488, 準確率=81.97%
   批次 391/391: 損失=0.8523, 準確率=81.73%
Epoch 18 完成: 訓練=81.73%, 驗證=76.33%, 時間=139.4s
   📊 訓練-驗證差異: 5.40%

Epoch 19/100, LR: 0.002583
   批次 100/391: 損失=0.8213, 準確率=83.09%
   批次 200/391: 損失=0.8339, 準確率=82.45%
   批次 300/391: 損失=0.8378, 準確率=82.21%
   批次 391/391: 損失=0.8415, 準確率=82.12%
Epoch 19 完成: 訓練=82.12%, 驗證=76.82%, 時間=139.3s
   📊 訓練-驗證差異: 5.30%

Epoch 20/100, LR: 0.002559
   批次 100/391: 損失=0.8215, 準確率=82.99%
   批次 200/391: 損失=0.8214, 準確率=82.96%
   批次 300/391: 損失=0.8234, 準確率=83.02%
   批次 391/391: 損失=0.8269, 準確率=82.85%
Epoch 20 完成: 訓練=82.85%, 驗證=77.49%, 時間=142.3s
   📊 訓練-驗證差異: 5.36%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.49%

Epoch 21/100, LR: 0.002533
   批次 100/391: 損失=0.7951, 準確率=84.30%
   批次 200/391: 損失=0.7992, 準確率=84.26%
   批次 300/391: 損失=0.8098, 準確率=83.64%
   批次 391/391: 損失=0.8133, 準確率=83.38%
Epoch 21 完成: 訓練=83.38%, 驗證=77.80%, 時間=141.6s
   📊 訓練-驗證差異: 5.58%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.80%

Epoch 22/100, LR: 0.002507
   批次 100/391: 損失=0.7793, 準確率=85.04%
   批次 200/391: 損失=0.7873, 準確率=84.62%
   批次 300/391: 損失=0.7956, 準確率=84.19%
   批次 391/391: 損失=0.7999, 準確率=83.96%
Epoch 22 完成: 訓練=83.96%, 驗證=77.70%, 時間=141.5s
   📊 訓練-驗證差異: 6.26%

Epoch 23/100, LR: 0.002480
   批次 100/391: 損失=0.7736, 準確率=85.14%
   批次 200/391: 損失=0.7859, 準確率=84.59%
   批次 300/391: 損失=0.7885, 準確率=84.34%
   批次 391/391: 損失=0.7909, 準確率=84.13%
Epoch 23 完成: 訓練=84.13%, 驗證=78.49%, 時間=141.0s
   📊 訓練-驗證差異: 5.64%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.49%

Epoch 24/100, LR: 0.002451
   批次 100/391: 損失=0.7596, 準確率=85.55%
   批次 200/391: 損失=0.7699, 準確率=85.12%
   批次 300/391: 損失=0.7761, 準確率=84.99%
   批次 391/391: 損失=0.7820, 準確率=84.76%
Epoch 24 完成: 訓練=84.76%, 驗證=78.09%, 時間=141.6s
   📊 訓練-驗證差異: 6.67%

Epoch 25/100, LR: 0.002422
   批次 100/391: 損失=0.7499, 準確率=86.05%
   批次 200/391: 損失=0.7578, 準確率=85.65%
   批次 300/391: 損失=0.7646, 準確率=85.42%
   批次 391/391: 損失=0.7711, 準確率=85.06%
Epoch 25 完成: 訓練=85.06%, 驗證=77.88%, 時間=139.7s
   📊 訓練-驗證差異: 7.18%

Epoch 26/100, LR: 0.002391
   批次 100/391: 損失=0.7513, 準確率=86.09%
   批次 200/391: 損失=0.7547, 準確率=86.01%
   批次 300/391: 損失=0.7575, 準確率=85.85%
   批次 391/391: 損失=0.7603, 準確率=85.71%
Epoch 26 完成: 訓練=85.71%, 驗證=78.13%, 時間=142.7s
   📊 訓練-驗證差異: 7.58%

Epoch 27/100, LR: 0.002360
   批次 100/391: 損失=0.7329, 準確率=86.93%
   批次 200/391: 損失=0.7426, 準確率=86.44%
   批次 300/391: 損失=0.7479, 準確率=86.15%
   批次 391/391: 損失=0.7506, 準確率=86.00%
Epoch 27 完成: 訓練=86.00%, 驗證=78.35%, 時間=139.8s
   📊 訓練-驗證差異: 7.65%

Epoch 28/100, LR: 0.002327
   批次 100/391: 損失=0.7307, 準確率=87.22%
   批次 200/391: 損失=0.7353, 準確率=86.80%
   批次 300/391: 損失=0.7404, 準確率=86.52%
   批次 391/391: 損失=0.7406, 準確率=86.53%
Epoch 28 完成: 訓練=86.53%, 驗證=78.68%, 時間=143.4s
   📊 訓練-驗證差異: 7.85%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.68%

Epoch 29/100, LR: 0.002294
   批次 100/391: 損失=0.7160, 準確率=87.88%
   批次 200/391: 損失=0.7272, 準確率=87.18%
   批次 300/391: 損失=0.7274, 準確率=87.11%
   批次 391/391: 損失=0.7312, 準確率=86.91%
Epoch 29 完成: 訓練=86.91%, 驗證=78.86%, 時間=140.3s
   ⚠️  過擬合警告: 差異 8.05% > 閾值 8% (1/8)
   📊 訓練-驗證差異: 8.05%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.86%

Epoch 30/100, LR: 0.002260
   批次 100/391: 損失=0.7090, 準確率=87.78%
   批次 200/391: 損失=0.7138, 準確率=87.51%
   批次 300/391: 損失=0.7180, 準確率=87.38%
   批次 391/391: 損失=0.7222, 準確率=87.21%
Epoch 30 完成: 訓練=87.21%, 驗證=79.13%, 時間=142.6s
   ⚠️  過擬合警告: 差異 8.08% > 閾值 8% (2/8)
   📊 訓練-驗證差異: 8.08%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.13%

Epoch 31/100, LR: 0.002225
   批次 100/391: 損失=0.7029, 準確率=87.91%
   批次 200/391: 損失=0.7060, 準確率=87.79%
   批次 300/391: 損失=0.7127, 準確率=87.47%
   批次 391/391: 損失=0.7181, 準確率=87.33%
Epoch 31 完成: 訓練=87.33%, 驗證=78.84%, 時間=142.1s
   ⚠️  過擬合警告: 差異 8.49% > 閾值 8% (3/8)
   📊 訓練-驗證差異: 8.49%

Epoch 32/100, LR: 0.002189
   批次 100/391: 損失=0.6944, 準確率=88.36%
   批次 200/391: 損失=0.7002, 準確率=88.05%
   批次 300/391: 損失=0.7065, 準確率=87.83%
   批次 391/391: 損失=0.7083, 準確率=87.82%
Epoch 32 完成: 訓練=87.82%, 驗證=78.74%, 時間=141.5s
   ⚠️  過擬合警告: 差異 9.08% > 閾值 8% (4/8)
   📊 訓練-驗證差異: 9.08%

Epoch 33/100, LR: 0.002152
   批次 100/391: 損失=0.6866, 準確率=88.72%
   批次 200/391: 損失=0.6910, 準確率=88.49%
   批次 300/391: 損失=0.6959, 準確率=88.25%
   批次 391/391: 損失=0.7025, 準確率=87.95%
Epoch 33 完成: 訓練=87.95%, 驗證=79.00%, 時間=139.3s
   ⚠️  過擬合警告: 差異 8.95% > 閾值 8% (5/8)
   📊 訓練-驗證差異: 8.95%

Epoch 34/100, LR: 0.002115
   批次 100/391: 損失=0.6783, 準確率=89.22%
   批次 200/391: 損失=0.6845, 準確率=88.77%
   批次 300/391: 損失=0.6902, 準確率=88.60%
   批次 391/391: 損失=0.6935, 準確率=88.45%
Epoch 34 完成: 訓練=88.45%, 驗證=78.86%, 時間=144.3s
   ⚠️  過擬合警告: 差異 9.59% > 閾值 8% (6/8)
   📊 訓練-驗證差異: 9.59%

Epoch 35/100, LR: 0.002077
   批次 100/391: 損失=0.6685, 準確率=89.87%
   批次 200/391: 損失=0.6755, 準確率=89.37%
   批次 300/391: 損失=0.6816, 準確率=89.05%
   批次 391/391: 損失=0.6868, 準確率=88.76%
Epoch 35 完成: 訓練=88.76%, 驗證=78.75%, 時間=143.2s
   ⚠️  過擬合警告: 差異 10.01% > 閾值 8% (7/8)
   📊 訓練-驗證差異: 10.01%

Epoch 36/100, LR: 0.002038
   批次 100/391: 損失=0.6627, 準確率=89.95%
   批次 200/391: 損失=0.6663, 準確率=89.94%
   批次 300/391: 損失=0.6747, 準確率=89.57%
   批次 391/391: 損失=0.6786, 準確率=89.38%
Epoch 36 完成: 訓練=89.38%, 驗證=79.26%, 時間=139.8s
   ⚠️  過擬合警告: 差異 10.12% > 閾值 8% (8/8)
   🛑 過擬合早停: 連續 8 epochs 訓練-驗證差異超過 8%

⏱️ 超快速 aMLP 訓練時間統計:
   • 總訓練時間: 5044.5s (84.1min)
   • 實際訓練epochs: 36 / 100
   • 平均每epoch: 140.1s
   • 最佳 aMLP 驗證準確率: 79.13%
   🔥 注意力機制效果: 預期比原 gMLP 提升 2-5%
   • 已載入最佳 aMLP 模型權重

📈 繪製超快速 aMLP 訓練歷史...

📊 評估超縮小版 aMLP 模型...
   ✓ aMLP 整體準確率: 79.13%
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:697: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.tight_layout()
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:698: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.savefig("ultra_small_amlp_results.png", dpi=300, bbox_inches="tight")
C:\Users\n10825019\AppData\Local\Programs\Python\Python311\Lib\tkinter\__init__.py:861: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  func(*args)

🎉 aMLP 訓練完成總結:
   • 選擇模型: aMLP-Test (注意力增強)
   • 最終準確率: 79.13%
   • 總訓練時間: 84.1 分鐘
   • 平均每epoch: 140.1 秒
   • 實際訓練輪數: 36/100
   • 早停狀態: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
   ✅ 良好性能！注意力機制有效

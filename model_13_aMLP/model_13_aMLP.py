"""
è¶…ç¸®å°ç‰ˆ aMLP åœ–åƒåˆ†é¡æ¨¡å‹ - è‡ªç”±é¸æ“‡ç‰ˆæœ¬
åŸºæ–¼è«–æ–‡æ¶æ§‹ä½†å¤§å¹…ç¸®å°è¦æ¨¡ä»¥æé«˜è¨“ç·´æ•ˆç‡
é‡å°å¿«é€ŸåŸå‹é–‹ç™¼å’Œè³‡æºå—é™ç’°å¢ƒå„ªåŒ–
æ”¯æ´äº’å‹•å¼æ¨¡å‹é¸æ“‡
åŠ å…¥æ³¨æ„åŠ›æ©Ÿåˆ¶ (aMLP) ä»¥æå‡æ•ˆèƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
from g_mlp_pytorch import gMLPVision
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns


def load_cifar10_data_ultrafast(quick_test=True):
    """åŠ è¼‰è¶…å¿«é€Ÿ CIFAR-10 æ•¸æ“šé›† - é‡å°å¿«é€Ÿè¨“ç·´å„ªåŒ–"""
    print("ğŸ“¦ åŠ è¼‰è¶…å¿«é€Ÿ CIFAR-10 æ•¸æ“šé›†...")

    # ç°¡åŒ–çš„æ•¸æ“šå¢å¼·ç­–ç•¥ - æ¸›å°‘è¨ˆç®—é–‹éŠ·
    transform_train = transforms.Compose(
        [
            transforms.RandomCrop(32, padding=2),
            transforms.RandomHorizontalFlip(p=0.3),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    transform_test = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ]
    )

    trainset = torchvision.datasets.CIFAR10(
        root="./data", train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root="./data", train=False, download=True, transform=transform_test
    )

    if quick_test:
        trainset = Subset(trainset, range(50000))
        testset = Subset(testset, range(10000))
        print("   ğŸš€ è¶…å¿«é€Ÿæ¨¡å¼ï¼šå°è¦æ¨¡æ•¸æ“šé›†è¨“ç·´")

    batch_size = 128
    num_workers = 0
    pin_memory = False

    trainloader = DataLoader(
        trainset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    testloader = DataLoader(
        testset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )

    classes = [
        "Airplane",
        "Automobile",
        "Bird",
        "Cat",
        "Deer",
        "Dog",
        "Frog",
        "Horse",
        "Ship",
        "Truck",
    ]

    print(f"   âœ“ è¨“ç·´æ¨£æœ¬: {len(trainset)}")
    print(f"   âœ“ æ¸¬è©¦æ¨£æœ¬: {len(testset)}")
    print(f"   âœ“ Batchå¤§å°: {batch_size}")

    return trainloader, testloader, classes


def display_model_options():
    """é¡¯ç¤ºæ‰€æœ‰å¯ç”¨çš„ aMLP æ¨¡å‹é¸é …"""
    print("\n" + "=" * 90)
    print("ğŸ—ï¸  å¯ç”¨çš„ aMLP æ¨¡å‹æ¶æ§‹ (é™„åŠ æ³¨æ„åŠ›æ©Ÿåˆ¶)")
    print("=" * 90)

    models_info = {
        "Test": {
            "depth": 8,
            "dim": 64,
            "ff_mult": 2,
            "attn_dim": 64,
            "params": "0.18M",
            "time": "<40ç§’",
            "risk": "æ¥µä½",
            "desc": "è¶…æ¥µé€Ÿæ¸¬è©¦æ¨¡å‹ + æ³¨æ„åŠ›",
        },
        "Nano": {
            "depth": 6,
            "dim": 64,
            "ff_mult": 2,
            "attn_dim": 64,
            "params": "0.25M",
            "time": "~1.5åˆ†é˜",
            "risk": "å¾ˆä½",
            "desc": "æ¥µå°å¿«é€Ÿæ¨¡å‹ + æ³¨æ„åŠ›",
        },
        "XS": {
            "depth": 8,
            "dim": 80,
            "ff_mult": 3,
            "attn_dim": 40,
            "params": "0.35M",
            "time": "~2.5åˆ†é˜",
            "risk": "ä½",
            "desc": "è¶…å°å¹³è¡¡æ¨¡å‹ + æ³¨æ„åŠ›",
        },
        "S": {
            "depth": 12,
            "dim": 128,
            "ff_mult": 3,
            "attn_dim": 64,
            "params": "0.85M",
            "time": "~6åˆ†é˜",
            "risk": "ä¸­ç­‰",
            "desc": "å°å‹æ€§èƒ½æ¨¡å‹ + æ³¨æ„åŠ›",
        },
        "M": {
            "depth": 16,
            "dim": 160,
            "ff_mult": 4,
            "attn_dim": 80,
            "params": "1.65M",
            "time": "~12åˆ†é˜",
            "risk": "è¼ƒé«˜",
            "desc": "ä¸­å‹é«˜æ€§èƒ½æ¨¡å‹ + æ³¨æ„åŠ›",
        },
        "L": {
            "depth": 30,
            "dim": 128,
            "ff_mult": 6,
            "attn_dim": 64,
            "params": "2.15M",
            "time": "~18åˆ†é˜",
            "risk": "å¾ˆé«˜",
            "desc": "å¤§å‹é ‚ç´šæ¨¡å‹ + æ³¨æ„åŠ›",
        },
    }

    print(
        f"{'ç·¨è™Ÿ':<4} {'åç¨±':<6} {'æ·±åº¦':<6} {'ç¶­åº¦':<6} {'FFN':<5} {'æ³¨æ„åŠ›':<8} {'åƒæ•¸':<8} {'æ™‚é–“':<10} {'é¢¨éšª':<8} {'æè¿°':<25}"
    )
    print("-" * 90)

    for i, (name, info) in enumerate(models_info.items(), 1):
        print(
            f"{i:<4} {name:<6} {info['depth']:<6} {info['dim']:<6} {info['ff_mult']:<5} "
            f"{info['attn_dim']:<8} {info['params']:<8} {info['time']:<10} {info['risk']:<8} {info['desc']:<25}"
        )

    print("-" * 90)
    print("ğŸ’¡ æ¨è–¦é¸æ“‡ (aMLP ç‰ˆæœ¬):")
    print("   ğŸš€ å¿«é€Ÿæ¸¬è©¦: Test (1) æˆ– Nano (2) - æ³¨æ„åŠ›å¢å¼·")
    print("   âš–ï¸  å¹³è¡¡æ€§èƒ½: XS (3) æˆ– S (4) - æœ€æ¨è–¦ï¼Œæ•ˆèƒ½æå‡æ˜é¡¯")
    print("   ğŸ¯ é«˜æ€§èƒ½: M (5) æˆ– L (6) - é ‚ç´šæ³¨æ„åŠ›æ•ˆèƒ½")
    print("   ğŸ”¥ æ³¨æ„åŠ›æ©Ÿåˆ¶å°‡é¡¯è‘—æå‡æº–ç¢ºç‡ï¼Œå»ºè­°å¾ XS é–‹å§‹å˜—è©¦")
    print("=" * 90)

    return models_info


def get_user_model_choice():
    """ç²å–ç”¨æˆ¶çš„ aMLP æ¨¡å‹é¸æ“‡"""
    models_info = display_model_options()
    model_names = list(models_info.keys())

    while True:
        try:
            print("\nğŸ¤– è«‹é¸æ“‡è¦ä½¿ç”¨çš„ aMLP æ¨¡å‹:")
            choice = input(
                "   è¼¸å…¥ç·¨è™Ÿ (1-6) æˆ–æ¨¡å‹åç¨± (Test/Nano/XS/S/M/L): "
            ).strip()

            # è™•ç†æ•¸å­—è¼¸å…¥
            if choice.isdigit():
                choice_num = int(choice)
                if 1 <= choice_num <= 6:
                    selected_model = model_names[choice_num - 1]
                    break
                else:
                    print("   âŒ è«‹è¼¸å…¥ 1-6 ä¹‹é–“çš„æ•¸å­—")
                    continue

            # è™•ç†åç¨±è¼¸å…¥
            elif choice.upper() in model_names:
                selected_model = choice.upper()
                break
            elif choice.lower() in [name.lower() for name in model_names]:
                # å¤§å°å¯«ä¸æ•æ„ŸåŒ¹é…
                selected_model = next(
                    name for name in model_names if name.lower() == choice.lower()
                )
                break
            else:
                print("   âŒ ç„¡æ•ˆè¼¸å…¥ï¼Œè«‹è¼¸å…¥æ­£ç¢ºçš„ç·¨è™Ÿæˆ–æ¨¡å‹åç¨±")
                continue

        except (ValueError, KeyboardInterrupt):
            print("   âŒ è¼¸å…¥éŒ¯èª¤ï¼Œè«‹é‡æ–°è¼¸å…¥")
            continue

    # é¡¯ç¤ºé¸æ“‡ç¢ºèª
    selected_info = models_info[selected_model]
    print(f"\nâœ… æ‚¨é¸æ“‡äº†: aMLP-{selected_model} æ¨¡å‹")
    print(f"   ğŸ“‹ æ¨¡å‹è©³æƒ…:")
    print(f"      â€¢ æ·±åº¦: {selected_info['depth']} å±¤")
    print(f"      â€¢ ç¶­åº¦: {selected_info['dim']}")
    print(f"      â€¢ FFNå€æ•¸: {selected_info['ff_mult']}")
    print(f"      â€¢ æ³¨æ„åŠ›ç¶­åº¦: {selected_info['attn_dim']} (ğŸ”¥ æ€§èƒ½æå‡é—œéµ)")
    print(f"      â€¢ é ä¼°åƒæ•¸: {selected_info['params']}")
    print(f"      â€¢ é ä¼°æ™‚é–“: {selected_info['time']}")
    print(f"      â€¢ éæ“¬åˆé¢¨éšª: {selected_info['risk']}")
    print(f"      â€¢ æè¿°: {selected_info['desc']}")

    # ç¢ºèªé¸æ“‡
    confirm = (
        input(f"\n   ç¢ºèªä½¿ç”¨ aMLP-{selected_model} æ¨¡å‹å—? (y/n, é è¨­=y): ")
        .strip()
        .lower()
    )
    if confirm in ["n", "no"]:
        print("   ğŸ”„ é‡æ–°é¸æ“‡...")
        return get_user_model_choice()  # éè¿´é‡æ–°é¸æ“‡

    return selected_model


def get_training_parameters():
    """ç²å–è¨“ç·´åƒæ•¸è¨­ç½®"""
    print("\n" + "=" * 60)
    print("âš™ï¸  aMLP è¨“ç·´åƒæ•¸è¨­ç½®")
    print("=" * 60)

    # é è¨­åƒæ•¸
    default_epochs = 50
    default_quick_test = True

    try:
        # é¸æ“‡æ•¸æ“šé›†æ¨¡å¼
        print("\nğŸ“¦ æ•¸æ“šé›†æ¨¡å¼é¸æ“‡:")
        print("   1. å¿«é€Ÿæ¨¡å¼ (50Kè¨“ç·´ + 10Kæ¸¬è©¦) - æ¨è–¦")
        print("   2. å®Œæ•´æ¨¡å¼ (50Kè¨“ç·´ + 10Kæ¸¬è©¦)")

        data_choice = input("   é¸æ“‡æ¨¡å¼ (1/2, é è¨­=1): ").strip()
        quick_test = True if data_choice != "2" else False

        # è¨­ç½®è¨“ç·´è¼ªæ•¸
        epochs_input = input(f"\nğŸ‹ï¸  è¨“ç·´è¼ªæ•¸ (é è¨­={default_epochs}): ").strip()
        epochs = int(epochs_input) if epochs_input.isdigit() else default_epochs

        # é¸æ“‡æ˜¯å¦å•Ÿç”¨æ—©åœ
        print("\nğŸ›¡ï¸  éæ“¬åˆä¿è­·:")
        print("   1. å•Ÿç”¨æ—©åœæ©Ÿåˆ¶ (æ¨è–¦)")
        print("   2. é—œé–‰æ—©åœæ©Ÿåˆ¶")

        early_stop_choice = input("   é¸æ“‡ (1/2, é è¨­=1): ").strip()
        enable_early_stop = True if early_stop_choice != "2" else False

        print(f"\nâœ… aMLP è¨“ç·´åƒæ•¸ç¢ºèª:")
        print(f"   ğŸ“¦ æ•¸æ“šæ¨¡å¼: {'å¿«é€Ÿæ¨¡å¼' if quick_test else 'å®Œæ•´æ¨¡å¼'}")
        print(f"   ğŸ‹ï¸  è¨“ç·´è¼ªæ•¸: {epochs}")
        print(f"   ğŸ›¡ï¸  æ—©åœæ©Ÿåˆ¶: {'å•Ÿç”¨' if enable_early_stop else 'é—œé–‰'}")
        print(f"   ğŸ”¥ æ³¨æ„åŠ›æ©Ÿåˆ¶: å·²å•Ÿç”¨ (aMLP)")

        return {
            "epochs": epochs,
            "quick_test": quick_test,
            "enable_early_stop": enable_early_stop,
        }

    except (ValueError, KeyboardInterrupt):
        print("   âš ï¸  ä½¿ç”¨é è¨­åƒæ•¸")
        return {
            "epochs": default_epochs,
            "quick_test": default_quick_test,
            "enable_early_stop": True,
        }


def create_ultra_small_amlp_model(model_size="L"):
    """å‰µå»ºè¶…ç¸®å°ç‰ˆ aMLP æ¨¡å‹æ¶æ§‹ (é™„åŠ æ³¨æ„åŠ›æ©Ÿåˆ¶)"""
    print(f"\nğŸ—ï¸ å‰µå»ºè¶…ç¸®å°ç‰ˆ aMLP-{model_size} æ¨¡å‹...")

    # CPUå°ˆç”¨å„ªåŒ–è¨­ç½®
    torch.set_num_threads(4)
    print("   âš¡ CPUæ¨¡å¼ï¼šå·²è¨­ç½®4å€‹ç·šç¨‹")

    # è¶…ç¸®å°ç‰ˆ aMLP æ¶æ§‹é…ç½®
    if model_size == "Test":
        config = {
            "depth": 8,
            "dim": 64,
            "ff_mult": 2,
            "attn_dim": 32,
            "prob_survival": 1.00,
            "params_target": 0.18,
        }
    elif model_size == "Nano":
        config = {
            "depth": 6,
            "dim": 64,
            "ff_mult": 2,
            "attn_dim": 32,
            "prob_survival": 1.00,
            "params_target": 0.25,
        }
    elif model_size == "XS":
        config = {
            "depth": 8,
            "dim": 80,
            "ff_mult": 3,
            "attn_dim": 40,
            "prob_survival": 1.00,
            "params_target": 0.35,
        }
    elif model_size == "S":
        config = {
            "depth": 12,
            "dim": 128,
            "ff_mult": 3,
            "attn_dim": 64,
            "prob_survival": 0.98,
            "params_target": 0.85,
        }
    elif model_size == "M":
        config = {
            "depth": 16,
            "dim": 160,
            "ff_mult": 4,
            "attn_dim": 80,
            "prob_survival": 0.95,
            "params_target": 1.65,
        }
    elif model_size == "L":
        config = {
            "depth": 30,
            "dim": 128,
            "ff_mult": 6,
            "attn_dim": 64,
            "prob_survival": 1.00,
            "params_target": 2.15,
        }
    else:
        raise ValueError(
            f"ä¸æ”¯æ´çš„æ¨¡å‹å¤§å°: {model_size}ã€‚æ”¯æ´: Test, Nano, XS, S, M, L"
        )

    # å‰µå»º aMLP æ¨¡å‹ (é™„åŠ æ³¨æ„åŠ›æ©Ÿåˆ¶)
    model = gMLPVision(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=config["dim"],
        depth=config["depth"],
        ff_mult=config["ff_mult"],
        channels=3,
        prob_survival=config["prob_survival"],
        attn_dim=config["attn_dim"],  # ğŸ”¥ é—œéµï¼šå•Ÿç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    total_params = sum(p.numel() for p in model.parameters())
    params_M = total_params / 1e6

    print(f"\nâœ… è¶…ç¸®å°ç‰ˆ aMLP-{model_size} æ¨¡å‹å‰µå»ºå®Œæˆ")
    print(f"   âœ“ è¨­å‚™: {device}")
    print(f"   âœ“ å¯¦éš›åƒæ•¸æ•¸é‡: {total_params:,} ({params_M:.2f}M)")
    print(f"   âœ“ ç›®æ¨™åƒæ•¸é æœŸ: {config['params_target']}M")
    print(f"   âœ“ æ³¨æ„åŠ›ç¶­åº¦: {config['attn_dim']} (ğŸ”¥ aMLP å¢å¼·)")
    print(
        f"   âœ“ æ¶æ§‹é…ç½®: depth={config['depth']}, dim={config['dim']}, ff_mult={config['ff_mult']}"
    )
    print(f"   ğŸ”¥ æ³¨æ„åŠ›æ©Ÿåˆ¶å·²å•Ÿç”¨ï¼Œé æœŸæº–ç¢ºç‡æå‡ 2-5%")

    return model, device


def train_ultra_fast_amlp(
    model, trainloader, testloader, device, epochs=50, enable_early_stop=True
):
    """è¶…å¿«é€Ÿ aMLP è¨“ç·´é…ç½® - æ”¯æ´è‡ªå®šç¾©æ—©åœè¨­ç½®"""
    print(f"\nğŸ‹ï¸ é–‹å§‹è¶…å¿«é€Ÿ aMLP è¨“ç·´ ({epochs} å€‹ epochs)...")
    if enable_early_stop:
        print("   ğŸ›¡ï¸  å•Ÿç”¨éæ“¬åˆæ—©åœä¿è­·")
    else:
        print("   âš ï¸  æ—©åœä¿è­·å·²é—œé–‰")
    print("   ğŸ”¥ æ³¨æ„åŠ›æ©Ÿåˆ¶å·²å•Ÿç”¨ - é æœŸæ›´ä½³æ”¶æ–‚æ•ˆæœ")

    # aMLP å„ªåŒ–è¨“ç·´é…ç½®
    criterion = nn.CrossEntropyLoss(label_smoothing=0.08)  # ç•¥å¾®å¢åŠ æ¨™ç±¤å¹³æ»‘
    optimizer = optim.AdamW(
        model.parameters(),
        lr=2.8e-3,  # ç•¥å¾®é™ä½å­¸ç¿’ç‡ä»¥é…åˆæ³¨æ„åŠ›æ©Ÿåˆ¶
        weight_decay=0.012,  # ç•¥å¾®å¢åŠ æ¬Šé‡è¡°æ¸›
        betas=(0.9, 0.95),
    )

    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=epochs, eta_min=8e-6
    )

    train_losses = []
    train_accs = []
    val_accs = []
    val_losses = []
    epoch_times = []

    best_val_acc = 0
    patience = (
        18 if enable_early_stop else epochs + 1
    )  # å¢åŠ è€å¿ƒå› ç‚º aMLP éœ€è¦æ›´å¤šæ™‚é–“æ”¶æ–‚
    patience_counter = 0

    # éæ“¬åˆæ—©åœé…ç½® - aMLP èª¿æ•´
    overfitting_patience = 8 if enable_early_stop else epochs + 1  # å¢åŠ è€å¿ƒ
    overfitting_counter = 0
    overfitting_threshold = 8  # ç•¥å¾®é™ä½é–¾å€¼
    min_epochs_before_overfitting_check = 10  # å¢åŠ æª¢æŸ¥å‰çš„æœ€å°epoch

    total_start_time = time.time()

    for epoch in range(epochs):
        epoch_start_time = time.time()

        # è¨“ç·´éšæ®µ
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        print(
            f"\nEpoch {epoch + 1}/{epochs}, LR: {optimizer.param_groups[0]['lr']:.6f}"
        )

        for i, (inputs, labels) in enumerate(trainloader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()

            torch.nn.utils.clip_grad_norm_(
                model.parameters(), max_norm=0.8
            )  # å¢åŠ æ¢¯åº¦è£å‰ª
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            if (i + 1) % 100 == 0 or (i + 1) == len(trainloader):
                acc = 100.0 * correct / total
                print(
                    f"   æ‰¹æ¬¡ {i+1:3d}/{len(trainloader)}: æå¤±={running_loss/(i+1):.4f}, æº–ç¢ºç‡={acc:.2f}%"
                )

        epoch_loss = running_loss / len(trainloader)
        epoch_acc = 100.0 * correct / total
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)

        # é©—è­‰éšæ®µ
        val_acc, val_loss = quick_validate_with_loss(
            model, testloader, device, criterion
        )
        val_accs.append(val_acc)
        val_losses.append(val_loss)

        scheduler.step()

        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        epoch_times.append(epoch_duration)

        print(
            f"Epoch {epoch + 1} å®Œæˆ: è¨“ç·´={epoch_acc:.2f}%, é©—è­‰={val_acc:.2f}%, æ™‚é–“={epoch_duration:.1f}s"
        )

        # æ—©åœæª¢æ¸¬ (åªåœ¨å•Ÿç”¨æ™‚åŸ·è¡Œ)
        if enable_early_stop:
            # éæ“¬åˆæ—©åœæª¢æ¸¬
            train_val_diff = epoch_acc - val_acc
            if epoch >= min_epochs_before_overfitting_check:
                if train_val_diff > overfitting_threshold:
                    overfitting_counter += 1
                    print(
                        f"   âš ï¸  éæ“¬åˆè­¦å‘Š: å·®ç•° {train_val_diff:.2f}% > é–¾å€¼ {overfitting_threshold}% ({overfitting_counter}/{overfitting_patience})"
                    )

                    if overfitting_counter >= overfitting_patience:
                        print(
                            f"   ğŸ›‘ éæ“¬åˆæ—©åœ: é€£çºŒ {overfitting_patience} epochs è¨“ç·´-é©—è­‰å·®ç•°è¶…é {overfitting_threshold}%"
                        )
                        break
                else:
                    overfitting_counter = 0

            if train_val_diff > 5:
                print(f"   ğŸ“Š è¨“ç·´-é©—è­‰å·®ç•°: {train_val_diff:.2f}%")

            # æ€§èƒ½æ—©åœæ©Ÿåˆ¶
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                torch.save(model.state_dict(), "best_ultra_small_amlp_model.pth")
                print(f"   ğŸ’¾ æ–°æœ€ä½³ aMLP æ¨¡å‹å·²ä¿å­˜: é©—è­‰æº–ç¢ºç‡ {best_val_acc:.2f}%")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"   â° æ€§èƒ½æ—©åœï¼šé©—è­‰æº–ç¢ºç‡ {patience} å€‹epochæœªæå‡")
                    break
        else:
            # ä¸å•Ÿç”¨æ—©åœæ™‚ä»ç„¶ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(model.state_dict(), "best_ultra_small_amlp_model.pth")
                print(f"   ğŸ’¾ æ–°æœ€ä½³ aMLP æ¨¡å‹å·²ä¿å­˜: é©—è­‰æº–ç¢ºç‡ {best_val_acc:.2f}%")

    total_end_time = time.time()
    total_training_time = total_end_time - total_start_time

    print(f"\nâ±ï¸ è¶…å¿«é€Ÿ aMLP è¨“ç·´æ™‚é–“çµ±è¨ˆ:")
    print(
        f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.1f}s ({total_training_time/60:.1f}min)"
    )
    print(f"   â€¢ å¯¦éš›è¨“ç·´epochs: {len(train_losses)} / {epochs}")
    print(f"   â€¢ å¹³å‡æ¯epoch: {np.mean(epoch_times):.1f}s")
    print(f"   â€¢ æœ€ä½³ aMLP é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
    print(f"   ğŸ”¥ æ³¨æ„åŠ›æ©Ÿåˆ¶æ•ˆæœ: é æœŸæ¯”åŸ gMLP æå‡ 2-5%")

    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    if best_val_acc > 0:
        model.load_state_dict(torch.load("best_ultra_small_amlp_model.pth"))
        print("   â€¢ å·²è¼‰å…¥æœ€ä½³ aMLP æ¨¡å‹æ¬Šé‡")

    return (
        train_losses,
        train_accs,
        val_accs,
        val_losses,
        epoch_times,
        total_training_time,
    )


def quick_validate_with_loss(model, testloader, device, criterion):
    """å¿«é€Ÿé©—è­‰ï¼ˆåŒæ™‚è¨ˆç®—æº–ç¢ºç‡å’Œæå¤±ï¼‰"""
    model.eval()
    correct = 0
    total = 0
    running_loss = 0.0
    num_batches = 0

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            num_batches += 1

            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    accuracy = 100.0 * correct / total
    avg_loss = running_loss / num_batches if num_batches > 0 else 0.0

    return accuracy, avg_loss


def evaluate_ultra_amlp_model(model, testloader, device, classes):
    """è©•ä¼°è¶…ç¸®å°ç‰ˆ aMLP æ¨¡å‹"""
    print("\nğŸ“Š è©•ä¼°è¶…ç¸®å°ç‰ˆ aMLP æ¨¡å‹...")

    model.eval()
    all_predictions = []
    all_labels = []
    correct = 0
    total = 0
    class_correct = [0] * 10
    class_total = [0] * 10

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)

            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            for i in range(labels.size(0)):
                label = labels[i]
                class_correct[label] += (predicted[i] == label).item()
                class_total[label] += 1

    overall_acc = 100.0 * correct / total
    print(f"   âœ“ aMLP æ•´é«”æº–ç¢ºç‡: {overall_acc:.2f}%")

    # ç°¡åŒ–çš„çµæœå¯è¦–åŒ–
    plt.figure(figsize=(14, 8))

    # å„é¡åˆ¥æº–ç¢ºç‡
    plt.subplot(2, 2, 1)
    class_accs = [
        100.0 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0
        for i in range(10)
    ]
    bars = plt.bar(classes, class_accs, color=plt.cm.tab10(np.arange(10)))
    plt.title("Ultra-Small aMLP: Class Accuracy", fontweight="bold", fontsize=14)
    plt.ylabel("Accuracy (%)")
    plt.xticks(rotation=45)
    plt.grid(axis="y", alpha=0.3)

    for bar, acc in zip(bars, class_accs):
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{acc:.1f}%",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # æ··æ·†çŸ©é™£
    plt.subplot(2, 2, 2)
    cm = confusion_matrix(all_labels, all_predictions)
    sns.heatmap(
        cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes
    )
    plt.title("aMLP Confusion Matrix", fontweight="bold", fontsize=14)
    plt.xlabel("Predicted")
    plt.ylabel("True")

    # æº–ç¢ºç‡åˆ†ä½ˆ
    plt.subplot(2, 2, 3)
    plt.hist(class_accs, bins=8, alpha=0.7, color="lightcoral", edgecolor="black")
    plt.title("aMLP Class Accuracy Distribution", fontweight="bold", fontsize=14)
    plt.xlabel("Accuracy (%)")
    plt.ylabel("Count")
    plt.grid(axis="y", alpha=0.3)

    # æ¨¡å‹çµ±è¨ˆ
    plt.subplot(2, 2, 4)
    stats_text = f"""aMLP Model Statistics:
â€¢ Overall Accuracy: {overall_acc:.2f}%
â€¢ Best Class: {classes[np.argmax(class_accs)]} ({max(class_accs):.1f}%)
â€¢ Worst Class: {classes[np.argmin(class_accs)]} ({min(class_accs):.1f}%)
â€¢ Average Class Accuracy: {np.mean(class_accs):.1f}%
â€¢ Standard Deviation: {np.std(class_accs):.1f}%
â€¢ ğŸ”¥ Attention Enhanced: aMLP"""

    plt.text(
        0.1,
        0.5,
        stats_text,
        fontsize=11,
        verticalalignment="center",
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.8),
    )
    plt.axis("off")
    plt.title("aMLP Model Statistics", fontweight="bold", fontsize=14)

    plt.tight_layout()
    plt.savefig("ultra_small_amlp_results.png", dpi=300, bbox_inches="tight")
    plt.show()

    return overall_acc


def plot_ultra_amlp_training_history(
    train_losses, train_accs, val_accs, val_losses, epoch_times
):
    """ç¹ªè£½è¶…å¿«é€Ÿ aMLP è¨“ç·´æ­·å²"""
    print("\nğŸ“ˆ ç¹ªè£½è¶…å¿«é€Ÿ aMLP è¨“ç·´æ­·å²...")

    plt.figure(figsize=(18, 5))

    # æå¤±æ›²ç·šæ¯”è¼ƒ
    plt.subplot(1, 4, 1)
    plt.plot(train_losses, "b-", linewidth=2, label="Training Loss")
    plt.plot(val_losses, "r-", linewidth=2, label="Validation Loss")
    plt.title("aMLP Loss Comparison", fontweight="bold", fontsize=14)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # æº–ç¢ºç‡æ›²ç·š
    plt.subplot(1, 4, 2)
    plt.plot(train_accs, "g-", linewidth=2, label="Training Acc", alpha=0.8)
    plt.plot(val_accs, "r-", linewidth=2, label="Validation Acc")
    plt.title("aMLP Accuracy Curves", fontweight="bold", fontsize=14)
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.grid(True, alpha=0.3)
    plt.legend()

    # éæ“¬åˆç›£æ§
    plt.subplot(1, 4, 3)
    acc_diff = np.array(train_accs) - np.array(val_accs)
    plt.plot(acc_diff, "purple", linewidth=2, label="Accuracy Diff")
    plt.axhline(y=5, color="orange", linestyle="--", alpha=0.7, label="Warning (5%)")
    plt.title("aMLP Overfitting Monitor", fontweight="bold", fontsize=14)
    plt.xlabel("Epoch")
    plt.ylabel("Training - Validation (%)")
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color="black", linestyle="--", alpha=0.5)
    plt.legend()

    # è¨“ç·´æ™‚é–“
    plt.subplot(1, 4, 4)
    plt.plot(epoch_times, "darkorange", linewidth=2, marker="o", markersize=3)
    avg_time = np.mean(epoch_times)
    plt.axhline(
        y=avg_time,
        color="red",
        linestyle="--",
        alpha=0.7,
        label=f"Avg: {avg_time:.1f}s",
    )
    plt.title("aMLP Training Time per Epoch", fontweight="bold", fontsize=14)
    plt.xlabel("Epoch")
    plt.ylabel("Time (seconds)")
    plt.grid(True, alpha=0.3)
    plt.legend()

    plt.tight_layout()
    plt.savefig("ultra_small_amlp_training_history.png", dpi=300, bbox_inches="tight")
    plt.show()


def main():
    """ä¸»å‡½æ•¸ - aMLP è‡ªç”±é¸æ“‡ç‰ˆæœ¬"""
    print("ğŸš€ è¶…ç¸®å°ç‰ˆ aMLP åœ–åƒåˆ†é¡è¨“ç·´ - æ³¨æ„åŠ›å¢å¼·ç‰ˆæœ¬")
    print("ğŸ”¥ é™„åŠ æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œé æœŸæº–ç¢ºç‡æå‡ 2-5%")
    print("=" * 75)

    try:
        # ç”¨æˆ¶é¸æ“‡ aMLP æ¨¡å‹
        model_size = get_user_model_choice()

        # ç”¨æˆ¶è¨­ç½®è¨“ç·´åƒæ•¸
        training_params = get_training_parameters()

        # æ•¸æ“šåŠ è¼‰
        trainloader, testloader, classes = load_cifar10_data_ultrafast(
            quick_test=training_params["quick_test"]
        )

        # å‰µå»ºé¸å®šçš„ aMLP æ¨¡å‹
        model, device = create_ultra_small_amlp_model(model_size=model_size)

        # é–‹å§‹ aMLP è¨“ç·´
        print(f"\nğŸ¬ é–‹å§‹è¨“ç·´ aMLP-{model_size} æ¨¡å‹...")
        print("ğŸ”¥ æ³¨æ„åŠ›æ©Ÿåˆ¶å·²å•Ÿç”¨ï¼Œé æœŸæ›´ä½³è¨“ç·´æ•ˆæœ...")
        train_losses, train_accs, val_accs, val_losses, epoch_times, total_time = (
            train_ultra_fast_amlp(
                model,
                trainloader,
                testloader,
                device,
                epochs=training_params["epochs"],
                enable_early_stop=training_params["enable_early_stop"],
            )
        )

        # aMLP çµæœå¯è¦–åŒ–
        plot_ultra_amlp_training_history(
            train_losses, train_accs, val_accs, val_losses, epoch_times
        )

        # aMLP æ¨¡å‹è©•ä¼°
        final_acc = evaluate_ultra_amlp_model(model, testloader, device, classes)

        # aMLP è¨“ç·´ç¸½çµ
        print(f"\nğŸ‰ aMLP è¨“ç·´å®Œæˆç¸½çµ:")
        print(f"   â€¢ é¸æ“‡æ¨¡å‹: aMLP-{model_size} (æ³¨æ„åŠ›å¢å¼·)")
        print(f"   â€¢ æœ€çµ‚æº–ç¢ºç‡: {final_acc:.2f}%")
        print(f"   â€¢ ç¸½è¨“ç·´æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
        print(f"   â€¢ å¹³å‡æ¯epoch: {np.mean(epoch_times):.1f} ç§’")
        print(f"   â€¢ å¯¦éš›è¨“ç·´è¼ªæ•¸: {len(train_losses)}/{training_params['epochs']}")
        print(
            f"   â€¢ æ—©åœç‹€æ…‹: {'å•Ÿç”¨' if training_params['enable_early_stop'] else 'é—œé–‰'}"
        )
        print(f"   ğŸ”¥ æ³¨æ„åŠ›æ©Ÿåˆ¶: å·²å•Ÿç”¨ (aMLP)")

        # æ€§èƒ½è©•ä¼°
        if final_acc >= 82:
            print(f"   ğŸ† å„ªç§€æ€§èƒ½ï¼aMLP æ•ˆæœé¡¯è‘—")
        elif final_acc >= 78:
            print(f"   âœ… è‰¯å¥½æ€§èƒ½ï¼æ³¨æ„åŠ›æ©Ÿåˆ¶æœ‰æ•ˆ")
        else:
            print(f"   âš ï¸  å¯å˜—è©¦æ›´å¤§æ¨¡å‹æˆ–èª¿æ•´åƒæ•¸")

        # è©¢å•æ˜¯å¦ç¹¼çºŒè¨“ç·´å…¶ä»–æ¨¡å‹
        print(f"\n" + "=" * 75)
        continue_choice = input("ğŸ”„ æ˜¯å¦è¦è¨“ç·´å…¶ä»– aMLP æ¨¡å‹? (y/n): ").strip().lower()
        if continue_choice in ["y", "yes"]:
            print("\n" + "ğŸ”„ é‡æ–°é–‹å§‹..." + "\n")
            main()  # éè¿´èª¿ç”¨é‡æ–°é–‹å§‹

    except KeyboardInterrupt:
        print("\n\nâŒ ç”¨æˆ¶ä¸­æ–·ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("   è«‹æª¢æŸ¥è¼¸å…¥ä¸¦é‡è©¦")


if __name__ == "__main__":
    main()

🏗️  可用的 aMLP 模型架構 (附加注意力機制)
==========================================================================================
編號   名稱     深度     維度     FFN   注意力      參數       時間         風險       描述
------------------------------------------------------------------------------------------
1    Test   8      50     2     64       0.18M    <40秒       極低       超極速測試模型 + 注意力
2    Nano   6      64     2     64       0.25M    ~1.5分鐘     很低       極小快速模型 + 注意力
3    XS     8      80     3     40       0.35M    ~2.5分鐘     低        超小平衡模型 + 注意力
4    S      12     128    3     64       0.85M    ~6分鐘       中等       小型性能模型 + 注意力
5    M      16     160    4     80       1.65M    ~12分鐘      較高       中型高性能模型 + 注意力
6    L      30     128    6     64       2.15M    ~18分鐘      很高       大型頂級模型 + 注意力
------------------------------------------------------------------------------------------
💡 推薦選擇 (aMLP 版本):
   🚀 快速測試: Test (1) 或 Nano (2) - 注意力增強
   ⚖️  平衡性能: XS (3) 或 S (4) - 最推薦，效能提升明顯
   🎯 高性能: M (5) 或 L (6) - 頂級注意力效能
   🔥 注意力機制將顯著提升準確率，建議從 XS 開始嘗試
==========================================================================================

🤖 請選擇要使用的 aMLP 模型:
   輸入編號 (1-6) 或模型名稱 (Test/Nano/XS/S/M/L): 1

✅ 您選擇了: aMLP-Test 模型
   📋 模型詳情:
      • 深度: 8 層
      • 維度: 50
      • FFN倍數: 2
      • 注意力維度: 64 (🔥 性能提升關鍵)
      • 預估參數: 0.18M
      • 預估時間: <40秒
      • 過擬合風險: 極低
      • 描述: 超極速測試模型 + 注意力

   確認使用 aMLP-Test 模型嗎? (y/n, 預設=y): y

============================================================
⚙️  aMLP 訓練參數設置
============================================================

📦 數據集模式選擇:
   1. 快速模式 (50K訓練 + 10K測試) - 推薦
   2. 完整模式 (50K訓練 + 10K測試)
   選擇模式 (1/2, 預設=1): 2

🏋️  訓練輪數 (預設=50): 100

🛡️  過擬合保護:
   1. 啟用早停機制 (推薦)
   2. 關閉早停機制
   選擇 (1/2, 預設=1): 1

✅ aMLP 訓練參數確認:
   📦 數據模式: 完整模式
   🏋️  訓練輪數: 100
   🛡️  早停機制: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
📦 加載超快速 CIFAR-10 數據集...
   ✓ 訓練樣本: 50000
   ✓ 測試樣本: 10000
   ✓ Batch大小: 128

🏗️ 創建超縮小版 aMLP-Test 模型...
   ⚡ CPU模式：已設置4個線程

✅ 超縮小版 aMLP-Test 模型創建完成
   ✓ 設備: cpu
   ✓ 實際參數數量: 104,522 (0.10M)
   ✓ 目標參數預期: 0.18M
   ✓ 注意力維度: 32 (🔥 aMLP 增強)
   ✓ 架構配置: depth=4, dim=64, ff_mult=2
   🔥 注意力機制已啟用，預期準確率提升 2-5%

🎬 開始訓練 aMLP-Test 模型...
🔥 注意力機制已啟用，預期更佳訓練效果...

🏋️ 開始超快速 aMLP 訓練 (100 個 epochs)...
   🛡️  啟用過擬合早停保護
   🔥 注意力機制已啟用 - 預期更佳收斂效果

Epoch 1/100, LR: 0.002800
   批次 100/391: 損失=1.9124, 準確率=31.94%
   批次 200/391: 損失=1.7847, 準確率=38.24%
   批次 300/391: 損失=1.7135, 準確率=41.86%
   批次 391/391: 損失=1.6712, 準確率=43.91%
Epoch 1 完成: 訓練=43.91%, 驗證=53.48%, 時間=87.6s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 53.48%

Epoch 2/100, LR: 0.002799
   批次 100/391: 損失=1.4704, 準確率=53.70%
   批次 200/391: 損失=1.4434, 準確率=54.98%
   批次 300/391: 損失=1.4280, 準確率=55.67%
   批次 391/391: 損失=1.4174, 準確率=56.19%
Epoch 2 完成: 訓練=56.19%, 驗證=58.36%, 時間=85.4s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 58.36%

Epoch 3/100, LR: 0.002797
   批次 100/391: 損失=1.3358, 準確率=60.40%
   批次 200/391: 損失=1.3319, 準確率=60.16%
   批次 300/391: 損失=1.3219, 準確率=60.73%
   批次 391/391: 損失=1.3181, 準確率=60.98%
Epoch 3 完成: 訓練=60.98%, 驗證=62.53%, 時間=85.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 62.53%

Epoch 4/100, LR: 0.002794
   批次 100/391: 損失=1.2718, 準確率=63.02%
   批次 200/391: 損失=1.2584, 準確率=63.38%
   批次 300/391: 損失=1.2547, 準確率=63.76%
   批次 391/391: 損失=1.2547, 準確率=63.76%
Epoch 4 完成: 訓練=63.76%, 驗證=64.53%, 時間=84.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 64.53%

Epoch 5/100, LR: 0.002789
   批次 100/391: 損失=1.2292, 準確率=65.05%
   批次 200/391: 損失=1.2155, 準確率=65.52%
   批次 300/391: 損失=1.2093, 準確率=65.69%
   批次 391/391: 損失=1.2035, 準確率=66.05%
Epoch 5 完成: 訓練=66.05%, 驗證=66.26%, 時間=90.4s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 66.26%

Epoch 6/100, LR: 0.002783
   批次 100/391: 損失=1.1630, 準確率=68.36%
   批次 200/391: 損失=1.1658, 準確率=68.03%
   批次 300/391: 損失=1.1665, 準確率=67.85%
   批次 391/391: 損失=1.1683, 準確率=67.79%
Epoch 6 完成: 訓練=67.79%, 驗證=69.34%, 時間=86.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 69.34%

Epoch 7/100, LR: 0.002775
   批次 100/391: 損失=1.1208, 準確率=70.00%
   批次 200/391: 損失=1.1317, 準確率=69.32%
   批次 300/391: 損失=1.1325, 準確率=69.44%
   批次 391/391: 損失=1.1363, 準確率=69.30%
Epoch 7 完成: 訓練=69.30%, 驗證=68.06%, 時間=84.1s

Epoch 8/100, LR: 0.002766
   批次 100/391: 損失=1.0967, 準確率=71.24%
   批次 200/391: 損失=1.1015, 準確率=70.90%
   批次 300/391: 損失=1.1053, 準確率=70.78%
   批次 391/391: 損失=1.1083, 準確率=70.61%
Epoch 8 完成: 訓練=70.61%, 驗證=69.88%, 時間=87.6s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 69.88%

Epoch 9/100, LR: 0.002756
   批次 100/391: 損失=1.0992, 準確率=70.74%
   批次 200/391: 損失=1.0900, 準確率=71.38%
   批次 300/391: 損失=1.0867, 準確率=71.46%
   批次 391/391: 損失=1.0901, 準確率=71.27%
Epoch 9 完成: 訓練=71.27%, 驗證=70.97%, 時間=83.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 70.97%

Epoch 10/100, LR: 0.002745
   批次 100/391: 損失=1.0697, 準確率=72.20%
   批次 200/391: 損失=1.0716, 準確率=72.24%
   批次 300/391: 損失=1.0753, 準確率=72.10%
   批次 391/391: 損失=1.0744, 準確率=72.15%
Epoch 10 完成: 訓練=72.15%, 驗證=71.13%, 時間=85.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 71.13%

Epoch 11/100, LR: 0.002732
   批次 100/391: 損失=1.0481, 準確率=73.12%
   批次 200/391: 損失=1.0514, 準確率=72.95%
   批次 300/391: 損失=1.0521, 準確率=73.00%
   批次 391/391: 損失=1.0539, 準確率=72.96%
Epoch 11 完成: 訓練=72.96%, 驗證=71.21%, 時間=84.7s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 71.21%

Epoch 12/100, LR: 0.002717
   批次 100/391: 損失=1.0302, 準確率=73.84%
   批次 200/391: 損失=1.0345, 準確率=73.74%
   批次 300/391: 損失=1.0382, 準確率=73.72%
   批次 391/391: 損失=1.0348, 準確率=73.86%
Epoch 12 完成: 訓練=73.86%, 驗證=72.71%, 時間=83.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 72.71%

Epoch 13/100, LR: 0.002702
   批次 100/391: 損失=1.0107, 準確率=75.02%
   批次 200/391: 損失=1.0128, 準確率=75.08%
   批次 300/391: 損失=1.0143, 準確率=74.97%
   批次 391/391: 損失=1.0145, 準確率=74.96%
Epoch 13 完成: 訓練=74.96%, 驗證=73.97%, 時間=89.8s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 73.97%

Epoch 14/100, LR: 0.002685
   批次 100/391: 損失=1.0002, 準確率=75.41%
   批次 200/391: 損失=0.9899, 準確率=76.08%
   批次 300/391: 損失=0.9957, 準確率=75.70%
   批次 391/391: 損失=0.9965, 準確率=75.61%
Epoch 14 完成: 訓練=75.61%, 驗證=74.29%, 時間=94.0s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 74.29%

Epoch 15/100, LR: 0.002667
   批次 100/391: 損失=0.9829, 準確率=76.24%
   批次 200/391: 損失=0.9844, 準確率=76.24%
   批次 300/391: 損失=0.9858, 準確率=76.20%
   批次 391/391: 損失=0.9877, 準確率=76.09%
Epoch 15 完成: 訓練=76.09%, 驗證=73.58%, 時間=87.0s

Epoch 16/100, LR: 0.002648
   批次 100/391: 損失=0.9741, 準確率=76.91%
   批次 200/391: 損失=0.9753, 準確率=76.85%
   批次 300/391: 損失=0.9764, 準確率=76.65%
   批次 391/391: 損失=0.9782, 準確率=76.45%
Epoch 16 完成: 訓練=76.45%, 驗證=74.44%, 時間=86.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 74.44%

Epoch 17/100, LR: 0.002627
   批次 100/391: 損失=0.9580, 準確率=77.16%
   批次 200/391: 損失=0.9620, 準確率=76.95%
   批次 300/391: 損失=0.9663, 準確率=76.80%
   批次 391/391: 損失=0.9682, 準確率=76.80%
Epoch 17 完成: 訓練=76.80%, 驗證=74.34%, 時間=86.4s

Epoch 18/100, LR: 0.002606
   批次 100/391: 損失=0.9484, 準確率=77.70%
   批次 200/391: 損失=0.9528, 準確率=77.71%
   批次 300/391: 損失=0.9562, 準確率=77.51%
   批次 391/391: 損失=0.9545, 準確率=77.56%
Epoch 18 完成: 訓練=77.56%, 驗證=75.00%, 時間=90.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.00%

Epoch 19/100, LR: 0.002583
   批次 100/391: 損失=0.9303, 準確率=78.52%
   批次 200/391: 損失=0.9430, 準確率=78.14%
   批次 300/391: 損失=0.9458, 準確率=77.89%
   批次 391/391: 損失=0.9466, 準確率=77.77%
Epoch 19 完成: 訓練=77.77%, 驗證=75.58%, 時間=88.4s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.58%

Epoch 20/100, LR: 0.002559
   批次 100/391: 損失=0.9296, 準確率=78.81%
   批次 200/391: 損失=0.9350, 準確率=78.50%
   批次 300/391: 損失=0.9394, 準確率=78.16%
   批次 391/391: 損失=0.9379, 準確率=78.20%
Epoch 20 完成: 訓練=78.20%, 驗證=75.62%, 時間=88.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.62%

Epoch 21/100, LR: 0.002533
   批次 100/391: 損失=0.9268, 準確率=78.72%
   批次 200/391: 損失=0.9329, 準確率=78.66%
   批次 300/391: 損失=0.9322, 準確率=78.77%
   批次 391/391: 損失=0.9331, 準確率=78.63%
Epoch 21 完成: 訓練=78.63%, 驗證=75.07%, 時間=91.2s

Epoch 22/100, LR: 0.002507
   批次 100/391: 損失=0.9161, 準確率=79.30%
   批次 200/391: 損失=0.9218, 準確率=78.75%
   批次 300/391: 損失=0.9227, 準確率=78.76%
   批次 391/391: 損失=0.9222, 準確率=78.79%
Epoch 22 完成: 訓練=78.79%, 驗證=75.51%, 時間=105.9s

Epoch 23/100, LR: 0.002480
   批次 100/391: 損失=0.9098, 準確率=79.31%
   批次 200/391: 損失=0.9085, 準確率=79.58%
   批次 300/391: 損失=0.9136, 準確率=79.31%
   批次 391/391: 損失=0.9174, 準確率=79.19%
Epoch 23 完成: 訓練=79.19%, 驗證=75.89%, 時間=86.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.89%

Epoch 24/100, LR: 0.002451
   批次 100/391: 損失=0.8984, 準確率=79.85%
   批次 200/391: 損失=0.8984, 準確率=79.87%
   批次 300/391: 損失=0.9039, 準確率=79.70%
   批次 391/391: 損失=0.9081, 準確率=79.48%
Epoch 24 完成: 訓練=79.48%, 驗證=75.58%, 時間=81.7s

Epoch 25/100, LR: 0.002422
   批次 100/391: 損失=0.8913, 準確率=80.34%
   批次 200/391: 損失=0.8928, 準確率=80.15%
   批次 300/391: 損失=0.8955, 準確率=80.00%
   批次 391/391: 損失=0.9006, 準確率=79.86%
Epoch 25 完成: 訓練=79.86%, 驗證=76.25%, 時間=83.7s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.25%

Epoch 26/100, LR: 0.002391
   批次 100/391: 損失=0.8965, 準確率=80.02%
   批次 200/391: 損失=0.9003, 準確率=79.87%
   批次 300/391: 損失=0.8980, 準確率=80.01%
   批次 391/391: 損失=0.8990, 準確率=79.97%
Epoch 26 完成: 訓練=79.97%, 驗證=76.40%, 時間=85.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.40%

Epoch 27/100, LR: 0.002360
   批次 100/391: 損失=0.8847, 準確率=80.31%
   批次 200/391: 損失=0.8881, 準確率=80.39%
   批次 300/391: 損失=0.8862, 準確率=80.50%
   批次 391/391: 損失=0.8886, 準確率=80.38%
Epoch 27 完成: 訓練=80.38%, 驗證=75.89%, 時間=82.0s

Epoch 28/100, LR: 0.002327
   批次 100/391: 損失=0.8748, 準確率=81.01%
   批次 200/391: 損失=0.8796, 準確率=80.88%
   批次 300/391: 損失=0.8864, 準確率=80.49%
   批次 391/391: 損失=0.8885, 準確率=80.37%
Epoch 28 完成: 訓練=80.37%, 驗證=76.16%, 時間=83.1s

Epoch 29/100, LR: 0.002294
   批次 100/391: 損失=0.8685, 準確率=81.42%
   批次 200/391: 損失=0.8710, 準確率=81.41%
   批次 300/391: 損失=0.8755, 準確率=81.00%
   批次 391/391: 損失=0.8774, 準確率=81.01%
Epoch 29 完成: 訓練=81.01%, 驗證=75.24%, 時間=85.3s
   📊 訓練-驗證差異: 5.77%

Epoch 30/100, LR: 0.002260
   批次 100/391: 損失=0.8626, 準確率=81.30%
   批次 200/391: 損失=0.8666, 準確率=81.32%
   批次 300/391: 損失=0.8695, 準確率=81.21%
   批次 391/391: 損失=0.8702, 準確率=81.19%
Epoch 30 完成: 訓練=81.19%, 驗證=77.04%, 時間=85.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.04%

Epoch 31/100, LR: 0.002225
   批次 100/391: 損失=0.8566, 準確率=81.99%
   批次 200/391: 損失=0.8559, 準確率=81.81%
   批次 300/391: 損失=0.8613, 準確率=81.58%
   批次 391/391: 損失=0.8667, 準確率=81.25%
Epoch 31 完成: 訓練=81.25%, 驗證=76.84%, 時間=81.5s

Epoch 32/100, LR: 0.002189
   批次 100/391: 損失=0.8568, 準確率=81.79%
   批次 200/391: 損失=0.8570, 準確率=81.90%
   批次 300/391: 損失=0.8600, 準確率=81.66%
   批次 391/391: 損失=0.8606, 準確率=81.60%
Epoch 32 完成: 訓練=81.60%, 驗證=76.59%, 時間=83.1s
   📊 訓練-驗證差異: 5.01%

Epoch 33/100, LR: 0.002152
   批次 100/391: 損失=0.8384, 準確率=82.68%
   批次 200/391: 損失=0.8471, 準確率=82.16%
   批次 300/391: 損失=0.8530, 準確率=81.97%
   批次 391/391: 損失=0.8567, 準確率=81.80%
Epoch 33 完成: 訓練=81.80%, 驗證=77.32%, 時間=84.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.32%

Epoch 34/100, LR: 0.002115
   批次 100/391: 損失=0.8418, 準確率=82.77%
   批次 200/391: 損失=0.8466, 準確率=82.54%
   批次 300/391: 損失=0.8470, 準確率=82.47%
   批次 391/391: 損失=0.8489, 準確率=82.24%
Epoch 34 完成: 訓練=82.24%, 驗證=76.96%, 時間=84.8s
   📊 訓練-驗證差異: 5.28%

Epoch 35/100, LR: 0.002077
   批次 100/391: 損失=0.8408, 準確率=82.29%
   批次 200/391: 損失=0.8459, 準確率=82.27%
   批次 300/391: 損失=0.8458, 準確率=82.25%
   批次 391/391: 損失=0.8462, 準確率=82.24%
Epoch 35 完成: 訓練=82.24%, 驗證=76.97%, 時間=86.6s
   📊 訓練-驗證差異: 5.27%

Epoch 36/100, LR: 0.002038
   批次 100/391: 損失=0.8336, 準確率=82.98%
   批次 200/391: 損失=0.8374, 準確率=82.80%
   批次 300/391: 損失=0.8397, 準確率=82.65%
   批次 391/391: 損失=0.8400, 準確率=82.58%
Epoch 36 完成: 訓練=82.58%, 驗證=76.96%, 時間=86.7s
   📊 訓練-驗證差異: 5.62%

Epoch 37/100, LR: 0.001998
   批次 100/391: 損失=0.8297, 準確率=83.02%
   批次 200/391: 損失=0.8336, 準確率=82.79%
   批次 300/391: 損失=0.8353, 準確率=82.71%
   批次 391/391: 損失=0.8383, 準確率=82.59%
Epoch 37 完成: 訓練=82.59%, 驗證=77.09%, 時間=81.1s
   📊 訓練-驗證差異: 5.50%

Epoch 38/100, LR: 0.001958
   批次 100/391: 損失=0.8248, 準確率=83.40%
   批次 200/391: 損失=0.8251, 準確率=83.50%
   批次 300/391: 損失=0.8276, 準確率=83.34%
   批次 391/391: 損失=0.8311, 準確率=83.15%
Epoch 38 完成: 訓練=83.15%, 驗證=77.48%, 時間=83.9s
   📊 訓練-驗證差異: 5.67%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.48%

Epoch 39/100, LR: 0.001918
   批次 100/391: 損失=0.8124, 準確率=83.85%
   批次 200/391: 損失=0.8186, 準確率=83.68%
   批次 300/391: 損失=0.8194, 準確率=83.75%
   批次 391/391: 損失=0.8221, 準確率=83.52%
Epoch 39 完成: 訓練=83.52%, 驗證=77.48%, 時間=83.6s
   📊 訓練-驗證差異: 6.04%

Epoch 40/100, LR: 0.001877
   批次 100/391: 損失=0.8016, 準確率=84.19%
   批次 200/391: 損失=0.8115, 準確率=83.77%
   批次 300/391: 損失=0.8179, 準確率=83.48%
   批次 391/391: 損失=0.8193, 準確率=83.47%
Epoch 40 完成: 訓練=83.47%, 驗證=77.25%, 時間=82.0s
   📊 訓練-驗證差異: 6.22%

Epoch 41/100, LR: 0.001835
   批次 100/391: 損失=0.8177, 準確率=83.95%
   批次 200/391: 損失=0.8142, 準確率=83.86%
   批次 300/391: 損失=0.8164, 準確率=83.75%
   批次 391/391: 損失=0.8161, 準確率=83.78%
Epoch 41 完成: 訓練=83.78%, 驗證=76.35%, 時間=87.5s
   📊 訓練-驗證差異: 7.43%

Epoch 42/100, LR: 0.001793
   批次 100/391: 損失=0.7991, 準確率=84.73%
   批次 200/391: 損失=0.8053, 準確率=84.36%
   批次 300/391: 損失=0.8097, 準確率=84.09%
   批次 391/391: 損失=0.8106, 準確率=84.05%
Epoch 42 完成: 訓練=84.05%, 驗證=77.37%, 時間=82.2s
   📊 訓練-驗證差異: 6.68%

Epoch 43/100, LR: 0.001751
   批次 100/391: 損失=0.7998, 準確率=84.38%
   批次 200/391: 損失=0.8005, 準確率=84.46%
   批次 300/391: 損失=0.8041, 準確率=84.39%
   批次 391/391: 損失=0.8073, 準確率=84.27%
Epoch 43 完成: 訓練=84.27%, 驗證=77.13%, 時間=85.2s
   📊 訓練-驗證差異: 7.14%

Epoch 44/100, LR: 0.001709
   批次 100/391: 損失=0.7970, 準確率=84.65%
   批次 200/391: 損失=0.8023, 準確率=84.25%
   批次 300/391: 損失=0.8027, 準確率=84.28%
   批次 391/391: 損失=0.8030, 準確率=84.29%
Epoch 44 完成: 訓練=84.29%, 驗證=77.58%, 時間=86.0s
   📊 訓練-驗證差異: 6.71%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.58%

Epoch 45/100, LR: 0.001666
   批次 100/391: 損失=0.7826, 準確率=84.94%
   批次 200/391: 損失=0.7899, 準確率=84.88%
   批次 300/391: 損失=0.7946, 準確率=84.75%
   批次 391/391: 損失=0.7981, 準確率=84.49%
Epoch 45 完成: 訓練=84.49%, 驗證=76.86%, 時間=82.9s
   📊 訓練-驗證差異: 7.63%

Epoch 46/100, LR: 0.001622
   批次 100/391: 損失=0.7871, 準確率=85.41%
   批次 200/391: 損失=0.7897, 準確率=85.28%
   批次 300/391: 損失=0.7919, 準確率=85.06%
   批次 391/391: 損失=0.7926, 準確率=84.91%
Epoch 46 完成: 訓練=84.91%, 驗證=77.21%, 時間=85.3s
   📊 訓練-驗證差異: 7.70%

Epoch 47/100, LR: 0.001579
   批次 100/391: 損失=0.7796, 準確率=85.54%
   批次 200/391: 損失=0.7815, 準確率=85.30%
   批次 300/391: 損失=0.7852, 準確率=85.09%
   批次 391/391: 損失=0.7882, 準確率=84.92%
Epoch 47 完成: 訓練=84.92%, 驗證=77.55%, 時間=85.0s
   📊 訓練-驗證差異: 7.37%

Epoch 48/100, LR: 0.001535
   批次 100/391: 損失=0.7660, 準確率=85.98%
   批次 200/391: 損失=0.7759, 準確率=85.40%
   批次 300/391: 損失=0.7786, 準確率=85.33%
   批次 391/391: 損失=0.7823, 準確率=85.18%
Epoch 48 完成: 訓練=85.18%, 驗證=77.59%, 時間=82.9s
   📊 訓練-驗證差異: 7.59%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.59%

Epoch 49/100, LR: 0.001492
   批次 100/391: 損失=0.7689, 準確率=85.70%
   批次 200/391: 損失=0.7698, 準確率=85.71%
   批次 300/391: 損失=0.7724, 準確率=85.70%
   批次 391/391: 損失=0.7765, 準確率=85.59%
Epoch 49 完成: 訓練=85.59%, 驗證=77.19%, 時間=82.2s
   ⚠️  過擬合警告: 差異 8.40% > 閾值 8% (1/8)
   📊 訓練-驗證差異: 8.40%

Epoch 50/100, LR: 0.001448
   批次 100/391: 損失=0.7714, 準確率=86.01%
   批次 200/391: 損失=0.7724, 準確率=85.89%
   批次 300/391: 損失=0.7744, 準確率=85.68%
   批次 391/391: 損失=0.7757, 準確率=85.62%
Epoch 50 完成: 訓練=85.62%, 驗證=77.72%, 時間=85.6s
   📊 訓練-驗證差異: 7.90%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.72%

Epoch 51/100, LR: 0.001404
   批次 100/391: 損失=0.7651, 準確率=86.27%
   批次 200/391: 損失=0.7680, 準確率=85.89%
   批次 300/391: 損失=0.7695, 準確率=85.95%
   批次 391/391: 損失=0.7703, 準確率=85.84%
Epoch 51 完成: 訓練=85.84%, 驗證=77.47%, 時間=85.7s
   ⚠️  過擬合警告: 差異 8.37% > 閾值 8% (1/8)
   📊 訓練-驗證差異: 8.37%

Epoch 52/100, LR: 0.001360
   批次 100/391: 損失=0.7521, 準確率=86.39%
   批次 200/391: 損失=0.7583, 準確率=86.17%
   批次 300/391: 損失=0.7619, 準確率=86.08%
   批次 391/391: 損失=0.7667, 準確率=85.85%
Epoch 52 完成: 訓練=85.85%, 驗證=77.85%, 時間=82.8s
   ⚠️  過擬合警告: 差異 8.00% > 閾值 8% (2/8)
   📊 訓練-驗證差異: 8.00%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.85%

Epoch 53/100, LR: 0.001316
   批次 100/391: 損失=0.7565, 準確率=86.42%
   批次 200/391: 損失=0.7557, 準確率=86.61%
   批次 300/391: 損失=0.7588, 準確率=86.45%
   批次 391/391: 損失=0.7601, 準確率=86.38%
Epoch 53 完成: 訓練=86.38%, 驗證=77.28%, 時間=86.3s
   ⚠️  過擬合警告: 差異 9.10% > 閾值 8% (3/8)
   📊 訓練-驗證差異: 9.10%

Epoch 54/100, LR: 0.001273
   批次 100/391: 損失=0.7509, 準確率=86.59%
   批次 200/391: 損失=0.7577, 準確率=86.29%
   批次 300/391: 損失=0.7554, 準確率=86.45%
   批次 391/391: 損失=0.7589, 準確率=86.29%
Epoch 54 完成: 訓練=86.29%, 驗證=77.58%, 時間=85.1s
   ⚠️  過擬合警告: 差異 8.71% > 閾值 8% (4/8)
   📊 訓練-驗證差異: 8.71%

Epoch 55/100, LR: 0.001229
   批次 100/391: 損失=0.7492, 準確率=86.98%
   批次 200/391: 損失=0.7500, 準確率=86.76%
   批次 300/391: 損失=0.7522, 準確率=86.68%
   批次 391/391: 損失=0.7571, 準確率=86.51%
Epoch 55 完成: 訓練=86.51%, 驗證=76.98%, 時間=82.8s
   ⚠️  過擬合警告: 差異 9.53% > 閾值 8% (5/8)
   📊 訓練-驗證差異: 9.53%

Epoch 56/100, LR: 0.001186
   批次 100/391: 損失=0.7369, 準確率=87.52%
   批次 200/391: 損失=0.7415, 準確率=87.25%
   批次 300/391: 損失=0.7453, 準確率=87.13%
   批次 391/391: 損失=0.7489, 準確率=86.90%
Epoch 56 完成: 訓練=86.90%, 驗證=77.20%, 時間=83.9s
   ⚠️  過擬合警告: 差異 9.70% > 閾值 8% (6/8)
   📊 訓練-驗證差異: 9.70%

Epoch 57/100, LR: 0.001142
   批次 100/391: 損失=0.7318, 準確率=87.76%
   批次 200/391: 損失=0.7330, 準確率=87.75%
   批次 300/391: 損失=0.7362, 準確率=87.57%
   批次 391/391: 損失=0.7431, 準確率=87.20%
Epoch 57 完成: 訓練=87.20%, 驗證=76.96%, 時間=85.4s
   ⚠️  過擬合警告: 差異 10.24% > 閾值 8% (7/8)
   📊 訓練-驗證差異: 10.24%

Epoch 58/100, LR: 0.001099
   批次 100/391: 損失=0.7374, 準確率=87.42%
   批次 200/391: 損失=0.7382, 準確率=87.41%
   批次 300/391: 損失=0.7400, 準確率=87.29%
   批次 391/391: 損失=0.7425, 準確率=87.18%
Epoch 58 完成: 訓練=87.18%, 驗證=76.90%, 時間=82.8s
   ⚠️  過擬合警告: 差異 10.28% > 閾值 8% (8/8)
   🛑 過擬合早停: 連續 8 epochs 訓練-驗證差異超過 8%

⏱️ 超快速 aMLP 訓練時間統計:
   • 總訓練時間: 4965.8s (82.8min)
   • 實際訓練epochs: 58 / 100
   • 平均每epoch: 85.6s
   • 最佳 aMLP 驗證準確率: 77.85%
   🔥 注意力機制效果: 預期比原 gMLP 提升 2-5%
   • 已載入最佳 aMLP 模型權重

📈 繪製超快速 aMLP 訓練歷史...

📊 評估超縮小版 aMLP 模型...
   ✓ aMLP 整體準確率: 77.85%
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:697: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.tight_layout()
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:698: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.savefig("ultra_small_amlp_results.png", dpi=300, bbox_inches="tight")
C:\Users\n10825019\AppData\Local\Programs\Python\Python311\Lib\tkinter\__init__.py:861: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  func(*args)

🎉 aMLP 訓練完成總結:
   • 選擇模型: aMLP-Test (注意力增強)
   • 最終準確率: 77.85%
   • 總訓練時間: 82.8 分鐘
   • 平均每epoch: 85.6 秒
   • 實際訓練輪數: 58/100
   • 早停狀態: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
   ⚠️  可嘗試更大模型或調整參數

🔄 是否要訓練其他 aMLP 模型? (y/n): n
PS E:\實驗室\gMLP> & C:/Users/n10825019/AppData/Local/Programs/Python/Python311/python.exe "e:/實驗室/gMLP/Image Classification/model_13_aMLP/model_13_aMLP.py"
🚀 超縮小版 aMLP 圖像分類訓練 - 注意力增強版本
🔥 附加注意力機制，預期準確率提升 2-5%
===========================================================================

==========================================================================================
🏗️  可用的 aMLP 模型架構 (附加注意力機制)
==========================================================================================
編號   名稱     深度     維度     FFN   注意力      參數       時間         風險       描述
------------------------------------------------------------------------------------------
1    Test   6      64     4     64       0.18M    <40秒       極低       超極速測試模型 + 注意力
2    Nano   6      64     2     64       0.25M    ~1.5分鐘     很低       極小快速模型 + 注意力
3    XS     8      80     3     40       0.35M    ~2.5分鐘     低        超小平衡模型 + 注意力
4    S      12     128    3     64       0.85M    ~6分鐘       中等       小型性能模型 + 注意力
5    M      16     160    4     80       1.65M    ~12分鐘      較高       中型高性能模型 + 注意力
6    L      30     128    6     64       2.15M    ~18分鐘      很高       大型頂級模型 + 注意力
------------------------------------------------------------------------------------------
💡 推薦選擇 (aMLP 版本):
   🚀 快速測試: Test (1) 或 Nano (2) - 注意力增強
   ⚖️  平衡性能: XS (3) 或 S (4) - 最推薦，效能提升明顯
   🎯 高性能: M (5) 或 L (6) - 頂級注意力效能
   🔥 注意力機制將顯著提升準確率，建議從 XS 開始嘗試
==========================================================================================

🤖 請選擇要使用的 aMLP 模型:
   輸入編號 (1-6) 或模型名稱 (Test/Nano/XS/S/M/L): Test

✅ 您選擇了: aMLP-Test 模型
   📋 模型詳情:
      • 深度: 6 層
      • 維度: 64
      • FFN倍數: 4
      • 注意力維度: 64 (🔥 性能提升關鍵)
      • 預估參數: 0.18M
      • 預估時間: <40秒
      • 過擬合風險: 極低
      • 描述: 超極速測試模型 + 注意力

   確認使用 aMLP-Test 模型嗎? (y/n, 預設=y): y

============================================================
⚙️  aMLP 訓練參數設置
============================================================

📦 數據集模式選擇:
   1. 快速模式 (50K訓練 + 10K測試) - 推薦
   2. 完整模式 (50K訓練 + 10K測試)
   選擇模式 (1/2, 預設=1): 2

🏋️  訓練輪數 (預設=50): 100

🛡️  過擬合保護:
   1. 啟用早停機制 (推薦)
   2. 關閉早停機制
   選擇 (1/2, 預設=1): 1

✅ aMLP 訓練參數確認:
   📦 數據模式: 完整模式
   🏋️  訓練輪數: 100
   🛡️  早停機制: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
📦 加載超快速 CIFAR-10 數據集...
   ✓ 訓練樣本: 50000
   ✓ 測試樣本: 10000
   ✓ Batch大小: 128

🏗️ 創建超縮小版 aMLP-Test 模型...
   ⚡ CPU模式：已設置4個線程

✅ 超縮小版 aMLP-Test 模型創建完成
   ✓ 設備: cpu
   ✓ 實際參數數量: 104,522 (0.10M)
   ✓ 目標參數預期: 0.18M
   ✓ 注意力維度: 32 (🔥 aMLP 增強)
   ✓ 架構配置: depth=4, dim=64, ff_mult=2
   🔥 注意力機制已啟用，預期準確率提升 2-5%

🎬 開始訓練 aMLP-Test 模型...
🔥 注意力機制已啟用，預期更佳訓練效果...

🏋️ 開始超快速 aMLP 訓練 (100 個 epochs)...
   🛡️  啟用過擬合早停保護
   🔥 注意力機制已啟用 - 預期更佳收斂效果

Epoch 1/100, LR: 0.002800
   批次 100/391: 損失=1.9197, 準確率=32.37%
   批次 200/391: 損失=1.7897, 準確率=38.31%
   批次 300/391: 損失=1.7139, 準確率=41.83%
   批次 391/391: 損失=1.6648, 準確率=44.25%
Epoch 1 完成: 訓練=44.25%, 驗證=53.95%, 時間=95.6s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 53.95%

Epoch 2/100, LR: 0.002799
   批次 100/391: 損失=1.4549, 準確率=54.20%
   批次 200/391: 損失=1.4400, 準確率=54.87%
   批次 300/391: 損失=1.4249, 準確率=55.62%
   批次 391/391: 損失=1.4141, 準確率=56.11%
Epoch 2 完成: 訓練=56.11%, 驗證=59.19%, 時間=92.0s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 59.19%

Epoch 3/100, LR: 0.002797
   批次 100/391: 損失=1.3339, 準確率=60.21%
   批次 200/391: 損失=1.3252, 準確率=60.43%
   批次 300/391: 損失=1.3224, 準確率=60.43%
   批次 391/391: 損失=1.3180, 準確率=60.66%
Epoch 3 完成: 訓練=60.66%, 驗證=62.93%, 時間=90.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 62.93%

Epoch 4/100, LR: 0.002794
   批次 100/391: 損失=1.2640, 準確率=63.28%
   批次 200/391: 損失=1.2614, 準確率=63.43%
   批次 300/391: 損失=1.2584, 準確率=63.50%
   批次 391/391: 損失=1.2526, 準確率=63.78%
Epoch 4 完成: 訓練=63.78%, 驗證=66.01%, 時間=85.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 66.01%

Epoch 5/100, LR: 0.002789
   批次 100/391: 損失=1.1876, 準確率=66.37%
   批次 200/391: 損失=1.1948, 準確率=66.02%
   批次 300/391: 損失=1.1960, 準確率=66.07%
   批次 391/391: 損失=1.1968, 準確率=66.20%
Epoch 5 完成: 訓練=66.20%, 驗證=66.87%, 時間=84.8s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 66.87%

Epoch 6/100, LR: 0.002783
   批次 100/391: 損失=1.1603, 準確率=67.53%
   批次 200/391: 損失=1.1587, 準確率=68.01%
   批次 300/391: 損失=1.1573, 準確率=68.05%
   批次 391/391: 損失=1.1529, 準確率=68.28%
Epoch 6 完成: 訓練=68.28%, 驗證=69.29%, 時間=86.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 69.29%

Epoch 7/100, LR: 0.002775
   批次 100/391: 損失=1.1259, 準確率=69.30%
   批次 200/391: 損失=1.1297, 準確率=69.21%
   批次 300/391: 損失=1.1249, 準確率=69.58%
   批次 391/391: 損失=1.1230, 準確率=69.71%
Epoch 7 完成: 訓練=69.71%, 驗證=69.83%, 時間=93.6s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 69.83%

Epoch 8/100, LR: 0.002766
   批次 100/391: 損失=1.0913, 準確率=71.11%
   批次 200/391: 損失=1.1031, 準確率=70.55%
   批次 300/391: 損失=1.1023, 準確率=70.50%
   批次 391/391: 損失=1.1001, 準確率=70.61%
Epoch 8 完成: 訓練=70.61%, 驗證=70.01%, 時間=86.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 70.01%

Epoch 9/100, LR: 0.002756
   批次 100/391: 損失=1.0684, 準確率=72.18%
   批次 200/391: 損失=1.0712, 準確率=71.94%
   批次 300/391: 損失=1.0734, 準確率=71.91%
   批次 391/391: 損失=1.0734, 準確率=71.93%
Epoch 9 完成: 訓練=71.93%, 驗證=70.35%, 時間=86.4s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 70.35%

Epoch 10/100, LR: 0.002745
   批次 100/391: 損失=1.0559, 準確率=72.51%
   批次 200/391: 損失=1.0528, 準確率=72.92%
   批次 300/391: 損失=1.0522, 準確率=72.96%
   批次 391/391: 損失=1.0536, 準確率=72.86%
Epoch 10 完成: 訓練=72.86%, 驗證=72.10%, 時間=96.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 72.10%

Epoch 11/100, LR: 0.002732
   批次 100/391: 損失=1.0268, 準確率=73.93%
   批次 200/391: 損失=1.0320, 準確率=73.88%
   批次 300/391: 損失=1.0338, 準確率=73.85%
   批次 391/391: 損失=1.0341, 準確率=73.84%
Epoch 11 完成: 訓練=73.84%, 驗證=72.62%, 時間=99.6s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 72.62%

Epoch 12/100, LR: 0.002717
   批次 100/391: 損失=1.0116, 準確率=74.58%
   批次 200/391: 損失=1.0167, 準確率=74.34%
   批次 300/391: 損失=1.0180, 準確率=74.26%
   批次 391/391: 損失=1.0190, 準確率=74.29%
Epoch 12 完成: 訓練=74.29%, 驗證=73.22%, 時間=88.8s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 73.22%

Epoch 13/100, LR: 0.002702
   批次 100/391: 損失=0.9911, 準確率=75.51%
   批次 200/391: 損失=1.0096, 準確率=74.74%
   批次 300/391: 損失=1.0082, 準確率=74.85%
   批次 391/391: 損失=1.0076, 準確率=74.90%
Epoch 13 完成: 訓練=74.90%, 驗證=73.25%, 時間=91.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 73.25%

Epoch 14/100, LR: 0.002685
   批次 100/391: 損失=0.9917, 準確率=75.10%
   批次 200/391: 損失=0.9879, 準確率=75.64%
   批次 300/391: 損失=0.9923, 準確率=75.53%
   批次 391/391: 損失=0.9950, 準確率=75.38%
Epoch 14 完成: 訓練=75.38%, 驗證=72.95%, 時間=89.9s

Epoch 15/100, LR: 0.002667
   批次 100/391: 損失=0.9654, 準確率=76.79%
   批次 200/391: 損失=0.9691, 準確率=76.56%
   批次 300/391: 損失=0.9761, 準確率=76.20%
   批次 391/391: 損失=0.9803, 準確率=76.02%
Epoch 15 完成: 訓練=76.02%, 驗證=74.28%, 時間=90.4s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 74.28%

Epoch 16/100, LR: 0.002648
   批次 100/391: 損失=0.9647, 準確率=77.12%
   批次 200/391: 損失=0.9664, 準確率=77.02%
   批次 300/391: 損失=0.9708, 準確率=76.69%
   批次 391/391: 損失=0.9735, 準確率=76.54%
Epoch 16 完成: 訓練=76.54%, 驗證=74.10%, 時間=95.5s

Epoch 17/100, LR: 0.002627
   批次 100/391: 損失=0.9557, 準確率=77.03%
   批次 200/391: 損失=0.9598, 準確率=76.98%
   批次 300/391: 損失=0.9617, 準確率=76.87%
   批次 391/391: 損失=0.9630, 準確率=76.84%
Epoch 17 完成: 訓練=76.84%, 驗證=74.87%, 時間=89.6s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 74.87%

Epoch 18/100, LR: 0.002606
   批次 100/391: 損失=0.9494, 準確率=77.44%
   批次 200/391: 損失=0.9516, 準確率=77.12%
   批次 300/391: 損失=0.9542, 準確率=77.01%
   批次 391/391: 損失=0.9552, 準確率=77.03%
Epoch 18 完成: 訓練=77.03%, 驗證=74.64%, 時間=88.8s

Epoch 19/100, LR: 0.002583
   批次 100/391: 損失=0.9357, 準確率=77.84%
   批次 200/391: 損失=0.9387, 準確率=77.85%
   批次 300/391: 損失=0.9459, 準確率=77.55%
   批次 391/391: 損失=0.9464, 準確率=77.60%
Epoch 19 完成: 訓練=77.60%, 驗證=74.69%, 時間=89.0s

Epoch 20/100, LR: 0.002559
   批次 100/391: 損失=0.9212, 準確率=78.67%
   批次 200/391: 損失=0.9277, 準確率=78.31%
   批次 300/391: 損失=0.9342, 準確率=78.11%
   批次 391/391: 損失=0.9350, 準確率=78.06%
Epoch 20 完成: 訓練=78.06%, 驗證=74.51%, 時間=90.0s

Epoch 21/100, LR: 0.002533
   批次 100/391: 損失=0.9190, 準確率=78.88%
   批次 200/391: 損失=0.9246, 準確率=78.72%
   批次 300/391: 損失=0.9256, 準確率=78.61%
   批次 391/391: 損失=0.9286, 準確率=78.43%
Epoch 21 完成: 訓練=78.43%, 驗證=75.72%, 時間=89.6s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.72%

Epoch 22/100, LR: 0.002507
   批次 100/391: 損失=0.9112, 準確率=79.24%
   批次 200/391: 損失=0.9201, 準確率=78.91%
   批次 300/391: 損失=0.9188, 準確率=78.91%
   批次 391/391: 損失=0.9215, 準確率=78.68%
Epoch 22 完成: 訓練=78.68%, 驗證=76.17%, 時間=88.7s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.17%

Epoch 23/100, LR: 0.002480
   批次 100/391: 損失=0.9091, 準確率=79.14%
   批次 200/391: 損失=0.9108, 準確率=79.28%
   批次 300/391: 損失=0.9121, 準確率=79.15%
   批次 391/391: 損失=0.9162, 準確率=79.03%
Epoch 23 完成: 訓練=79.03%, 驗證=76.07%, 時間=88.5s

Epoch 24/100, LR: 0.002451
   批次 100/391: 損失=0.8972, 準確率=79.95%
   批次 200/391: 損失=0.9001, 準確率=79.65%
   批次 300/391: 損失=0.9026, 準確率=79.58%
   批次 391/391: 損失=0.9074, 準確率=79.34%
Epoch 24 完成: 訓練=79.34%, 驗證=75.74%, 時間=88.9s

Epoch 25/100, LR: 0.002422
   批次 100/391: 損失=0.8879, 準確率=80.25%
   批次 200/391: 損失=0.8980, 準確率=79.67%
   批次 300/391: 損失=0.9015, 準確率=79.64%
   批次 391/391: 損失=0.9017, 準確率=79.66%
Epoch 25 完成: 訓練=79.66%, 驗證=76.18%, 時間=88.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.18%

Epoch 26/100, LR: 0.002391
   批次 100/391: 損失=0.8936, 準確率=79.84%
   批次 200/391: 損失=0.8900, 準確率=79.88%
   批次 300/391: 損失=0.8914, 準確率=79.84%
   批次 391/391: 損失=0.8944, 準確率=79.88%
Epoch 26 完成: 訓練=79.88%, 驗證=76.49%, 時間=94.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.49%

Epoch 27/100, LR: 0.002360
   批次 100/391: 損失=0.8832, 準確率=80.46%
   批次 200/391: 損失=0.8827, 準確率=80.43%
   批次 300/391: 損失=0.8867, 準確率=80.18%
   批次 391/391: 損失=0.8882, 準確率=80.14%
Epoch 27 完成: 訓練=80.14%, 驗證=76.41%, 時間=89.2s

Epoch 28/100, LR: 0.002327
   批次 100/391: 損失=0.8620, 準確率=81.48%
   批次 200/391: 損失=0.8738, 準確率=80.96%
   批次 300/391: 損失=0.8756, 準確率=80.84%
   批次 391/391: 損失=0.8798, 準確率=80.63%
Epoch 28 完成: 訓練=80.63%, 驗證=76.87%, 時間=89.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.87%

Epoch 29/100, LR: 0.002294
   批次 100/391: 損失=0.8678, 準確率=81.21%
   批次 200/391: 損失=0.8710, 準確率=81.12%
   批次 300/391: 損失=0.8757, 準確率=80.82%
   批次 391/391: 損失=0.8760, 準確率=80.78%
Epoch 29 完成: 訓練=80.78%, 驗證=76.73%, 時間=89.3s

Epoch 30/100, LR: 0.002260
   批次 100/391: 損失=0.8571, 準確率=81.93%
   批次 200/391: 損失=0.8647, 準確率=81.55%
   批次 300/391: 損失=0.8662, 準確率=81.29%
   批次 391/391: 損失=0.8680, 準確率=81.14%
Epoch 30 完成: 訓練=81.14%, 驗證=77.00%, 時間=91.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.00%

Epoch 31/100, LR: 0.002225
   批次 100/391: 損失=0.8585, 準確率=81.40%
   批次 200/391: 損失=0.8640, 準確率=81.16%
   批次 300/391: 損失=0.8639, 準確率=81.09%
   批次 391/391: 損失=0.8659, 準確率=81.11%
Epoch 31 完成: 訓練=81.11%, 驗證=76.38%, 時間=85.7s

Epoch 32/100, LR: 0.002189
   批次 100/391: 損失=0.8465, 準確率=82.33%
   批次 200/391: 損失=0.8513, 準確率=81.88%
   批次 300/391: 損失=0.8590, 準確率=81.48%
   批次 391/391: 損失=0.8613, 準確率=81.41%
Epoch 32 完成: 訓練=81.41%, 驗證=76.41%, 時間=87.4s

Epoch 33/100, LR: 0.002152
   批次 100/391: 損失=0.8431, 準確率=81.94%
   批次 200/391: 損失=0.8489, 準確率=81.89%
   批次 300/391: 損失=0.8550, 準確率=81.66%
   批次 391/391: 損失=0.8558, 準確率=81.66%
Epoch 33 完成: 訓練=81.66%, 驗證=76.45%, 時間=90.1s
   📊 訓練-驗證差異: 5.21%

Epoch 34/100, LR: 0.002115
   批次 100/391: 損失=0.8441, 準確率=82.23%
   批次 200/391: 損失=0.8489, 準確率=82.03%
   批次 300/391: 損失=0.8516, 準確率=81.93%
   批次 391/391: 損失=0.8514, 準確率=81.92%
Epoch 34 完成: 訓練=81.92%, 驗證=77.40%, 時間=95.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.40%

Epoch 35/100, LR: 0.002077
   批次 100/391: 損失=0.8309, 準確率=82.84%
   批次 200/391: 損失=0.8381, 準確率=82.50%
   批次 300/391: 損失=0.8411, 準確率=82.36%
   批次 391/391: 損失=0.8436, 準確率=82.24%
Epoch 35 完成: 訓練=82.24%, 驗證=77.26%, 時間=89.9s

Epoch 36/100, LR: 0.002038
   批次 100/391: 損失=0.8348, 準確率=82.67%
   批次 200/391: 損失=0.8400, 準確率=82.20%
   批次 300/391: 損失=0.8399, 準確率=82.35%
   批次 391/391: 損失=0.8412, 準確率=82.33%
Epoch 36 完成: 訓練=82.33%, 驗證=77.38%, 時間=90.1s

Epoch 37/100, LR: 0.001998
   批次 100/391: 損失=0.8280, 準確率=82.85%
   批次 200/391: 損失=0.8276, 準確率=82.98%
   批次 300/391: 損失=0.8332, 準確率=82.76%
   批次 391/391: 損失=0.8365, 準確率=82.56%
Epoch 37 完成: 訓練=82.56%, 驗證=77.09%, 時間=88.9s
   📊 訓練-驗證差異: 5.47%

Epoch 38/100, LR: 0.001958
   批次 100/391: 損失=0.8240, 準確率=83.22%
   批次 200/391: 損失=0.8314, 準確率=82.89%
   批次 300/391: 損失=0.8278, 準確率=83.04%
   批次 391/391: 損失=0.8292, 準確率=82.94%
Epoch 38 完成: 訓練=82.94%, 驗證=77.08%, 時間=91.6s
   📊 訓練-驗證差異: 5.86%

Epoch 39/100, LR: 0.001918
   批次 100/391: 損失=0.8151, 準確率=83.62%
   批次 200/391: 損失=0.8160, 準確率=83.50%
   批次 300/391: 損失=0.8233, 準確率=83.20%
   批次 391/391: 損失=0.8273, 準確率=83.02%
Epoch 39 完成: 訓練=83.02%, 驗證=77.55%, 時間=90.9s
   📊 訓練-驗證差異: 5.47%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.55%

Epoch 40/100, LR: 0.001877
   批次 100/391: 損失=0.8045, 準確率=83.95%
   批次 200/391: 損失=0.8117, 準確率=83.65%
   批次 300/391: 損失=0.8184, 準確率=83.29%
   批次 391/391: 損失=0.8205, 準確率=83.23%
Epoch 40 完成: 訓練=83.23%, 驗證=77.61%, 時間=85.9s
   📊 訓練-驗證差異: 5.62%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.61%

Epoch 41/100, LR: 0.001835
   批次 100/391: 損失=0.8095, 準確率=83.51%
   批次 200/391: 損失=0.8141, 準確率=83.41%
   批次 300/391: 損失=0.8172, 準確率=83.27%
   批次 391/391: 損失=0.8165, 準確率=83.27%
Epoch 41 完成: 訓練=83.27%, 驗證=77.37%, 時間=87.7s
   📊 訓練-驗證差異: 5.90%

Epoch 42/100, LR: 0.001793
   批次 100/391: 損失=0.7995, 準確率=84.40%
   批次 200/391: 損失=0.8026, 準確率=84.23%
   批次 300/391: 損失=0.8057, 準確率=84.01%
   批次 391/391: 損失=0.8119, 準確率=83.71%
Epoch 42 完成: 訓練=83.71%, 驗證=77.55%, 時間=88.1s
   📊 訓練-驗證差異: 6.16%

Epoch 43/100, LR: 0.001751
   批次 100/391: 損失=0.7944, 準確率=84.66%
   批次 200/391: 損失=0.8012, 準確率=84.23%
   批次 300/391: 損失=0.8067, 準確率=83.96%
   批次 391/391: 損失=0.8093, 準確率=83.78%
Epoch 43 完成: 訓練=83.78%, 驗證=77.88%, 時間=87.5s
   📊 訓練-驗證差異: 5.90%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.88%

Epoch 44/100, LR: 0.001709
   批次 100/391: 損失=0.7920, 準確率=84.76%
   批次 200/391: 損失=0.7939, 準確率=84.59%
   批次 300/391: 損失=0.8016, 準確率=84.20%
   批次 391/391: 損失=0.8017, 準確率=84.17%
Epoch 44 完成: 訓練=84.17%, 驗證=77.87%, 時間=87.3s
   📊 訓練-驗證差異: 6.30%

Epoch 45/100, LR: 0.001666
   批次 100/391: 損失=0.7878, 準確率=84.59%
   批次 200/391: 損失=0.7922, 準確率=84.39%
   批次 300/391: 損失=0.7960, 準確率=84.25%
   批次 391/391: 損失=0.7972, 準確率=84.22%
Epoch 45 完成: 訓練=84.22%, 驗證=77.06%, 時間=88.3s
   📊 訓練-驗證差異: 7.16%

Epoch 46/100, LR: 0.001622
   批次 100/391: 損失=0.7890, 準確率=84.56%
   批次 200/391: 損失=0.7901, 準確率=84.57%
   批次 300/391: 損失=0.7955, 準確率=84.31%
   批次 391/391: 損失=0.7957, 準確率=84.32%
Epoch 46 完成: 訓練=84.32%, 驗證=77.71%, 時間=87.9s
   📊 訓練-驗證差異: 6.61%

Epoch 47/100, LR: 0.001579
   批次 100/391: 損失=0.7751, 準確率=85.41%
   批次 200/391: 損失=0.7785, 準確率=85.01%
   批次 300/391: 損失=0.7858, 準確率=84.77%
   批次 391/391: 損失=0.7898, 準確率=84.61%
Epoch 47 完成: 訓練=84.61%, 驗證=76.78%, 時間=87.7s
   📊 訓練-驗證差異: 7.83%

Epoch 48/100, LR: 0.001535
   批次 100/391: 損失=0.7744, 準確率=85.23%
   批次 200/391: 損失=0.7832, 準確率=84.94%
   批次 300/391: 損失=0.7844, 準確率=84.88%
   批次 391/391: 損失=0.7860, 準確率=84.81%
Epoch 48 完成: 訓練=84.81%, 驗證=77.68%, 時間=87.6s
   📊 訓練-驗證差異: 7.13%

Epoch 49/100, LR: 0.001492
   批次 100/391: 損失=0.7671, 準確率=85.57%
   批次 200/391: 損失=0.7777, 準確率=85.16%
   批次 300/391: 損失=0.7803, 準確率=85.14%
   批次 391/391: 損失=0.7831, 準確率=85.01%
Epoch 49 完成: 訓練=85.01%, 驗證=78.01%, 時間=86.7s
   📊 訓練-驗證差異: 7.00%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.01%

Epoch 50/100, LR: 0.001448
   批次 100/391: 損失=0.7758, 準確率=85.44%
   批次 200/391: 損失=0.7738, 準確率=85.33%
   批次 300/391: 損失=0.7720, 準確率=85.35%
   批次 391/391: 損失=0.7759, 準確率=85.22%
Epoch 50 完成: 訓練=85.22%, 驗證=78.31%, 時間=91.5s
   📊 訓練-驗證差異: 6.91%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.31%

Epoch 51/100, LR: 0.001404
   批次 100/391: 損失=0.7637, 準確率=85.89%
   批次 200/391: 損失=0.7651, 準確率=86.00%
   批次 300/391: 損失=0.7696, 準確率=85.72%
   批次 391/391: 損失=0.7718, 準確率=85.54%
Epoch 51 完成: 訓練=85.54%, 驗證=77.74%, 時間=87.3s
   📊 訓練-驗證差異: 7.80%

Epoch 52/100, LR: 0.001360
   批次 100/391: 損失=0.7492, 準確率=86.44%
   批次 200/391: 損失=0.7573, 準確率=86.20%
   批次 300/391: 損失=0.7638, 準確率=85.95%
   批次 391/391: 損失=0.7651, 準確率=85.84%
Epoch 52 完成: 訓練=85.84%, 驗證=78.62%, 時間=87.8s
   📊 訓練-驗證差異: 7.22%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.62%

Epoch 53/100, LR: 0.001316
   批次 100/391: 損失=0.7606, 準確率=86.30%
   批次 200/391: 損失=0.7571, 準確率=86.41%
   批次 300/391: 損失=0.7589, 準確率=86.36%
   批次 391/391: 損失=0.7608, 準確率=86.22%
Epoch 53 完成: 訓練=86.22%, 驗證=77.97%, 時間=87.6s
   📊 訓練-驗證差異: 8.25%

Epoch 54/100, LR: 0.001273
   批次 100/391: 損失=0.7386, 準確率=87.18%
   批次 200/391: 損失=0.7429, 準確率=87.07%
   批次 300/391: 損失=0.7539, 準確率=86.39%
   批次 391/391: 損失=0.7572, 準確率=86.27%
Epoch 54 完成: 訓練=86.27%, 驗證=78.13%, 時間=88.8s
   📊 訓練-驗證差異: 8.14%

Epoch 55/100, LR: 0.001229
   批次 100/391: 損失=0.7448, 準確率=86.88%
   批次 200/391: 損失=0.7508, 準確率=86.46%
   批次 300/391: 損失=0.7524, 準確率=86.45%
   批次 391/391: 損失=0.7542, 準確率=86.36%
Epoch 55 完成: 訓練=86.36%, 驗證=78.50%, 時間=89.0s
   📊 訓練-驗證差異: 7.86%

Epoch 56/100, LR: 0.001186
   批次 100/391: 損失=0.7439, 準確率=86.53%
   批次 200/391: 損失=0.7422, 準確率=86.71%
   批次 300/391: 損失=0.7479, 準確率=86.53%
   批次 391/391: 損失=0.7491, 準確率=86.54%
Epoch 56 完成: 訓練=86.54%, 驗證=78.00%, 時間=88.3s
   📊 訓練-驗證差異: 8.54%

Epoch 57/100, LR: 0.001142
   批次 100/391: 損失=0.7420, 準確率=87.05%
   批次 200/391: 損失=0.7427, 準確率=86.96%
   批次 300/391: 損失=0.7431, 準確率=86.90%
   批次 391/391: 損失=0.7444, 準確率=86.76%
Epoch 57 完成: 訓練=86.76%, 驗證=78.61%, 時間=88.1s
   📊 訓練-驗證差異: 8.15%

Epoch 58/100, LR: 0.001099
   批次 100/391: 損失=0.7373, 準確率=86.93%
   批次 200/391: 損失=0.7366, 準確率=87.19%
   批次 300/391: 損失=0.7386, 準確率=87.14%
   批次 391/391: 損失=0.7428, 準確率=86.96%
Epoch 58 完成: 訓練=86.96%, 驗證=78.15%, 時間=86.7s
   📊 訓練-驗證差異: 8.81%

Epoch 59/100, LR: 0.001057
   批次 100/391: 損失=0.7383, 準確率=87.13%
   批次 200/391: 損失=0.7388, 準確率=87.07%
   批次 300/391: 損失=0.7381, 準確率=87.16%
   批次 391/391: 損失=0.7404, 準確率=87.07%
Epoch 59 完成: 訓練=87.07%, 驗證=78.46%, 時間=88.6s
   📊 訓練-驗證差異: 8.61%

Epoch 60/100, LR: 0.001015
   批次 100/391: 損失=0.7217, 準確率=87.50%
   批次 200/391: 損失=0.7281, 準確率=87.39%
   批次 300/391: 損失=0.7293, 準確率=87.44%
   批次 391/391: 損失=0.7337, 準確率=87.25%
Epoch 60 完成: 訓練=87.25%, 驗證=78.26%, 時間=88.0s
   📊 訓練-驗證差異: 8.99%

Epoch 61/100, LR: 0.000973
   批次 100/391: 損失=0.7219, 準確率=87.80%
   批次 200/391: 損失=0.7271, 準確率=87.56%
   批次 300/391: 損失=0.7297, 準確率=87.43%
   批次 391/391: 損失=0.7314, 準確率=87.41%
Epoch 61 完成: 訓練=87.41%, 驗證=78.40%, 時間=87.4s
   📊 訓練-驗證差異: 9.01%

Epoch 62/100, LR: 0.000931
   批次 100/391: 損失=0.7131, 準確率=88.50%
   批次 200/391: 損失=0.7198, 準確率=88.12%
   批次 300/391: 損失=0.7257, 準確率=87.80%
   批次 391/391: 損失=0.7285, 準確率=87.62%
Epoch 62 完成: 訓練=87.62%, 驗證=78.49%, 時間=90.2s
   📊 訓練-驗證差異: 9.13%

Epoch 63/100, LR: 0.000890
   批次 100/391: 損失=0.7156, 準確率=88.34%
   批次 200/391: 損失=0.7196, 準確率=88.08%
   批次 300/391: 損失=0.7213, 準確率=87.97%
   批次 391/391: 損失=0.7236, 準確率=87.90%
Epoch 63 完成: 訓練=87.90%, 驗證=78.81%, 時間=88.8s
   📊 訓練-驗證差異: 9.09%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.81%

Epoch 64/100, LR: 0.000850
   批次 100/391: 損失=0.7084, 準確率=88.52%
   批次 200/391: 損失=0.7140, 準確率=88.36%
   批次 300/391: 損失=0.7169, 準確率=88.29%
   批次 391/391: 損失=0.7198, 準確率=88.12%
Epoch 64 完成: 訓練=88.12%, 驗證=78.14%, 時間=87.1s
   📊 訓練-驗證差異: 9.98%

Epoch 65/100, LR: 0.000810
   批次 100/391: 損失=0.7065, 準確率=88.63%
   批次 200/391: 損失=0.7110, 準確率=88.46%
   批次 300/391: 損失=0.7130, 準確率=88.40%
   批次 391/391: 損失=0.7144, 準確率=88.32%
Epoch 65 完成: 訓練=88.32%, 驗證=78.65%, 時間=87.9s
   📊 訓練-驗證差異: 9.67%

Epoch 66/100, LR: 0.000770
   批次 100/391: 損失=0.6996, 準確率=88.90%
   批次 200/391: 損失=0.7076, 準確率=88.54%
   批次 300/391: 損失=0.7098, 準確率=88.47%
   批次 391/391: 損失=0.7124, 準確率=88.34%
Epoch 66 完成: 訓練=88.34%, 驗證=78.48%, 時間=90.7s
   📊 訓練-驗證差異: 9.86%

Epoch 67/100, LR: 0.000731
   批次 100/391: 損失=0.7009, 準確率=88.69%
   批次 200/391: 損失=0.7078, 準確率=88.49%
   批次 300/391: 損失=0.7078, 準確率=88.57%
   批次 391/391: 損失=0.7103, 準確率=88.45%
Epoch 67 完成: 訓練=88.45%, 驗證=78.83%, 時間=87.8s
   📊 訓練-驗證差異: 9.62%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.83%

Epoch 68/100, LR: 0.000693
   批次 100/391: 損失=0.6944, 準確率=89.19%
   批次 200/391: 損失=0.6982, 準確率=89.15%
   批次 300/391: 損失=0.7045, 準確率=88.81%
   批次 391/391: 損失=0.7070, 準確率=88.67%
Epoch 68 完成: 訓練=88.67%, 驗證=79.08%, 時間=86.6s
   📊 訓練-驗證差異: 9.59%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.08%

Epoch 69/100, LR: 0.000656
   批次 100/391: 損失=0.6964, 準確率=89.09%
   批次 200/391: 損失=0.7011, 準確率=88.95%
   批次 300/391: 損失=0.6986, 準確率=89.03%
   批次 391/391: 損失=0.7017, 準確率=88.84%
Epoch 69 完成: 訓練=88.84%, 驗證=78.77%, 時間=86.9s
   ⚠️  過擬合警告: 差異 10.07% > 閾值 10% (1/10)
   📊 訓練-驗證差異: 10.07%

Epoch 70/100, LR: 0.000619
   批次 100/391: 損失=0.6951, 準確率=89.20%
   批次 200/391: 損失=0.6923, 準確率=89.21%
   批次 300/391: 損失=0.6975, 準確率=89.01%
   批次 391/391: 損失=0.6982, 準確率=89.03%
Epoch 70 完成: 訓練=89.03%, 驗證=78.77%, 時間=89.7s
   ⚠️  過擬合警告: 差異 10.26% > 閾值 10% (2/10)
   📊 訓練-驗證差異: 10.26%

Epoch 71/100, LR: 0.000583
   批次 100/391: 損失=0.6825, 準確率=89.91%
   批次 200/391: 損失=0.6875, 準確率=89.66%
   批次 300/391: 損失=0.6908, 準確率=89.55%
   批次 391/391: 損失=0.6940, 準確率=89.38%
Epoch 71 完成: 訓練=89.38%, 驗證=78.57%, 時間=101.5s
   ⚠️  過擬合警告: 差異 10.81% > 閾值 10% (3/10)
   📊 訓練-驗證差異: 10.81%

Epoch 72/100, LR: 0.000548
   批次 100/391: 損失=0.6823, 準確率=90.02%
   批次 200/391: 損失=0.6865, 準確率=89.80%
   批次 300/391: 損失=0.6902, 準確率=89.53%
   批次 391/391: 損失=0.6913, 準確率=89.43%
Epoch 72 完成: 訓練=89.43%, 驗證=78.65%, 時間=93.8s
   ⚠️  過擬合警告: 差異 10.78% > 閾值 10% (4/10)
   📊 訓練-驗證差異: 10.78%

Epoch 73/100, LR: 0.000514
   批次 100/391: 損失=0.6888, 準確率=89.90%
   批次 200/391: 損失=0.6896, 準確率=89.69%
   批次 300/391: 損失=0.6887, 準確率=89.66%
   批次 391/391: 損失=0.6894, 準確率=89.63%
Epoch 73 完成: 訓練=89.63%, 驗證=78.66%, 時間=91.8s
   ⚠️  過擬合警告: 差異 10.97% > 閾值 10% (5/10)
   📊 訓練-驗證差異: 10.97%

Epoch 74/100, LR: 0.000481
   批次 100/391: 損失=0.6840, 準確率=89.64%
   批次 200/391: 損失=0.6845, 準確率=89.65%
   批次 300/391: 損失=0.6848, 準確率=89.67%
   批次 391/391: 損失=0.6874, 準確率=89.52%
Epoch 74 完成: 訓練=89.52%, 驗證=78.23%, 時間=90.5s
   ⚠️  過擬合警告: 差異 11.29% > 閾值 10% (6/10)
   📊 訓練-驗證差異: 11.29%

Epoch 75/100, LR: 0.000448
   批次 100/391: 損失=0.6704, 準確率=90.59%
   批次 200/391: 損失=0.6782, 準確率=90.07%
   批次 300/391: 損失=0.6835, 準確率=89.73%
   批次 391/391: 損失=0.6847, 準確率=89.69%
Epoch 75 完成: 訓練=89.69%, 驗證=78.65%, 時間=90.0s
   ⚠️  過擬合警告: 差異 11.04% > 閾值 10% (7/10)
   📊 訓練-驗證差異: 11.04%

Epoch 76/100, LR: 0.000417
   批次 100/391: 損失=0.6706, 準確率=90.75%
   批次 200/391: 損失=0.6736, 準確率=90.48%
   批次 300/391: 損失=0.6780, 準確率=90.22%
   批次 391/391: 損失=0.6811, 準確率=90.01%
Epoch 76 完成: 訓練=90.01%, 驗證=78.70%, 時間=90.1s
   ⚠️  過擬合警告: 差異 11.31% > 閾值 10% (8/10)
   📊 訓練-驗證差異: 11.31%

Epoch 77/100, LR: 0.000386
   批次 100/391: 損失=0.6729, 準確率=90.25%
   批次 200/391: 損失=0.6759, 準確率=90.09%
   批次 300/391: 損失=0.6762, 準確率=90.08%
   批次 391/391: 損失=0.6794, 準確率=89.94%
Epoch 77 完成: 訓練=89.94%, 驗證=78.56%, 時間=90.1s
   ⚠️  過擬合警告: 差異 11.38% > 閾值 10% (9/10)
   📊 訓練-驗證差異: 11.38%

Epoch 78/100, LR: 0.000357
   批次 100/391: 損失=0.6676, 準確率=90.53%
   批次 200/391: 損失=0.6697, 準確率=90.48%
   批次 300/391: 損失=0.6727, 準確率=90.38%
   批次 391/391: 損失=0.6747, 準確率=90.27%
Epoch 78 完成: 訓練=90.27%, 驗證=78.51%, 時間=90.4s
   ⚠️  過擬合警告: 差異 11.76% > 閾值 10% (10/10)
   🛑 過擬合早停: 連續 10 epochs 訓練-驗證差異超過 10%

⏱️ 超快速 aMLP 訓練時間統計:
   • 總訓練時間: 6986.5s (116.4min)
   • 實際訓練epochs: 78 / 100
   • 平均每epoch: 89.6s
   • 最佳 aMLP 驗證準確率: 79.08%
   🔥 注意力機制效果: 預期比原 gMLP 提升 2-5%
   • 已載入最佳 aMLP 模型權重

📈 繪製超快速 aMLP 訓練歷史...

📊 評估超縮小版 aMLP 模型...
   ✓ aMLP 整體準確率: 79.08%
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:697: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.tight_layout()
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:698: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.savefig("ultra_small_amlp_results.png", dpi=300, bbox_inches="tight")
C:\Users\n10825019\AppData\Local\Programs\Python\Python311\Lib\tkinter\__init__.py:861: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  func(*args)

🎉 aMLP 訓練完成總結:
   • 選擇模型: aMLP-Test (注意力增強)
   • 最終準確率: 79.08%
   • 總訓練時間: 116.4 分鐘
   • 平均每epoch: 89.6 秒
   • 實際訓練輪數: 78/100
   • 早停狀態: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
   ✅ 良好性能！注意力機制有效

📦 數據集模式選擇:
   1. 快速模式 (50K訓練 + 10K測試) - 推薦
   2. 完整模式 (50K訓練 + 10K測試)
   選擇模式 (1/2, 預設=1): 2

🏋️  訓練輪數 (預設=50): 100

🛡️  過擬合保護:
   1. 啟用早停機制 (推薦)
   2. 關閉早停機制
   選擇 (1/2, 預設=1): 1

✅ aMLP 訓練參數確認:
   📦 數據模式: 完整模式
   🏋️  訓練輪數: 100
   🛡️  早停機制: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
📦 加載超快速 CIFAR-10 數據集...
   ✓ 訓練樣本: 50000
   ✓ 測試樣本: 10000
   ✓ Batch大小: 128

🏗️ 創建超縮小版 aMLP-Nano 模型...
   ⚡ CPU模式：已設置4個線程

✅ 超縮小版 aMLP-Nano 模型創建完成
   ✓ 設備: cpu
   ✓ 實際參數數量: 154,826 (0.15M)
   ✓ 目標參數預期: 0.25M
   ✓ 注意力維度: 32 (🔥 aMLP 增強)
   ✓ 架構配置: depth=6, dim=64, ff_mult=2
   🔥 注意力機制已啟用，預期準確率提升 2-5%

🎬 開始訓練 aMLP-Nano 模型...
🔥 注意力機制已啟用，預期更佳訓練效果...

🏋️ 開始超快速 aMLP 訓練 (100 個 epochs)...
   🛡️  啟用過擬合早停保護
   🔥 注意力機制已啟用 - 預期更佳收斂效果

Epoch 1/100, LR: 0.002800
   批次  50/391: 損失=2.0044, 準確率=27.91%
   批次 100/391: 損失=1.8958, 準確率=33.50%
   批次 150/391: 損失=1.8214, 準確率=36.82%
   批次 200/391: 損失=1.7721, 準確率=39.25%
   批次 250/391: 損失=1.7324, 準確率=40.93%
   批次 300/391: 損失=1.6978, 準確率=42.62%
   批次 350/391: 損失=1.6743, 準確率=43.64%
   批次 391/391: 損失=1.6545, 準確率=44.65%
Epoch 1 完成: 訓練=44.65%, 驗證=52.41%, 時間=127.7s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 52.41%

Epoch 2/100, LR: 0.002799
   批次  50/391: 損失=1.4680, 準確率=53.89%
   批次 100/391: 損失=1.4455, 準確率=54.84%
   批次 150/391: 損失=1.4389, 準確率=55.16%
   批次 200/391: 損失=1.4321, 準確率=55.40%
   批次 250/391: 損失=1.4257, 準確率=55.71%
   批次 300/391: 損失=1.4170, 準確率=56.11%
   批次 350/391: 損失=1.4059, 準確率=56.74%
   批次 391/391: 損失=1.3994, 準確率=57.07%
Epoch 2 完成: 訓練=57.07%, 驗證=60.89%, 時間=127.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 60.89%

Epoch 3/100, LR: 0.002797
   批次  50/391: 損失=1.3081, 準確率=60.89%
   批次 100/391: 損失=1.3090, 準確率=61.11%
   批次 150/391: 損失=1.3045, 準確率=61.31%
   批次 200/391: 損失=1.3044, 準確率=61.31%
   批次 250/391: 損失=1.3002, 準確率=61.49%
   批次 300/391: 損失=1.2920, 準確率=61.91%
   批次 350/391: 損失=1.2841, 準確率=62.22%
   批次 391/391: 損失=1.2803, 準確率=62.40%
Epoch 3 完成: 訓練=62.40%, 驗證=65.07%, 時間=126.6s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 65.07%

Epoch 4/100, LR: 0.002794
   批次  50/391: 損失=1.2281, 準確率=64.67%
   批次 100/391: 損失=1.2244, 準確率=64.98%
   批次 150/391: 損失=1.2143, 準確率=65.28%
   批次 200/391: 損失=1.2163, 準確率=65.24%
   批次 250/391: 損失=1.2153, 準確率=65.24%
   批次 300/391: 損失=1.2123, 準確率=65.47%
   批次 350/391: 損失=1.2085, 準確率=65.63%
   批次 391/391: 損失=1.2057, 準確率=65.82%
Epoch 4 完成: 訓練=65.82%, 驗證=67.35%, 時間=131.7s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 67.35%

Epoch 5/100, LR: 0.002789
   批次  50/391: 損失=1.1221, 準確率=69.55%
   批次 100/391: 損失=1.1395, 準確率=68.85%
   批次 150/391: 損失=1.1418, 準確率=68.81%
   批次 200/391: 損失=1.1391, 準確率=68.95%
   批次 250/391: 損失=1.1428, 準確率=68.78%
   批次 300/391: 損失=1.1468, 準確率=68.60%
   批次 350/391: 損失=1.1465, 準確率=68.59%
   批次 391/391: 損失=1.1464, 準確率=68.60%
Epoch 5 完成: 訓練=68.60%, 驗證=68.91%, 時間=138.1s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 68.91%

Epoch 6/100, LR: 0.002783
   批次  50/391: 損失=1.1107, 準確率=70.22%
   批次 100/391: 損失=1.1115, 準確率=70.46%
   批次 150/391: 損失=1.1136, 準確率=70.33%
   批次 200/391: 損失=1.1121, 準確率=70.41%
   批次 250/391: 損失=1.1096, 準確率=70.45%
   批次 300/391: 損失=1.1097, 準確率=70.38%
   批次 350/391: 損失=1.1106, 準確率=70.30%
   批次 391/391: 損失=1.1111, 準確率=70.29%
Epoch 6 完成: 訓練=70.29%, 驗證=69.15%, 時間=119.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 69.15%

Epoch 7/100, LR: 0.002775
Epoch 6 完成: 訓練=70.29%, 驗證=69.15%, 時間=119.9s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 69.15%

Epoch 7/100, LR: 0.002775
   💾 新最佳 aMLP 模型已保存: 驗證準確率 69.15%

Epoch 7/100, LR: 0.002775
Epoch 7/100, LR: 0.002775
   批次  50/391: 損失=1.0588, 準確率=72.80%
   批次 100/391: 損失=1.0670, 準確率=71.90%
   批次 150/391: 損失=1.0703, 準確率=71.96%
   批次 200/391: 損失=1.0737, 準確率=71.88%
   批次 250/391: 損失=1.0766, 準確率=71.81%
   批次 300/391: 損失=1.0767, 準確率=71.82%
   批次 350/391: 損失=1.0767, 準確率=71.76%
   批次 391/391: 損失=1.0742, 準確率=71.90%
Epoch 7 完成: 訓練=71.90%, 驗證=72.52%, 時間=125.8s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 72.52%

Epoch 8/100, LR: 0.002766
   批次  50/391: 損失=1.0438, 準確率=73.52%
   批次 100/391: 損失=1.0391, 準確率=73.72%
   批次 150/391: 損失=1.0445, 準確率=73.26%
   批次 200/391: 損失=1.0433, 準確率=73.38%
   批次 250/391: 損失=1.0444, 準確率=73.33%
   批次 300/391: 損失=1.0445, 準確率=73.37%
   批次 350/391: 損失=1.0449, 準確率=73.36%
   批次 391/391: 損失=1.0441, 準確率=73.35%
Epoch 8 完成: 訓練=73.35%, 驗證=73.49%, 時間=128.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 73.49%

Epoch 9/100, LR: 0.002756
   批次  50/391: 損失=1.0310, 準確率=73.41%
   批次 100/391: 損失=1.0295, 準確率=73.42%
   批次 150/391: 損失=1.0250, 準確率=73.80%
   批次 200/391: 損失=1.0259, 準確率=73.88%
   批次 250/391: 損失=1.0273, 準確率=73.77%
   批次 300/391: 損失=1.0272, 準確率=73.79%
   批次 350/391: 損失=1.0247, 準確率=73.95%
   批次 391/391: 損失=1.0239, 準確率=73.94%
Epoch 9 完成: 訓練=73.94%, 驗證=73.42%, 時間=114.8s

Epoch 10/100, LR: 0.002745
   批次  50/391: 損失=0.9784, 準確率=76.42%
   批次 100/391: 損失=0.9954, 準確率=75.73%
   批次 150/391: 損失=0.9991, 準確率=75.53%
   批次 200/391: 損失=1.0004, 準確率=75.38%
   批次 250/391: 損失=1.0007, 準確率=75.31%
   批次 300/391: 損失=0.9992, 準確率=75.33%
   批次 350/391: 損失=0.9981, 準確率=75.40%
   批次 391/391: 損失=0.9985, 準確率=75.42%
Epoch 10 完成: 訓練=75.42%, 驗證=73.34%, 時間=116.3s

Epoch 11/100, LR: 0.002732
   批次  50/391: 損失=0.9709, 準確率=75.75%
   批次 100/391: 損失=0.9714, 準確率=75.97%
   批次 150/391: 損失=0.9743, 準確率=76.06%
   批次 200/391: 損失=0.9788, 準確率=75.95%
   批次 250/391: 損失=0.9775, 準確率=76.00%
   批次 300/391: 損失=0.9787, 準確率=75.97%
   批次 350/391: 損失=0.9812, 準確率=75.93%
   批次 391/391: 損失=0.9815, 準確率=75.91%
Epoch 11 完成: 訓練=75.91%, 驗證=74.21%, 時間=120.4s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 74.21%

Epoch 12/100, LR: 0.002717
   批次  50/391: 損失=0.9666, 準確率=76.16%
   批次 100/391: 損失=0.9668, 準確率=76.40%
   批次 150/391: 損失=0.9642, 準確率=76.60%
   批次 200/391: 損失=0.9635, 準確率=76.68%
   批次 250/391: 損失=0.9652, 準確率=76.67%
   批次 300/391: 損失=0.9636, 準確率=76.83%
   批次 350/391: 損失=0.9628, 準確率=76.83%
   批次 391/391: 損失=0.9661, 準確率=76.71%
Epoch 12 完成: 訓練=76.71%, 驗證=75.64%, 時間=132.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.64%

Epoch 13/100, LR: 0.002702
   批次  50/391: 損失=0.9410, 準確率=78.25%
   批次 100/391: 損失=0.9400, 準確率=78.07%
   批次 150/391: 損失=0.9429, 準確率=77.80%
   批次 200/391: 損失=0.9437, 準確率=77.73%
   批次 250/391: 損失=0.9462, 準確率=77.61%
   批次 300/391: 損失=0.9483, 準確率=77.48%
   批次 350/391: 損失=0.9475, 準確率=77.55%
   批次 391/391: 損失=0.9471, 準確率=77.57%
Epoch 13 完成: 訓練=77.57%, 驗證=74.82%, 時間=127.9s

Epoch 14/100, LR: 0.002685
   批次  50/391: 損失=0.9187, 準確率=78.83%
   批次 100/391: 損失=0.9196, 準確率=78.80%
   批次 150/391: 損失=0.9302, 準確率=78.38%
   批次 200/391: 損失=0.9331, 準確率=78.13%
   批次 250/391: 損失=0.9359, 準確率=77.96%
   批次 300/391: 損失=0.9349, 準確率=78.01%
   批次 350/391: 損失=0.9333, 準確率=78.07%
   批次 391/391: 損失=0.9341, 準確率=78.03%
Epoch 14 完成: 訓練=78.03%, 驗證=75.90%, 時間=121.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 75.90%

Epoch 15/100, LR: 0.002667
   批次  50/391: 損失=0.9061, 準確率=79.45%
   批次 100/391: 損失=0.9053, 準確率=79.23%
   批次 150/391: 損失=0.9082, 準確率=79.38%
   批次 200/391: 損失=0.9121, 準確率=79.13%
   批次 250/391: 損失=0.9130, 準確率=79.20%
   批次 300/391: 損失=0.9169, 準確率=78.98%
   批次 350/391: 損失=0.9180, 準確率=78.98%
   批次 391/391: 損失=0.9184, 準確率=78.94%
Epoch 15 完成: 訓練=78.94%, 驗證=76.21%, 時間=113.0s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.21%

Epoch 16/100, LR: 0.002648
   批次  50/391: 損失=0.8867, 準確率=80.36%
   批次 100/391: 損失=0.8969, 準確率=79.80%
   批次 150/391: 損失=0.8946, 準確率=79.89%
   批次 200/391: 損失=0.8955, 準確率=79.92%
   批次 250/391: 損失=0.8978, 準確率=79.83%
   批次 300/391: 損失=0.8997, 準確率=79.69%
   批次 350/391: 損失=0.9020, 準確率=79.54%
   批次 391/391: 損失=0.9020, 準確率=79.54%
Epoch 16 完成: 訓練=79.54%, 驗證=76.20%, 時間=115.4s

Epoch 17/100, LR: 0.002627
   批次  50/391: 損失=0.8638, 準確率=81.45%
   批次 100/391: 損失=0.8769, 準確率=80.85%
   批次 150/391: 損失=0.8820, 準確率=80.63%
   批次 200/391: 損失=0.8857, 準確率=80.36%
   批次 250/391: 損失=0.8881, 準確率=80.26%
   批次 300/391: 損失=0.8901, 準確率=80.12%
   批次 350/391: 損失=0.8909, 準確率=80.09%
   批次 391/391: 損失=0.8928, 準確率=80.04%
Epoch 17 完成: 訓練=80.04%, 驗證=76.80%, 時間=115.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 76.80%

Epoch 18/100, LR: 0.002606
   批次  50/391: 損失=0.8820, 準確率=80.34%
   批次 100/391: 損失=0.8837, 準確率=80.45%
   批次 150/391: 損失=0.8789, 準確率=80.71%
   批次 200/391: 損失=0.8773, 準確率=80.87%
   批次 250/391: 損失=0.8794, 準確率=80.65%
   批次 300/391: 損失=0.8842, 準確率=80.44%
   批次 350/391: 損失=0.8840, 準確率=80.44%
   批次 391/391: 損失=0.8835, 準確率=80.41%
Epoch 18 完成: 訓練=80.41%, 驗證=77.49%, 時間=114.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 77.49%

Epoch 19/100, LR: 0.002583
   批次  50/391: 損失=0.8559, 準確率=81.39%
   批次 100/391: 損失=0.8593, 準確率=81.41%
   批次 150/391: 損失=0.8617, 準確率=81.35%
   批次 200/391: 損失=0.8624, 準確率=81.44%
   批次 250/391: 損失=0.8645, 準確率=81.33%
   批次 300/391: 損失=0.8683, 準確率=81.10%
   批次 350/391: 損失=0.8698, 準確率=80.98%
   批次 391/391: 損失=0.8702, 準確率=80.97%
Epoch 19 完成: 訓練=80.97%, 驗證=76.54%, 時間=114.0s

Epoch 20/100, LR: 0.002559
   批次  50/391: 損失=0.8452, 準確率=81.91%
   批次 100/391: 損失=0.8442, 準確率=81.77%
   批次 150/391: 損失=0.8490, 準確率=81.62%
   批次 200/391: 損失=0.8525, 準確率=81.45%
   批次 250/391: 損失=0.8559, 準確率=81.26%
   批次 300/391: 損失=0.8577, 準確率=81.27%
   批次 350/391: 損失=0.8582, 準確率=81.32%
   批次 391/391: 損失=0.8602, 準確率=81.24%
Epoch 20 完成: 訓練=81.24%, 驗證=76.43%, 時間=112.8s

Epoch 21/100, LR: 0.002533
   批次  50/391: 損失=0.8391, 準確率=82.69%
   批次 100/391: 損失=0.8446, 準確率=82.30%
   批次 150/391: 損失=0.8458, 準確率=82.12%
   批次 200/391: 損失=0.8526, 準確率=81.72%
   批次 250/391: 損失=0.8532, 準確率=81.74%
   批次 300/391: 損失=0.8526, 準確率=81.77%
   批次 350/391: 損失=0.8536, 準確率=81.68%
   批次 391/391: 損失=0.8533, 準確率=81.69%
Epoch 21 完成: 訓練=81.69%, 驗證=78.14%, 時間=112.2s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.14%

Epoch 22/100, LR: 0.002507
   批次  50/391: 損失=0.8255, 準確率=83.20%
   批次 100/391: 損失=0.8289, 準確率=83.04%
   批次 150/391: 損失=0.8323, 準確率=82.82%
   批次 200/391: 損失=0.8388, 準確率=82.42%
   批次 250/391: 損失=0.8394, 準確率=82.47%
   批次 300/391: 損失=0.8405, 準確率=82.40%
   批次 350/391: 損失=0.8438, 準確率=82.22%
   批次 391/391: 損失=0.8433, 準確率=82.21%
Epoch 22 完成: 訓練=82.21%, 驗證=77.72%, 時間=113.6s

Epoch 23/100, LR: 0.002480
   批次  50/391: 損失=0.8042, 準確率=83.58%
   批次 100/391: 損失=0.8132, 準確率=83.40%
   批次 150/391: 損失=0.8168, 準確率=83.42%
   批次 200/391: 損失=0.8229, 準確率=83.16%
   批次 250/391: 損失=0.8295, 準確率=82.82%
   批次 300/391: 損失=0.8295, 準確率=82.82%
   批次 350/391: 損失=0.8306, 準確率=82.71%
   批次 391/391: 損失=0.8328, 準確率=82.60%
Epoch 23 完成: 訓練=82.60%, 驗證=77.92%, 時間=113.4s

Epoch 24/100, LR: 0.002451
   批次  50/391: 損失=0.8072, 準確率=83.94%
   批次 100/391: 損失=0.8137, 準確率=83.61%
   批次 150/391: 損失=0.8148, 準確率=83.46%
   批次 200/391: 損失=0.8157, 準確率=83.45%
   批次 250/391: 損失=0.8185, 準確率=83.32%
   批次 300/391: 損失=0.8220, 準確率=83.07%
   批次 350/391: 損失=0.8258, 準確率=82.91%
   批次 391/391: 損失=0.8273, 準確率=82.83%
Epoch 24 完成: 訓練=82.83%, 驗證=78.74%, 時間=114.3s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 78.74%

Epoch 25/100, LR: 0.002422
   批次  50/391: 損失=0.7971, 準確率=84.36%
   批次 100/391: 損失=0.8003, 準確率=84.24%
   批次 150/391: 損失=0.8083, 準確率=83.77%
   批次 200/391: 損失=0.8097, 準確率=83.61%
   批次 250/391: 損失=0.8125, 準確率=83.30%
   批次 300/391: 損失=0.8143, 準確率=83.27%
   批次 350/391: 損失=0.8166, 準確率=83.23%
   批次 391/391: 損失=0.8183, 準確率=83.18%
Epoch 25 完成: 訓練=83.18%, 驗證=78.60%, 時間=115.7s

Epoch 26/100, LR: 0.002391
   批次  50/391: 損失=0.8086, 準確率=83.86%
   批次 100/391: 損失=0.8074, 準確率=83.66%
   批次 150/391: 損失=0.8036, 準確率=83.86%
   批次 200/391: 損失=0.8039, 準確率=83.82%
   批次 250/391: 損失=0.8076, 準確率=83.57%
   批次 300/391: 損失=0.8078, 準確率=83.59%
   批次 350/391: 損失=0.8100, 準確率=83.55%
   批次 391/391: 損失=0.8096, 準確率=83.55%
Epoch 26 完成: 訓練=83.55%, 驗證=79.19%, 時間=113.5s
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.19%

Epoch 27/100, LR: 0.002360
   批次  50/391: 損失=0.7835, 準確率=84.89%
   批次 100/391: 損失=0.7848, 準確率=84.55%
   批次 150/391: 損失=0.7883, 準確率=84.47%
   批次 200/391: 損失=0.7934, 準確率=84.31%
   批次 250/391: 損失=0.7986, 準確率=84.22%
   批次 300/391: 損失=0.8001, 準確率=84.13%
   批次 350/391: 損失=0.8016, 準確率=84.08%
   批次 391/391: 損失=0.8028, 準確率=84.07%
Epoch 27 完成: 訓練=84.07%, 驗證=78.31%, 時間=114.7s
   📊 訓練-驗證差異: 5.76%

Epoch 28/100, LR: 0.002327
   批次  50/391: 損失=0.7699, 準確率=85.19%
   批次 100/391: 損失=0.7785, 準確率=84.75%
   批次 150/391: 損失=0.7841, 準確率=84.58%
   批次 200/391: 損失=0.7871, 準確率=84.44%
   批次 250/391: 損失=0.7906, 準確率=84.34%
   批次 300/391: 損失=0.7925, 準確率=84.25%
   批次 350/391: 損失=0.7924, 準確率=84.28%
   批次 391/391: 損失=0.7956, 準確率=84.13%
Epoch 28 完成: 訓練=84.13%, 驗證=78.32%, 時間=112.6s
   📊 訓練-驗證差異: 5.81%

Epoch 29/100, LR: 0.002294
   批次  50/391: 損失=0.7680, 準確率=85.61%
   批次 100/391: 損失=0.7755, 準確率=85.16%
   批次 150/391: 損失=0.7810, 準確率=84.78%
   批次 200/391: 損失=0.7836, 準確率=84.66%
   批次 250/391: 損失=0.7842, 準確率=84.60%
   批次 300/391: 損失=0.7851, 準確率=84.62%
   批次 350/391: 損失=0.7862, 準確率=84.60%
   批次 391/391: 損失=0.7869, 準確率=84.53%
Epoch 29 完成: 訓練=84.53%, 驗證=78.68%, 時間=113.7s
   📊 訓練-驗證差異: 5.85%

Epoch 30/100, LR: 0.002260
   批次  50/391: 損失=0.7498, 準確率=86.83%
   批次 100/391: 損失=0.7736, 準確率=85.62%
   批次 150/391: 損失=0.7750, 準確率=85.53%
   批次 200/391: 損失=0.7785, 準確率=85.21%
   批次 250/391: 損失=0.7793, 準確率=85.17%
   批次 300/391: 損失=0.7797, 準確率=85.13%
   批次 350/391: 損失=0.7812, 準確率=85.07%
   批次 391/391: 損失=0.7813, 準確率=85.05%
Epoch 30 完成: 訓練=85.05%, 驗證=78.67%, 時間=114.2s
   📊 訓練-驗證差異: 6.38%

Epoch 31/100, LR: 0.002225
   批次  50/391: 損失=0.7553, 準確率=86.02%
   批次 100/391: 損失=0.7577, 準確率=85.99%
   批次 150/391: 損失=0.7616, 準確率=85.91%
   批次 200/391: 損失=0.7641, 準確率=85.70%
   批次 250/391: 損失=0.7670, 準確率=85.51%
   批次 300/391: 損失=0.7699, 準確率=85.40%
   批次 350/391: 損失=0.7704, 準確率=85.35%
   批次 391/391: 損失=0.7726, 準確率=85.24%
Epoch 31 完成: 訓練=85.24%, 驗證=79.40%, 時間=113.4s
   📊 訓練-驗證差異: 5.84%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.40%

Epoch 32/100, LR: 0.002189
   批次  50/391: 損失=0.7473, 準確率=86.86%
   批次 100/391: 損失=0.7509, 準確率=86.40%
   批次 150/391: 損失=0.7581, 準確率=85.97%
   批次 200/391: 損失=0.7590, 準確率=85.91%
   批次 250/391: 損失=0.7604, 準確率=85.92%
   批次 300/391: 損失=0.7629, 準確率=85.77%
   批次 350/391: 損失=0.7652, 準確率=85.65%
   批次 391/391: 損失=0.7674, 準確率=85.56%
Epoch 32 完成: 訓練=85.56%, 驗證=79.70%, 時間=113.9s
   📊 訓練-驗證差異: 5.86%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.70%

Epoch 33/100, LR: 0.002152
   批次  50/391: 損失=0.7371, 準確率=86.75%
   批次 100/391: 損失=0.7455, 準確率=86.43%
   批次 150/391: 損失=0.7465, 準確率=86.30%
   批次 200/391: 損失=0.7465, 準確率=86.30%
   批次 250/391: 損失=0.7517, 準確率=86.10%
   批次 300/391: 損失=0.7552, 準確率=85.96%
   批次 350/391: 損失=0.7591, 準確率=85.80%
   批次 391/391: 損失=0.7609, 準確率=85.80%
Epoch 33 完成: 訓練=85.80%, 驗證=78.37%, 時間=113.3s
   📊 訓練-驗證差異: 7.43%

Epoch 34/100, LR: 0.002115
   批次  50/391: 損失=0.7435, 準確率=86.81%
   批次 100/391: 損失=0.7389, 準確率=87.02%
   批次 150/391: 損失=0.7448, 準確率=86.79%
   批次 200/391: 損失=0.7468, 準確率=86.60%
   批次 250/391: 損失=0.7480, 準確率=86.44%
   批次 300/391: 損失=0.7497, 準確率=86.37%
   批次 350/391: 損失=0.7536, 準確率=86.20%
   批次 391/391: 損失=0.7557, 準確率=86.13%
Epoch 34 完成: 訓練=86.13%, 驗證=79.02%, 時間=110.7s
   📊 訓練-驗證差異: 7.11%

Epoch 35/100, LR: 0.002077
   批次  50/391: 損失=0.7224, 準確率=87.44%
   批次 100/391: 損失=0.7279, 準確率=87.33%
   批次 150/391: 損失=0.7310, 準確率=87.24%
   批次 200/391: 損失=0.7340, 準確率=87.10%
   批次 250/391: 損失=0.7372, 準確率=86.89%
   批次 300/391: 損失=0.7420, 準確率=86.62%
   批次 350/391: 損失=0.7457, 準確率=86.47%
   批次 391/391: 損失=0.7463, 準確率=86.48%
Epoch 35 完成: 訓練=86.48%, 驗證=79.66%, 時間=109.7s
   📊 訓練-驗證差異: 6.82%

Epoch 36/100, LR: 0.002038
   批次  50/391: 損失=0.7204, 準確率=87.53%
   批次 100/391: 損失=0.7245, 準確率=87.30%
   批次 150/391: 損失=0.7290, 準確率=87.15%
   批次 200/391: 損失=0.7309, 準確率=87.09%
   批次 250/391: 損失=0.7376, 準確率=86.82%
   批次 300/391: 損失=0.7383, 準確率=86.84%
   批次 350/391: 損失=0.7410, 準確率=86.74%
   批次 391/391: 損失=0.7428, 準確率=86.65%
Epoch 36 完成: 訓練=86.65%, 驗證=78.97%, 時間=111.5s
   ⚠️  過擬合警告: 差異 7.68% > 閾值 7.5% (1/8)
   📊 訓練-驗證差異: 7.68%

Epoch 37/100, LR: 0.001998
   批次  50/391: 損失=0.7172, 準確率=88.09%
   批次 100/391: 損失=0.7267, 準確率=87.57%
   批次 150/391: 損失=0.7253, 準確率=87.50%
   批次 200/391: 損失=0.7272, 準確率=87.42%
   批次 250/391: 損失=0.7299, 準確率=87.28%
   批次 300/391: 損失=0.7316, 準確率=87.15%
   批次 350/391: 損失=0.7330, 準確率=87.06%
   批次 391/391: 損失=0.7345, 準確率=86.99%
Epoch 37 完成: 訓練=86.99%, 驗證=79.30%, 時間=112.7s
   ⚠️  過擬合警告: 差異 7.69% > 閾值 7.5% (2/8)
   📊 訓練-驗證差異: 7.69%

Epoch 38/100, LR: 0.001958
   批次  50/391: 損失=0.7278, 準確率=87.55%
   批次 100/391: 損失=0.7211, 準確率=87.83%
   批次 150/391: 損失=0.7222, 準確率=87.57%
   批次 200/391: 損失=0.7266, 準確率=87.37%
   批次 250/391: 損失=0.7284, 準確率=87.23%
   批次 300/391: 損失=0.7295, 準確率=87.16%
   批次 350/391: 損失=0.7309, 準確率=87.05%
   批次 391/391: 損失=0.7328, 準確率=86.99%
Epoch 38 完成: 訓練=86.99%, 驗證=79.05%, 時間=111.6s
   ⚠️  過擬合警告: 差異 7.94% > 閾值 7.5% (3/8)
   📊 訓練-驗證差異: 7.94%

Epoch 39/100, LR: 0.001918
   批次  50/391: 損失=0.7211, 準確率=87.89%
   批次 100/391: 損失=0.7177, 準確率=87.86%
   批次 150/391: 損失=0.7176, 準確率=87.69%
   批次 200/391: 損失=0.7185, 準確率=87.59%
   批次 250/391: 損失=0.7177, 準確率=87.66%
   批次 300/391: 損失=0.7207, 準確率=87.53%
   批次 350/391: 損失=0.7216, 準確率=87.54%
   批次 391/391: 損失=0.7221, 準確率=87.52%
Epoch 39 完成: 訓練=87.52%, 驗證=79.25%, 時間=114.5s
   ⚠️  過擬合警告: 差異 8.27% > 閾值 7.5% (4/8)
   📊 訓練-驗證差異: 8.27%

Epoch 40/100, LR: 0.001877
   批次  50/391: 損失=0.6953, 準確率=88.75%
   批次 100/391: 損失=0.6962, 準確率=88.71%
   批次 150/391: 損失=0.7017, 準確率=88.48%
   批次 200/391: 損失=0.7066, 準確率=88.18%
   批次 250/391: 損失=0.7110, 準確率=88.00%
   批次 300/391: 損失=0.7125, 準確率=87.88%
   批次 350/391: 損失=0.7142, 準確率=87.81%
   批次 391/391: 損失=0.7168, 準確率=87.70%
Epoch 40 完成: 訓練=87.70%, 驗證=79.74%, 時間=111.9s
   ⚠️  過擬合警告: 差異 7.96% > 閾值 7.5% (5/8)
   📊 訓練-驗證差異: 7.96%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.74%

Epoch 41/100, LR: 0.001835
   批次  50/391: 損失=0.6923, 準確率=88.86%
   批次 100/391: 損失=0.6923, 準確率=88.97%
   批次 150/391: 損失=0.6978, 準確率=88.74%
   批次 200/391: 損失=0.7039, 準確率=88.40%
   批次 250/391: 損失=0.7061, 準確率=88.29%
   批次 300/391: 損失=0.7086, 準確率=88.16%
   批次 350/391: 損失=0.7102, 準確率=88.08%
   批次 391/391: 損失=0.7123, 準確率=88.01%
Epoch 41 完成: 訓練=88.01%, 驗證=79.05%, 時間=111.0s
   ⚠️  過擬合警告: 差異 8.96% > 閾值 7.5% (6/8)
   📊 訓練-驗證差異: 8.96%

Epoch 42/100, LR: 0.001793
   批次  50/391: 損失=0.6975, 準確率=88.58%
   批次 100/391: 損失=0.6926, 準確率=88.74%
   批次 150/391: 損失=0.6972, 準確率=88.59%
   批次 200/391: 損失=0.6993, 準確率=88.44%
   批次 250/391: 損失=0.7019, 準確率=88.32%
   批次 300/391: 損失=0.7022, 準確率=88.32%
   批次 350/391: 損失=0.7043, 準確率=88.23%
   批次 391/391: 損失=0.7059, 準確率=88.17%
Epoch 42 完成: 訓練=88.17%, 驗證=79.85%, 時間=111.2s
   ⚠️  過擬合警告: 差異 8.32% > 閾值 7.5% (7/8)
   📊 訓練-驗證差異: 8.32%
   💾 新最佳 aMLP 模型已保存: 驗證準確率 79.85%

Epoch 43/100, LR: 0.001751
   批次  50/391: 損失=0.6862, 準確率=89.38%
   批次 100/391: 損失=0.6938, 準確率=88.82%
   批次 150/391: 損失=0.6946, 準確率=88.71%
   批次 200/391: 損失=0.6966, 準確率=88.66%
   批次 250/391: 損失=0.6961, 準確率=88.69%
   批次 300/391: 損失=0.6981, 準確率=88.62%
   批次 350/391: 損失=0.6975, 準確率=88.65%
   批次 391/391: 損失=0.7000, 準確率=88.59%
Epoch 43 完成: 訓練=88.59%, 驗證=79.44%, 時間=110.3s
   ⚠️  過擬合警告: 差異 9.15% > 閾值 7.5% (8/8)
   🛑 過擬合早停: 連續 8 epochs 訓練-驗證差異超過 7.5%

⏱️ 超快速 aMLP 訓練時間統計:
   • 總訓練時間: 5037.7s (84.0min)
   • 實際訓練epochs: 43 / 100
   • 平均每epoch: 117.1s
   • 最佳 aMLP 驗證準確率: 79.85%
   🔥 注意力機制效果: 預期比原 gMLP 提升 2-5%
   • 已載入最佳 aMLP 模型權重

📈 繪製超快速 aMLP 訓練歷史...

📊 評估超縮小版 aMLP 模型...
   ✓ aMLP 整體準確率: 79.85%
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:697: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.tight_layout()
e:\實驗室\gMLP\Image Classification\model_13_aMLP\model_13_aMLP.py:698: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  plt.savefig("ultra_small_amlp_results.png", dpi=300, bbox_inches="tight")
C:\Users\n10825019\AppData\Local\Programs\Python\Python311\Lib\tkinter\__init__.py:861: UserWarning: Glyph 128293 (\N{FIRE}) missing from current font.
  func(*args)

🎉 aMLP 訓練完成總結:
   • 選擇模型: aMLP-Nano (注意力增強)
   • 最終準確率: 79.85%
   • 總訓練時間: 84.0 分鐘
   • 平均每epoch: 117.1 秒
   • 實際訓練輪數: 43/100
   • 早停狀態: 啟用
   🔥 注意力機制: 已啟用 (aMLP)
   ✅ 良好性能！注意力機制有效
